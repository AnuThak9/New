{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a54b4474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io as sio\n",
    "from os import listdir\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm, trange\n",
    "import math   \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec25b150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io as sio\n",
    "from os import listdir\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm, trange\n",
    "import math   \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6887194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io as sio\n",
    "from os import listdir\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm, trange\n",
    "import math   \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Install Data as it is saved from MATLAB\n",
    "\n",
    "NumPerElement=79999+1\n",
    "\n",
    "im=torch.zeros(NumPerElement,1,2,3600)\n",
    "#im=torch.zeros(NumPerElement,1,128,40)\n",
    "label=torch.zeros(NumPerElement)\n",
    "\n",
    "count1=0\n",
    "count=-1\n",
    "folder_dir ='/Users/anuraagthakur/Desktop/RawData/FMCWT1'\n",
    "for images in os.listdir(folder_dir):\n",
    "    if images != '.DS_Store':\n",
    "        AA=sio.loadmat(folder_dir+'/'+images)\n",
    "        if(count1<NumPerElement):\n",
    "            count1=count1+1\n",
    "            count=count+1\n",
    "            for key, value in AA.items():\n",
    "                #im[count,0:1,0:128,0:40] = torch.from_numpy(value)\n",
    "                im[count,0:1,0:1,0:3600] = torch.from_numpy(value.real[0:1,0:3600])\n",
    "                im[count,0:1,1:2,0:3600] = torch.from_numpy(value.imag[0:1,0:3600].copy())\n",
    "            label[count]=0\n",
    "\n",
    "            \n",
    "folder_dir ='/Users/anuraagthakur/Desktop/RawData/FMCWT2'\n",
    "count2=0\n",
    "for images in os.listdir(folder_dir):\n",
    "    if images != '.DS_Store':\n",
    "        AA=sio.loadmat(folder_dir+'/'+images)\n",
    "        if(count2<NumPerElement):\n",
    "            count2=count2+1\n",
    "            count=count+1\n",
    "            for key, value in AA.items():\n",
    "                #im[count,0:1,0:128,0:40] = torch.from_numpy(value)\n",
    "                im[count,0:1,0:1,0:3600] = torch.from_numpy(value.real[0:1,0:3600])\n",
    "                im[count,0:1,1:2,0:3600] = torch.from_numpy(value.imag[0:1,0:3600].copy())\n",
    "            label[count]=1\n",
    "\n",
    "        \n",
    "folder_dir ='/Users/anuraagthakur/Desktop/RawData/FMCWT3'\n",
    "count3=0\n",
    "for images in os.listdir(folder_dir):\n",
    "    if images != '.DS_Store':\n",
    "        AA=sio.loadmat(folder_dir+'/'+images)\n",
    "        if(count3<NumPerElement):\n",
    "            count3=count3+1\n",
    "            count=count+1\n",
    "            for key, value in AA.items():\n",
    "                #im[count,0:1,0:128,0:40] = torch.from_numpy(value)\n",
    "                im[count,0:1,0:1,0:3600] = torch.from_numpy(value.real[0:1,0:3600])\n",
    "                im[count,0:1,1:2,0:3600] = torch.from_numpy(value.imag[0:1,0:3600].copy())\n",
    "            label[count]=2\n",
    "\n",
    " \n",
    "folder_dir ='/Users/anuraagthakur/Desktop/RawData/FMCWT4'\n",
    "count4=0\n",
    "for images in os.listdir(folder_dir):   \n",
    "    if images != '.DS_Store':\n",
    "        AA=sio.loadmat(folder_dir+'/'+images)\n",
    "        if(count4<NumPerElement):\n",
    "            count4=count4+1\n",
    "            count=count+1\n",
    "            for key, value in AA.items():\n",
    "                #im[count,0:1,0:128,0:40] = torch.from_numpy(value)\n",
    "                im[count,0:1,0:1,0:3600] = torch.from_numpy(value.real[0:1,0:3600])\n",
    "                im[count,0:1,1:2,0:3600] = torch.from_numpy(value.imag[0:1,0:3600].copy())\n",
    "            label[count]=3\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ab7d290-9203-465b-b20d-9194f0ade487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io as sio\n",
    "from os import listdir\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm, trange\n",
    "import math   \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Install Data as it is saved from MATLAB\n",
    "\n",
    "NumPerElement=79999+1\n",
    "\n",
    "im=torch.zeros(NumPerElement,1,2,3600)\n",
    "#im=torch.zeros(NumPerElement,1,128,40)\n",
    "label=torch.zeros(NumPerElement)\n",
    "\n",
    "count1=0\n",
    "count=-1\n",
    "folder_dir ='/Users/anuraagthakur/Desktop/RawData/2CW'\n",
    "for images in os.listdir(folder_dir):\n",
    "    if images != '.DS_Store':\n",
    "        AA=sio.loadmat(folder_dir+'/'+images)\n",
    "        if(count1<NumPerElement):\n",
    "            count1=count1+1\n",
    "            count=count+1\n",
    "            for key, value in AA.items():\n",
    "                #im[count,0:1,0:128,0:40] = torch.from_numpy(value)\n",
    "                im[count,0:1,0:1,0:3600] = torch.from_numpy(value.real[0:1,0:3600])\n",
    "                im[count,0:1,1:2,0:3600] = torch.from_numpy(value.imag[0:1,0:3600].copy())\n",
    "            label[count]=0\n",
    "\n",
    "            \n",
    "folder_dir ='/Users/anuraagthakur/Desktop/RawData/4CW'\n",
    "count2=0\n",
    "for images in os.listdir(folder_dir):\n",
    "    if images != '.DS_Store':\n",
    "        AA=sio.loadmat(folder_dir+'/'+images)\n",
    "        if(count2<NumPerElement):\n",
    "            count2=count2+1\n",
    "            count=count+1\n",
    "            for key, value in AA.items():\n",
    "                #im[count,0:1,0:128,0:40] = torch.from_numpy(value)\n",
    "                im[count,0:1,0:1,0:3600] = torch.from_numpy(value.real[0:1,0:3600])\n",
    "                im[count,0:1,1:2,0:3600] = torch.from_numpy(value.imag[0:1,0:3600].copy())\n",
    "            label[count]=1\n",
    "\n",
    "        \n",
    "folder_dir ='/Users/anuraagthakur/Desktop/RawData/6CW'\n",
    "count3=0\n",
    "for images in os.listdir(folder_dir):\n",
    "    if images != '.DS_Store':\n",
    "        AA=sio.loadmat(folder_dir+'/'+images)\n",
    "        if(count3<NumPerElement):\n",
    "            count3=count3+1\n",
    "            count=count+1\n",
    "            for key, value in AA.items():\n",
    "                #im[count,0:1,0:128,0:40] = torch.from_numpy(value)\n",
    "                im[count,0:1,0:1,0:3600] = torch.from_numpy(value.real[0:1,0:3600])\n",
    "                im[count,0:1,1:2,0:3600] = torch.from_numpy(value.imag[0:1,0:3600].copy())\n",
    "            label[count]=2\n",
    "\n",
    " \n",
    "folder_dir ='/Users/anuraagthakur/Desktop/RawData/8CW'\n",
    "count4=0\n",
    "for images in os.listdir(folder_dir):   \n",
    "    if images != '.DS_Store':\n",
    "        AA=sio.loadmat(folder_dir+'/'+images)\n",
    "        if(count4<NumPerElement):\n",
    "            count4=count4+1\n",
    "            count=count+1\n",
    "            for key, value in AA.items():\n",
    "                #im[count,0:1,0:128,0:40] = torch.from_numpy(value)\n",
    "                im[count,0:1,0:1,0:3600] = torch.from_numpy(value.real[0:1,0:3600])\n",
    "                im[count,0:1,1:2,0:3600] = torch.from_numpy(value.imag[0:1,0:3600].copy())\n",
    "            label[count]=3\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55c5ee4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79999\n"
     ]
    }
   ],
   "source": [
    "print(count)\n",
    "from sklearn.utils import shuffle\n",
    "im, label = shuffle(im, label, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2e885a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "doneing environment: \\ \n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.9.0\n",
      "  latest version: 25.3.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=25.3.1\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c pytorch pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04c108bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "doneing environment: / \n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.9.0\n",
      "  latest version: 25.3.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=25.3.1\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install pytorch torchvision -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b35d22fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some parts borrowed from:https://medium.com/@brianpulfer/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.utils import shuffle\n",
    "import math   \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4ec714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "# Uses the Dot Product to determine the correlation and this determines the level of similarity\n",
    "    def __init__(self, d, NumberHeadsGiven):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.d = d\n",
    "        self.NumberHeadsGivenLocal = NumberHeadsGiven\n",
    "\n",
    "        d_head = int(d / NumberHeadsGiven)\n",
    "        # mapping for query key and value is here from the open source transformer code I mentioned in the paper itself\n",
    "        self.q_mappingSelf = nn.ModuleList(\n",
    "            [nn.Linear(d_head, d_head) for _ in range(self.NumberHeadsGivenLocal)]\n",
    "        )\n",
    "        self.k_mappingSelf = nn.ModuleList(\n",
    "            [nn.Linear(d_head, d_head) for _ in range(self.NumberHeadsGivenLocal)]\n",
    "        )\n",
    "        self.v_mappingsSelf = nn.ModuleList(\n",
    "            [nn.Linear(d_head, d_head) for _ in range(self.NumberHeadsGivenLocal)]\n",
    "        )\n",
    "        self.d_head = d_head\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward (self, GivenSequence):\n",
    "        ResultofAttentionandValue = []\n",
    "        for sequence in GivenSequence: # Each head gets different part of the token \n",
    "            seq_ResultofAttentionandValue = []\n",
    "            for head in range(self.NumberHeadsGivenLocal):\n",
    "                q_mapping = self.q_mappingSelf[head]\n",
    "                k_mapping = self.k_mappingSelf[head]\n",
    "                v_mapping = self.v_mappingsSelf[head]\n",
    "\n",
    "                seq = sequence[:, head * self.d_head : (head + 1) * self.d_head] # Different from the original paper but used in newer designs\n",
    "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "\n",
    "                attention = self.softmax(q @ k.T / (self.d_head**0.5)) # Attention Dot product\n",
    "                seq_ResultofAttentionandValue.append(attention @ v) # Attention times value\n",
    "            ResultofAttentionandValue.append(torch.hstack(seq_ResultofAttentionandValue))\n",
    "        return torch.cat([torch.unsqueeze(r, dim=0) for r in ResultofAttentionandValue])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0afe84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, Hidden_DGiven, NumberHeadsGiven, HiddenMLPMultiplier=4): # Changed Ratio to 2 to experiment in some runs,not too much of a change\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.Hidden_DGivenLocal= Hidden_DGiven\n",
    "        self.NumberHeadsGivenLocal = NumberHeadsGiven\n",
    "\n",
    "        self.normComp1 = nn.LayerNorm(Hidden_DGiven)\n",
    "        self.mhsa = MultiHeadSelfAttention(Hidden_DGiven, NumberHeadsGiven)\n",
    "        self.normComp2 = nn.LayerNorm(Hidden_DGiven)\n",
    "        self.mlp = nn.Sequential( # Did some experiments here too but not much of a change for the artitecture, does not really matter\n",
    "            nn.Linear(Hidden_DGiven, HiddenMLPMultiplier * Hidden_DGiven),\n",
    "            nn.GELU(),# Maybe add more hidden layers but not actually needed here, overfits as is,  tweak the gelu a bit but no big change observed\n",
    "            nn.Linear(HiddenMLPMultiplier * Hidden_DGiven, Hidden_DGiven),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x + self.mhsa(self.normComp1(x)) # Residual Connection\n",
    "        output = output + self.mlp(self.normComp2(output)) # Residual Connection\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90b71b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different Types of Positional Embeddings are given here\n",
    "def get_positional_embeddings1(seq_len, d):\n",
    "    resultingOutput = torch.ones(seq_len, d)\n",
    "    for i in range(seq_len):\n",
    "        for j in range(d):\n",
    "            resultingOutput[i][j] = np.sin(i / (10000 ** (2*j / d))) if j % 2 == 0 else np.cos(i / (10000 ** (2*(j - 1) / d)))\n",
    "    return resultingOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b92f0824",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64000, 1, 2, 3600])\n",
      "torch.Size([16000, 1, 2, 3600])\n",
      "torch.Size([64000])\n",
      "torch.Size([16000])\n"
     ]
    }
   ],
   "source": [
    "#Split into trian and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(im, label, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.size())\n",
    "print(X_test.size())\n",
    "print(y_train.size())\n",
    "print(y_test.size())\n",
    "\n",
    "Length=30000\n",
    "Stride=800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "616045d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew, kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "18b8224c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                           | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4084, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3976, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3906, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3852, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3942, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.4049, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3916, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3920, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3951, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3964, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.4014, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3977, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3924, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3949, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3923, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3927, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3909, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3933, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3997, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3902, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3965, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3853, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3914, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3912, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3874, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3926, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3901, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3904, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3870, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3912, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3865, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3857, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3846, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3879, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3881, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3850, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3865, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3862, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3868, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3866, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3872, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3875, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3848, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3890, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3886, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3880, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3886, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3871, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3869, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3865, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3882, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3862, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3853, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3890, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3890, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3871, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3860, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3861, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3864, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3863, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3861, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3857, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3875, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3852, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3933, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3895, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3856, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3848, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3853, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3859, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3856, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3870, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3871, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3861, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3847, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3855, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3916, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3865, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3869, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3850, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|█████▊                       | 1/5 [1:14:58<4:59:54, 4498.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 loss: 111.15\n",
      "tensor(1.3867, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3903, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3925, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3928, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3887, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3865, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3867, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3858, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3874, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3883, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3888, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3870, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3862, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3873, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3880, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3874, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3865, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3860, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3862, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3860, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3869, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3871, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3903, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3880, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3857, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3865, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3863, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3864, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3851, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3873, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3860, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3853, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3850, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3880, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3886, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3850, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3861, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3870, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3859, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3882, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3858, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3861, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3854, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3871, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3863, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3862, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3880, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3870, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3864, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3867, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3871, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3866, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3860, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3876, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3873, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3868, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3863, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3863, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3866, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3863, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3865, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3846, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3876, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3852, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3921, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3892, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3852, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3845, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3861, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3857, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3859, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3863, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3878, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3860, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3848, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3854, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3918, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3879, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3866, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3861, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|███████████▌                 | 2/5 [2:26:57<3:39:38, 4392.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 loss: 110.96\n",
      "tensor(1.3861, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3885, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3908, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3914, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3886, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3862, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3875, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3851, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3864, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3868, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3875, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3867, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3862, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3867, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3868, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3866, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3859, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3859, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3871, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3853, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3871, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3869, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3894, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3873, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3859, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3865, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3865, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3868, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3847, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3874, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3860, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3852, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3848, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3879, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3884, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3855, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3860, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3870, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3859, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3883, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3857, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3861, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3857, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3869, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3863, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3862, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3879, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3871, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3865, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3868, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3868, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3863, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3858, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3877, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3873, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3868, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3864, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3866, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3862, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3863, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3865, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3846, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3875, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3853, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3919, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3892, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3853, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3847, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3857, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3856, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3858, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3863, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3878, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3861, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3850, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3853, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3914, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3874, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3866, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3863, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|█████████████████▍           | 3/5 [3:36:35<2:23:09, 4294.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 loss: 110.94\n",
      "tensor(1.3861, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3880, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3900, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3906, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3881, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3862, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3872, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3852, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3864, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3867, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3872, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3865, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3861, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3867, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3869, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3865, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3860, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3859, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3871, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3853, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3870, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3870, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3895, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3875, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3858, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3864, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3864, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3867, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3848, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3873, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3860, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3852, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3848, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3880, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3885, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3855, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3860, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3871, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3859, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3886, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3856, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3860, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3858, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3867, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3863, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3862, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3880, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3872, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3866, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3869, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3865, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3861, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3857, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3877, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3873, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3868, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3865, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3867, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3861, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3862, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3865, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3847, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3874, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3853, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3918, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3893, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3853, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3847, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3855, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3854, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3858, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3863, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3878, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3861, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3850, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3853, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3911, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3871, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3866, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3864, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|███████████████████████▏     | 4/5 [5:15:51<1:22:30, 4950.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 loss: 110.94\n",
      "tensor(1.3861, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3877, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3895, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3901, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3879, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3862, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3871, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3853, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3864, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3867, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3871, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3864, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3861, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3867, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3869, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3865, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3860, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3859, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3870, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3853, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3870, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3870, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3895, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3876, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3858, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3864, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3863, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3866, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3850, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3872, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3860, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3852, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3848, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3880, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3886, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3854, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3860, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3872, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3859, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3889, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3855, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3860, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3859, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3866, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3862, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3862, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3879, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3872, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3867, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3870, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3863, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3860, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3856, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3877, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3873, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3868, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3865, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3868, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3860, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3862, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3865, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3847, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3872, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3852, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3916, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3892, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3853, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3847, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3853, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3853, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3857, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3864, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3878, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3861, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3851, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3853, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3910, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3868, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3866, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3865, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 5/5 [6:45:11<00:00, 4862.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 loss: 110.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.39\n",
      "Test accuracy: 24.27%\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "class MyViT(nn.Module):\n",
    "    def __init__(self, Dimensions, number_patchesRow,number_patchesColumn, number_blocks, Hidden_DGiven, NumberHeadsGiven, output_dimension):\n",
    "        # This patches up the image in user specified coumn and row patches\n",
    "        super(MyViT, self).__init__()\n",
    "        self.Dimensions = Dimensions  \n",
    "        self.number_patchesRow = number_patchesRow\n",
    "        self.number_patchesColumn=number_patchesColumn\n",
    "        self.number_blocks = number_blocks\n",
    "        self.Hidden_DGiven= Hidden_DGiven\n",
    "        self.NumberHeadsGiven = NumberHeadsGiven\n",
    "        \n",
    "        self.patch_size = (math.floor(Dimensions[1] / number_patchesRow), math.floor(Dimensions[2] / number_patchesColumn))\n",
    "        \n",
    "        self.input_d = int(Dimensions[0] * self.patch_size[0] * self.patch_size[1])\n",
    "        self.linear_mapper = nn.Linear(self.input_d, self.Hidden_DGiven)\n",
    "\n",
    "        self.class_token = nn.Parameter(torch.rand(1, self.Hidden_DGiven))\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"positional_embeddings1\",\n",
    "            get_positional_embeddings1(number_patchesRow*number_patchesColumn + 1, Hidden_DGiven),\n",
    "            persistent=False,\n",
    "        )\n",
    "        \n",
    "        self.blocksA = nn.ModuleList(\n",
    "            [VisionTransformer(Hidden_DGiven, NumberHeadsGiven) for _ in range(number_blocks)]\n",
    "        )\n",
    "\n",
    "        self.mlp = nn.Sequential(nn.Linear(self.Hidden_DGiven, output_dimension), nn.Softmax(dim=-1))\n",
    "\n",
    "    def forward(self, images):        \n",
    "        n, c, h, w = images.shape\n",
    "        patches1A = images[0:n,0,0:self.patch_size[0]*self.number_patchesRow,0:self.patch_size[1]*self.number_patchesColumn]#patchify(images, self.n_patches).to(self.positional_embeddings1.device)\n",
    "        Size=48#(patches1A.size(1)*patches1A.size(2))/(self.number_patchesRow*self.number_patchesColumn)\n",
    "        #print(Size)\n",
    "        patches1=patches1A.reshape(n,self.number_patchesRow*self.number_patchesColumn,Size)\n",
    "        \n",
    "        tokens1 = self.linear_mapper(patches1)\n",
    "        tokensa = torch.cat((self.class_token.expand(n, 1, -1), tokens1), dim=1)\n",
    "\n",
    "        A    = tokensa\n",
    "\n",
    "        for block in self.blocksA:\n",
    "            A = block(A)\n",
    "        \n",
    "        output = torch.ones(n,self.Hidden_DGiven)\n",
    "        \n",
    "        output[0:n,self.Hidden_DGiven*0:self.Hidden_DGiven]  = A[:, 0]\n",
    "\n",
    "        return self.mlp(output)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    transform = ToTensor()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = MyViT(\n",
    "        (1,2,3600), number_patchesRow=1, number_patchesColumn=150, number_blocks=10, Hidden_DGiven=10, NumberHeadsGiven=10, output_dimension=4\n",
    "    ).to(device)\n",
    "\n",
    "    NUMBER_EPOCHS = 5\n",
    "    LearningRate = 0.001\n",
    "\n",
    "    # Training Data loop\n",
    "    optimizer = Adam(model.parameters(), lr= LearningRate)\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    for epoch in trange(NUMBER_EPOCHS, desc=\"Training\"):\n",
    "        training_loss = 0.0\n",
    "        for batch in range(0,Length,Stride):\n",
    "            y = y_train[batch:batch+Stride]\n",
    "            x = X_train[batch:batch+Stride,0:1,0:2,0:3600]\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = model(x)\n",
    "            \n",
    "            loss = criterion(y_hat, y.type(torch.LongTensor))\n",
    "            print(loss)\n",
    "            training_loss += loss.detach().cpu().item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(f\"Epoch {epoch + 1}/{NUMBER_EPOCHS} loss: {training_loss:.2f}\")\n",
    "\n",
    "    # Testing loop with testing data\n",
    "    with torch.no_grad():\n",
    "        correct, total = 0, 0\n",
    "        test_loss = 0.0\n",
    "        y = y_test\n",
    "        x = X_test\n",
    "\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_hat = model(x)\n",
    "        loss = criterion(y_hat, y.type(torch.LongTensor))\n",
    "\n",
    "        test_loss += loss.detach().cpu().item()\n",
    "\n",
    "        correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n",
    "        total += len(x)\n",
    "        print(f\"Test loss: {test_loss:.2f}\")\n",
    "        #cm = confusion_matrix(y_hat, y)\n",
    "        print(f\"Test accuracy: {correct / total * 100:.2f}%\")\n",
    "main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
