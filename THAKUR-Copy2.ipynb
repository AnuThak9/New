{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47dc980d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in /opt/anaconda3/lib/python3.9/site-packages (1.6.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from torchmetrics) (2.2.2)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /opt/anaconda3/lib/python3.9/site-packages (from torchmetrics) (0.14.0)\n",
      "Requirement already satisfied: numpy>1.20.0 in /opt/anaconda3/lib/python3.9/site-packages (from torchmetrics) (1.21.5)\n",
      "Requirement already satisfied: packaging>17.1 in /opt/anaconda3/lib/python3.9/site-packages (from torchmetrics) (21.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.9/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (63.4.1)\n",
      "Requirement already satisfied: typing_extensions in /opt/anaconda3/lib/python3.9/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/anaconda3/lib/python3.9/site-packages (from packaging>17.1->torchmetrics) (3.0.9)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->torchmetrics) (3.6.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->torchmetrics) (1.10.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->torchmetrics) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->torchmetrics) (2.11.3)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->torchmetrics) (2022.7.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch>=2.0.0->torchmetrics) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.9/site-packages (from sympy->torch>=2.0.0->torchmetrics) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9d3b334",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples loaded: 20000\n",
      "Labels for first few samples: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Epoch [1/5], Step [10/625], Loss: 1.8624\n",
      "Epoch [1/5], Step [20/625], Loss: 1.4224\n",
      "Epoch [1/5], Step [30/625], Loss: 1.3969\n",
      "Epoch [1/5], Step [40/625], Loss: 1.3771\n",
      "Epoch [1/5], Step [50/625], Loss: 1.3786\n",
      "Epoch [1/5], Step [60/625], Loss: 1.3820\n",
      "Epoch [1/5], Step [70/625], Loss: 1.3977\n",
      "Epoch [1/5], Step [80/625], Loss: 1.4313\n",
      "Epoch [1/5], Step [90/625], Loss: 1.3699\n",
      "Epoch [1/5], Step [100/625], Loss: 1.3918\n",
      "Epoch [1/5], Step [110/625], Loss: 1.4000\n",
      "Epoch [1/5], Step [120/625], Loss: 1.3907\n",
      "Epoch [1/5], Step [130/625], Loss: 1.4315\n",
      "Epoch [1/5], Step [140/625], Loss: 1.3962\n",
      "Epoch [1/5], Step [150/625], Loss: 1.3905\n",
      "Epoch [1/5], Step [160/625], Loss: 1.3971\n",
      "Epoch [1/5], Step [170/625], Loss: 1.3841\n",
      "Epoch [1/5], Step [180/625], Loss: 1.3780\n",
      "Epoch [1/5], Step [190/625], Loss: 1.3931\n",
      "Epoch [1/5], Step [200/625], Loss: 1.4128\n",
      "Epoch [1/5], Step [210/625], Loss: 1.3808\n",
      "Epoch [1/5], Step [220/625], Loss: 1.3759\n",
      "Epoch [1/5], Step [230/625], Loss: 1.4186\n",
      "Epoch [1/5], Step [240/625], Loss: 1.3842\n",
      "Epoch [1/5], Step [250/625], Loss: 1.3899\n",
      "Epoch [1/5], Step [260/625], Loss: 1.3592\n",
      "Epoch [1/5], Step [270/625], Loss: 1.3953\n",
      "Epoch [1/5], Step [280/625], Loss: 1.3888\n",
      "Epoch [1/5], Step [290/625], Loss: 1.4242\n",
      "Epoch [1/5], Step [300/625], Loss: 1.3916\n",
      "Epoch [1/5], Step [310/625], Loss: 1.3785\n",
      "Epoch [1/5], Step [320/625], Loss: 1.3839\n",
      "Epoch [1/5], Step [330/625], Loss: 1.3768\n",
      "Epoch [1/5], Step [340/625], Loss: 1.3878\n",
      "Epoch [1/5], Step [350/625], Loss: 1.3716\n",
      "Epoch [1/5], Step [360/625], Loss: 1.3826\n",
      "Epoch [1/5], Step [370/625], Loss: 1.4010\n",
      "Epoch [1/5], Step [380/625], Loss: 1.3653\n",
      "Epoch [1/5], Step [390/625], Loss: 1.3508\n",
      "Epoch [1/5], Step [400/625], Loss: 1.3772\n",
      "Epoch [1/5], Step [410/625], Loss: 1.4295\n",
      "Epoch [1/5], Step [420/625], Loss: 1.4067\n",
      "Epoch [1/5], Step [430/625], Loss: 1.3694\n",
      "Epoch [1/5], Step [440/625], Loss: 1.4121\n",
      "Epoch [1/5], Step [450/625], Loss: 1.3975\n",
      "Epoch [1/5], Step [460/625], Loss: 1.3817\n",
      "Epoch [1/5], Step [470/625], Loss: 1.3552\n",
      "Epoch [1/5], Step [480/625], Loss: 1.3969\n",
      "Epoch [1/5], Step [490/625], Loss: 1.4058\n",
      "Epoch [1/5], Step [500/625], Loss: 1.3928\n",
      "Epoch [1/5], Step [510/625], Loss: 1.3908\n",
      "Epoch [1/5], Step [520/625], Loss: 1.3671\n",
      "Epoch [1/5], Step [530/625], Loss: 1.3857\n",
      "Epoch [1/5], Step [540/625], Loss: 1.3776\n",
      "Epoch [1/5], Step [550/625], Loss: 1.3875\n",
      "Epoch [1/5], Step [560/625], Loss: 1.4024\n",
      "Epoch [1/5], Step [570/625], Loss: 1.3383\n",
      "Epoch [1/5], Step [580/625], Loss: 1.3829\n",
      "Epoch [1/5], Step [590/625], Loss: 1.4162\n",
      "Epoch [1/5], Step [600/625], Loss: 1.3605\n",
      "Epoch [1/5], Step [610/625], Loss: 1.3791\n",
      "Epoch [1/5], Step [620/625], Loss: 1.3753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Average Loss: 1.4031, Accuracy: 0.2650\n",
      "Class 0: Precision = 0.2718, Recall = 0.2960, F1 Score = 0.2834\n",
      "Class 1: Precision = 0.2576, Recall = 0.2726, F1 Score = 0.2649\n",
      "Class 2: Precision = 0.2635, Recall = 0.2290, F1 Score = 0.2450\n",
      "Class 3: Precision = 0.2693, Recall = 0.2622, F1 Score = 0.2657\n",
      "Class 4: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 5: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 6: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Epoch [2/5], Step [10/625], Loss: 1.3845\n",
      "Epoch [2/5], Step [20/625], Loss: 1.3797\n",
      "Epoch [2/5], Step [30/625], Loss: 1.3841\n",
      "Epoch [2/5], Step [40/625], Loss: 1.3852\n",
      "Epoch [2/5], Step [50/625], Loss: 1.3759\n",
      "Epoch [2/5], Step [60/625], Loss: 1.4825\n",
      "Epoch [2/5], Step [70/625], Loss: 1.3873\n",
      "Epoch [2/5], Step [80/625], Loss: 1.3801\n",
      "Epoch [2/5], Step [90/625], Loss: 1.3883\n",
      "Epoch [2/5], Step [100/625], Loss: 1.4287\n",
      "Epoch [2/5], Step [110/625], Loss: 1.3713\n",
      "Epoch [2/5], Step [120/625], Loss: 1.3703\n",
      "Epoch [2/5], Step [130/625], Loss: 1.4005\n",
      "Epoch [2/5], Step [140/625], Loss: 1.3650\n",
      "Epoch [2/5], Step [150/625], Loss: 1.2845\n",
      "Epoch [2/5], Step [160/625], Loss: 1.3991\n",
      "Epoch [2/5], Step [170/625], Loss: 1.4268\n",
      "Epoch [2/5], Step [180/625], Loss: 1.3922\n",
      "Epoch [2/5], Step [190/625], Loss: 1.3844\n",
      "Epoch [2/5], Step [200/625], Loss: 1.3781\n",
      "Epoch [2/5], Step [210/625], Loss: 1.3739\n",
      "Epoch [2/5], Step [220/625], Loss: 1.3915\n",
      "Epoch [2/5], Step [230/625], Loss: 1.3553\n",
      "Epoch [2/5], Step [240/625], Loss: 1.3166\n",
      "Epoch [2/5], Step [250/625], Loss: 1.2350\n",
      "Epoch [2/5], Step [260/625], Loss: 1.1777\n",
      "Epoch [2/5], Step [270/625], Loss: 1.3265\n",
      "Epoch [2/5], Step [280/625], Loss: 1.2366\n",
      "Epoch [2/5], Step [290/625], Loss: 1.1524\n",
      "Epoch [2/5], Step [300/625], Loss: 1.3449\n",
      "Epoch [2/5], Step [310/625], Loss: 1.4650\n",
      "Epoch [2/5], Step [320/625], Loss: 1.1976\n",
      "Epoch [2/5], Step [330/625], Loss: 1.3550\n",
      "Epoch [2/5], Step [340/625], Loss: 1.3449\n",
      "Epoch [2/5], Step [350/625], Loss: 1.3227\n",
      "Epoch [2/5], Step [360/625], Loss: 1.3785\n",
      "Epoch [2/5], Step [370/625], Loss: 1.3481\n",
      "Epoch [2/5], Step [380/625], Loss: 1.3776\n",
      "Epoch [2/5], Step [390/625], Loss: 1.3583\n",
      "Epoch [2/5], Step [400/625], Loss: 1.2840\n",
      "Epoch [2/5], Step [410/625], Loss: 1.3683\n",
      "Epoch [2/5], Step [420/625], Loss: 1.3825\n",
      "Epoch [2/5], Step [430/625], Loss: 1.3439\n",
      "Epoch [2/5], Step [440/625], Loss: 1.3629\n",
      "Epoch [2/5], Step [450/625], Loss: 1.3811\n",
      "Epoch [2/5], Step [460/625], Loss: 1.3678\n",
      "Epoch [2/5], Step [470/625], Loss: 1.3337\n",
      "Epoch [2/5], Step [480/625], Loss: 1.2976\n",
      "Epoch [2/5], Step [490/625], Loss: 1.2157\n",
      "Epoch [2/5], Step [500/625], Loss: 1.2737\n",
      "Epoch [2/5], Step [510/625], Loss: 1.1840\n",
      "Epoch [2/5], Step [520/625], Loss: 1.2671\n",
      "Epoch [2/5], Step [530/625], Loss: 1.1863\n",
      "Epoch [2/5], Step [540/625], Loss: 1.5085\n",
      "Epoch [2/5], Step [550/625], Loss: 1.3665\n",
      "Epoch [2/5], Step [560/625], Loss: 1.3624\n",
      "Epoch [2/5], Step [570/625], Loss: 1.3358\n",
      "Epoch [2/5], Step [580/625], Loss: 1.3506\n",
      "Epoch [2/5], Step [590/625], Loss: 1.3399\n",
      "Epoch [2/5], Step [600/625], Loss: 1.2959\n",
      "Epoch [2/5], Step [610/625], Loss: 1.1379\n",
      "Epoch [2/5], Step [620/625], Loss: 1.4429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Average Loss: 1.3520, Accuracy: 0.3257\n",
      "Class 0: Precision = 0.3681, Recall = 0.4486, F1 Score = 0.4044\n",
      "Class 1: Precision = 0.2762, Recall = 0.2138, F1 Score = 0.2410\n",
      "Class 2: Precision = 0.2787, Recall = 0.1966, F1 Score = 0.2306\n",
      "Class 3: Precision = 0.3411, Recall = 0.4440, F1 Score = 0.3858\n",
      "Class 4: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 5: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 6: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Epoch [3/5], Step [10/625], Loss: 1.3993\n",
      "Epoch [3/5], Step [20/625], Loss: 1.3849\n",
      "Epoch [3/5], Step [30/625], Loss: 1.3604\n",
      "Epoch [3/5], Step [40/625], Loss: 1.3229\n",
      "Epoch [3/5], Step [50/625], Loss: 1.3271\n",
      "Epoch [3/5], Step [60/625], Loss: 1.2253\n",
      "Epoch [3/5], Step [70/625], Loss: 1.1174\n",
      "Epoch [3/5], Step [80/625], Loss: 1.1773\n",
      "Epoch [3/5], Step [90/625], Loss: 1.8519\n",
      "Epoch [3/5], Step [100/625], Loss: 1.5562\n",
      "Epoch [3/5], Step [110/625], Loss: 1.4427\n",
      "Epoch [3/5], Step [120/625], Loss: 1.4466\n",
      "Epoch [3/5], Step [130/625], Loss: 1.3788\n",
      "Epoch [3/5], Step [140/625], Loss: 1.3749\n",
      "Epoch [3/5], Step [150/625], Loss: 1.3326\n",
      "Epoch [3/5], Step [160/625], Loss: 1.2365\n",
      "Epoch [3/5], Step [170/625], Loss: 1.3406\n",
      "Epoch [3/5], Step [180/625], Loss: 1.3160\n",
      "Epoch [3/5], Step [190/625], Loss: 1.2640\n",
      "Epoch [3/5], Step [200/625], Loss: 1.2744\n",
      "Epoch [3/5], Step [210/625], Loss: 1.2893\n",
      "Epoch [3/5], Step [220/625], Loss: 1.3376\n",
      "Epoch [3/5], Step [230/625], Loss: 1.2637\n",
      "Epoch [3/5], Step [240/625], Loss: 1.3708\n",
      "Epoch [3/5], Step [250/625], Loss: 1.3264\n",
      "Epoch [3/5], Step [260/625], Loss: 1.3543\n",
      "Epoch [3/5], Step [270/625], Loss: 1.3159\n",
      "Epoch [3/5], Step [280/625], Loss: 1.1852\n",
      "Epoch [3/5], Step [290/625], Loss: 1.2338\n",
      "Epoch [3/5], Step [300/625], Loss: 1.2879\n",
      "Epoch [3/5], Step [310/625], Loss: 1.1498\n",
      "Epoch [3/5], Step [320/625], Loss: 0.9805\n",
      "Epoch [3/5], Step [330/625], Loss: 1.5289\n",
      "Epoch [3/5], Step [340/625], Loss: 1.6019\n",
      "Epoch [3/5], Step [350/625], Loss: 1.4000\n",
      "Epoch [3/5], Step [360/625], Loss: 1.2815\n",
      "Epoch [3/5], Step [370/625], Loss: 1.3264\n",
      "Epoch [3/5], Step [380/625], Loss: 1.3655\n",
      "Epoch [3/5], Step [390/625], Loss: 1.3111\n",
      "Epoch [3/5], Step [400/625], Loss: 1.0648\n",
      "Epoch [3/5], Step [410/625], Loss: 0.9407\n",
      "Epoch [3/5], Step [420/625], Loss: 1.2163\n",
      "Epoch [3/5], Step [430/625], Loss: 1.0725\n",
      "Epoch [3/5], Step [440/625], Loss: 1.0313\n",
      "Epoch [3/5], Step [450/625], Loss: 1.0741\n",
      "Epoch [3/5], Step [460/625], Loss: 0.9599\n",
      "Epoch [3/5], Step [470/625], Loss: 0.9741\n",
      "Epoch [3/5], Step [480/625], Loss: 1.2483\n",
      "Epoch [3/5], Step [490/625], Loss: 0.9598\n",
      "Epoch [3/5], Step [500/625], Loss: 0.6230\n",
      "Epoch [3/5], Step [510/625], Loss: 0.7964\n",
      "Epoch [3/5], Step [520/625], Loss: 1.6995\n",
      "Epoch [3/5], Step [530/625], Loss: 0.8924\n",
      "Epoch [3/5], Step [540/625], Loss: 0.7481\n",
      "Epoch [3/5], Step [550/625], Loss: 1.0042\n",
      "Epoch [3/5], Step [560/625], Loss: 0.7009\n",
      "Epoch [3/5], Step [570/625], Loss: 0.7466\n",
      "Epoch [3/5], Step [580/625], Loss: 0.8606\n",
      "Epoch [3/5], Step [590/625], Loss: 1.3439\n",
      "Epoch [3/5], Step [600/625], Loss: 1.0308\n",
      "Epoch [3/5], Step [610/625], Loss: 0.8715\n",
      "Epoch [3/5], Step [620/625], Loss: 0.7707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Average Loss: 1.2029, Accuracy: 0.4314\n",
      "Class 0: Precision = 0.5295, Recall = 0.6708, F1 Score = 0.5918\n",
      "Class 1: Precision = 0.3617, Recall = 0.2390, F1 Score = 0.2878\n",
      "Class 2: Precision = 0.3472, Recall = 0.1952, F1 Score = 0.2499\n",
      "Class 3: Precision = 0.4109, Recall = 0.6206, F1 Score = 0.4945\n",
      "Class 4: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 5: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 6: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Epoch [4/5], Step [10/625], Loss: 0.7601\n",
      "Epoch [4/5], Step [20/625], Loss: 0.7362\n",
      "Epoch [4/5], Step [30/625], Loss: 0.9088\n",
      "Epoch [4/5], Step [40/625], Loss: 0.7651\n",
      "Epoch [4/5], Step [50/625], Loss: 0.7135\n",
      "Epoch [4/5], Step [60/625], Loss: 1.5295\n",
      "Epoch [4/5], Step [70/625], Loss: 1.0029\n",
      "Epoch [4/5], Step [80/625], Loss: 1.0986\n",
      "Epoch [4/5], Step [90/625], Loss: 0.8735\n",
      "Epoch [4/5], Step [100/625], Loss: 0.7360\n",
      "Epoch [4/5], Step [110/625], Loss: 0.7729\n",
      "Epoch [4/5], Step [120/625], Loss: 0.7844\n",
      "Epoch [4/5], Step [130/625], Loss: 0.6973\n",
      "Epoch [4/5], Step [140/625], Loss: 1.0158\n",
      "Epoch [4/5], Step [150/625], Loss: 0.8760\n",
      "Epoch [4/5], Step [160/625], Loss: 0.7754\n",
      "Epoch [4/5], Step [170/625], Loss: 0.8635\n",
      "Epoch [4/5], Step [180/625], Loss: 1.5318\n",
      "Epoch [4/5], Step [190/625], Loss: 1.5139\n",
      "Epoch [4/5], Step [200/625], Loss: 1.4077\n",
      "Epoch [4/5], Step [210/625], Loss: 1.2871\n",
      "Epoch [4/5], Step [220/625], Loss: 1.2391\n",
      "Epoch [4/5], Step [230/625], Loss: 1.1274\n",
      "Epoch [4/5], Step [240/625], Loss: 1.1107\n",
      "Epoch [4/5], Step [250/625], Loss: 0.9667\n",
      "Epoch [4/5], Step [260/625], Loss: 1.0202\n",
      "Epoch [4/5], Step [270/625], Loss: 1.0229\n",
      "Epoch [4/5], Step [280/625], Loss: 1.0078\n",
      "Epoch [4/5], Step [290/625], Loss: 0.9271\n",
      "Epoch [4/5], Step [300/625], Loss: 1.0178\n",
      "Epoch [4/5], Step [310/625], Loss: 1.0370\n",
      "Epoch [4/5], Step [320/625], Loss: 1.0320\n",
      "Epoch [4/5], Step [330/625], Loss: 0.8551\n",
      "Epoch [4/5], Step [340/625], Loss: 0.9349\n",
      "Epoch [4/5], Step [350/625], Loss: 0.8533\n",
      "Epoch [4/5], Step [360/625], Loss: 0.9667\n",
      "Epoch [4/5], Step [370/625], Loss: 0.8283\n",
      "Epoch [4/5], Step [380/625], Loss: 1.7016\n",
      "Epoch [4/5], Step [390/625], Loss: 1.5744\n",
      "Epoch [4/5], Step [400/625], Loss: 1.4420\n",
      "Epoch [4/5], Step [410/625], Loss: 1.3155\n",
      "Epoch [4/5], Step [420/625], Loss: 1.2905\n",
      "Epoch [4/5], Step [430/625], Loss: 1.2924\n",
      "Epoch [4/5], Step [440/625], Loss: 1.3092\n",
      "Epoch [4/5], Step [450/625], Loss: 1.3256\n",
      "Epoch [4/5], Step [460/625], Loss: 1.3394\n",
      "Epoch [4/5], Step [470/625], Loss: 1.3297\n",
      "Epoch [4/5], Step [480/625], Loss: 1.3157\n",
      "Epoch [4/5], Step [490/625], Loss: 1.2414\n",
      "Epoch [4/5], Step [500/625], Loss: 1.2377\n",
      "Epoch [4/5], Step [510/625], Loss: 1.2944\n",
      "Epoch [4/5], Step [520/625], Loss: 1.2842\n",
      "Epoch [4/5], Step [530/625], Loss: 1.3463\n",
      "Epoch [4/5], Step [540/625], Loss: 1.1719\n",
      "Epoch [4/5], Step [550/625], Loss: 1.0381\n",
      "Epoch [4/5], Step [560/625], Loss: 1.0167\n",
      "Epoch [4/5], Step [570/625], Loss: 1.0618\n",
      "Epoch [4/5], Step [580/625], Loss: 1.0597\n",
      "Epoch [4/5], Step [590/625], Loss: 1.2735\n",
      "Epoch [4/5], Step [600/625], Loss: 0.9413\n",
      "Epoch [4/5], Step [610/625], Loss: 1.4083\n",
      "Epoch [4/5], Step [620/625], Loss: 1.1265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Average Loss: 1.1166, Accuracy: 0.4774\n",
      "Class 0: Precision = 0.6215, Recall = 0.6706, F1 Score = 0.6451\n",
      "Class 1: Precision = 0.4239, Recall = 0.3340, F1 Score = 0.3736\n",
      "Class 2: Precision = 0.3434, Recall = 0.2448, F1 Score = 0.2858\n",
      "Class 3: Precision = 0.4650, Recall = 0.6604, F1 Score = 0.5457\n",
      "Class 4: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 5: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 6: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Epoch [5/5], Step [10/625], Loss: 1.2175\n",
      "Epoch [5/5], Step [20/625], Loss: 1.1890\n",
      "Epoch [5/5], Step [30/625], Loss: 1.0998\n",
      "Epoch [5/5], Step [40/625], Loss: 0.9493\n",
      "Epoch [5/5], Step [50/625], Loss: 0.9022\n",
      "Epoch [5/5], Step [60/625], Loss: 1.0677\n",
      "Epoch [5/5], Step [70/625], Loss: 0.9503\n",
      "Epoch [5/5], Step [80/625], Loss: 0.8342\n",
      "Epoch [5/5], Step [90/625], Loss: 0.8470\n",
      "Epoch [5/5], Step [100/625], Loss: 1.0873\n",
      "Epoch [5/5], Step [110/625], Loss: 1.4140\n",
      "Epoch [5/5], Step [120/625], Loss: 1.2951\n",
      "Epoch [5/5], Step [130/625], Loss: 1.1467\n",
      "Epoch [5/5], Step [140/625], Loss: 1.1196\n",
      "Epoch [5/5], Step [150/625], Loss: 1.0414\n",
      "Epoch [5/5], Step [160/625], Loss: 1.0123\n",
      "Epoch [5/5], Step [170/625], Loss: 0.7358\n",
      "Epoch [5/5], Step [180/625], Loss: 1.1375\n",
      "Epoch [5/5], Step [190/625], Loss: 1.0255\n",
      "Epoch [5/5], Step [200/625], Loss: 0.8212\n",
      "Epoch [5/5], Step [210/625], Loss: 0.8068\n",
      "Epoch [5/5], Step [220/625], Loss: 1.0397\n",
      "Epoch [5/5], Step [230/625], Loss: 0.9495\n",
      "Epoch [5/5], Step [240/625], Loss: 0.9809\n",
      "Epoch [5/5], Step [250/625], Loss: 0.9017\n",
      "Epoch [5/5], Step [260/625], Loss: 0.7749\n",
      "Epoch [5/5], Step [270/625], Loss: 0.6853\n",
      "Epoch [5/5], Step [280/625], Loss: 0.8040\n",
      "Epoch [5/5], Step [290/625], Loss: 0.7563\n",
      "Epoch [5/5], Step [300/625], Loss: 0.7204\n",
      "Epoch [5/5], Step [310/625], Loss: 0.6487\n",
      "Epoch [5/5], Step [320/625], Loss: 0.8147\n",
      "Epoch [5/5], Step [330/625], Loss: 0.7990\n",
      "Epoch [5/5], Step [340/625], Loss: 0.6810\n",
      "Epoch [5/5], Step [350/625], Loss: 0.8679\n",
      "Epoch [5/5], Step [360/625], Loss: 0.6585\n",
      "Epoch [5/5], Step [370/625], Loss: 0.8280\n",
      "Epoch [5/5], Step [380/625], Loss: 0.6529\n",
      "Epoch [5/5], Step [390/625], Loss: 0.7477\n",
      "Epoch [5/5], Step [400/625], Loss: 0.7270\n",
      "Epoch [5/5], Step [410/625], Loss: 0.8353\n",
      "Epoch [5/5], Step [420/625], Loss: 0.7542\n",
      "Epoch [5/5], Step [430/625], Loss: 0.6918\n",
      "Epoch [5/5], Step [440/625], Loss: 0.6688\n",
      "Epoch [5/5], Step [450/625], Loss: 0.7992\n",
      "Epoch [5/5], Step [460/625], Loss: 3.1148\n",
      "Epoch [5/5], Step [470/625], Loss: 1.8815\n",
      "Epoch [5/5], Step [480/625], Loss: 1.5190\n",
      "Epoch [5/5], Step [490/625], Loss: 1.2463\n",
      "Epoch [5/5], Step [500/625], Loss: 1.1427\n",
      "Epoch [5/5], Step [510/625], Loss: 1.1142\n",
      "Epoch [5/5], Step [520/625], Loss: 1.1399\n",
      "Epoch [5/5], Step [530/625], Loss: 1.2619\n",
      "Epoch [5/5], Step [540/625], Loss: 1.1299\n",
      "Epoch [5/5], Step [550/625], Loss: 1.2365\n",
      "Epoch [5/5], Step [560/625], Loss: 1.1611\n",
      "Epoch [5/5], Step [570/625], Loss: 1.1791\n",
      "Epoch [5/5], Step [580/625], Loss: 1.0509\n",
      "Epoch [5/5], Step [590/625], Loss: 1.0638\n",
      "Epoch [5/5], Step [600/625], Loss: 1.1312\n",
      "Epoch [5/5], Step [610/625], Loss: 1.0684\n",
      "Epoch [5/5], Step [620/625], Loss: 1.1234\n",
      "Epoch [5/5], Average Loss: 1.0381, Accuracy: 0.5225\n",
      "Class 0: Precision = 0.6793, Recall = 0.7282, F1 Score = 0.7029\n",
      "Class 1: Precision = 0.4343, Recall = 0.4822, F1 Score = 0.4570\n",
      "Class 2: Precision = 0.3742, Recall = 0.2296, F1 Score = 0.2846\n",
      "Class 3: Precision = 0.5398, Recall = 0.6500, F1 Score = 0.5898\n",
      "Class 4: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 5: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 6: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Constants\n",
    "NumPerElement = 5000  # Limit the number of samples to 100 per folder\n",
    "sequence_length = 3600  # The length of each sequence (number of time steps)\n",
    "input_size = 2  # Real and Imaginary components\n",
    "\n",
    "# Assuming you have 7 classes\n",
    "num_classes = 7\n",
    "hidden_size = 128  # Number of hidden units in LSTM\n",
    "\n",
    "# Folder paths and corresponding labels\n",
    "folders_and_labels = [\n",
    "    ('/Users/anuraagthakur/Desktop/RawData/FMCWD1', 0),\n",
    "    ('/Users/anuraagthakur/Desktop/RawData/FMCWD2', 1),\n",
    "    ('/Users/anuraagthakur/Desktop/RawData/FMCWD3', 2),\n",
    "    ('/Users/anuraagthakur/Desktop/RawData/FMCWD4', 3),\n",
    "    # Add other folders here if needed\n",
    "]\n",
    "\n",
    "# Prepare data tensors\n",
    "im = torch.zeros(NumPerElement * len(folders_and_labels), sequence_length, input_size)  # (samples, time_steps, features)\n",
    "label = torch.zeros(NumPerElement * len(folders_and_labels))\n",
    "\n",
    "# Counters for the samples processed\n",
    "count = -1\n",
    "\n",
    "# Loop through each folder and load the data\n",
    "for folder_dir, folder_label in folders_and_labels:\n",
    "    count_in_folder = 0\n",
    "    for images in os.listdir(folder_dir):\n",
    "        if images != '.DS_Store' and count_in_folder < NumPerElement:\n",
    "            # Load the .mat file\n",
    "            AA = sio.loadmat(os.path.join(folder_dir, images))\n",
    "            count_in_folder += 1\n",
    "            count += 1\n",
    "            for key, value in AA.items():\n",
    "                # Store real and imaginary parts into the tensor\n",
    "                im[count, :, 0] = torch.from_numpy(value.real[0, :])  # Real part\n",
    "                im[count, :, 1] = torch.from_numpy(value.imag[0, :].copy())  # Imaginary part\n",
    "            label[count] = folder_label\n",
    "\n",
    "        if count_in_folder >= NumPerElement:  # Stop processing after 1000 files per folder\n",
    "            break\n",
    "\n",
    "# Verify the data shape and labels\n",
    "print(f\"Total samples loaded: {count + 1}\")\n",
    "print(f\"Labels for first few samples: {label[:10]}\")\n",
    "\n",
    "# Define the LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM expects input of shape (batch_size, sequence_length, input_size)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Only take the output from the last time step\n",
    "        last_hidden_state = lstm_out[:, -1, :]\n",
    "        out = self.fc(last_hidden_state)\n",
    "        return out\n",
    "\n",
    "# Define the device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to the device\n",
    "model = LSTMModel(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes).to(device)\n",
    "\n",
    "# Convert to dataset and DataLoader\n",
    "dataset = TensorDataset(im, label)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Ensure the model is in training mode\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i, (data, labels) in enumerate(dataloader):\n",
    "        # Move data and labels to the same device as the model\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels.long())  # Ensure labels are of type long for classification\n",
    "        total_loss += loss.item() * data.size(0)  # Accumulate loss (weighted by batch size)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        # Collect all predictions and labels for metrics\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "        total_samples += data.size(0)\n",
    "\n",
    "        if (i+1) % 10 == 0:  # Print every 10 batches\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Convert all_preds and all_labels to numpy arrays\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    # Calculate Precision, Recall, F1 for each class\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average=None, labels=np.arange(num_classes))\n",
    "    recall = recall_score(all_labels, all_preds, average=None, labels=np.arange(num_classes))\n",
    "    f1 = f1_score(all_labels, all_preds, average=None, labels=np.arange(num_classes))\n",
    "\n",
    "    # Print the detailed metrics for each class\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {total_loss / total_samples:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    for i in range(num_classes):\n",
    "        print(f\"Class {i}: Precision = {precision[i]:.4f}, Recall = {recall[i]:.4f}, F1 Score = {f1[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3896bdba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2854bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301005ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c634fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8071e4a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples loaded: 4000\n",
      "Labels for first few samples: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Epoch [1/5], Step [10/125], Loss: 1.8470\n",
      "Epoch [1/5], Step [20/125], Loss: 1.4415\n",
      "Epoch [1/5], Step [30/125], Loss: 1.4329\n",
      "Epoch [1/5], Step [40/125], Loss: 1.3950\n",
      "Epoch [1/5], Step [50/125], Loss: 1.3771\n",
      "Epoch [1/5], Step [60/125], Loss: 1.4838\n",
      "Epoch [1/5], Step [70/125], Loss: 1.4013\n",
      "Epoch [1/5], Step [80/125], Loss: 1.3973\n",
      "Epoch [1/5], Step [90/125], Loss: 1.3730\n",
      "Epoch [1/5], Step [100/125], Loss: 1.3880\n",
      "Epoch [1/5], Step [110/125], Loss: 1.3820\n",
      "Epoch [1/5], Step [120/125], Loss: 1.3866\n",
      "Epoch [1/5], Average Loss: 1.4546, Accuracy: 0.2607\n",
      "Class 0: Precision = 0.2619, Recall = 0.2810, F1 Score = 0.2711\n",
      "Class 1: Precision = 0.2629, Recall = 0.2540, F1 Score = 0.2584\n",
      "Class 2: Precision = 0.2557, Recall = 0.2340, F1 Score = 0.2444\n",
      "Class 3: Precision = 0.2620, Recall = 0.2740, F1 Score = 0.2678\n",
      "Class 4: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 5: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 6: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Epoch [2/5], Step [10/125], Loss: 1.4344\n",
      "Epoch [2/5], Step [20/125], Loss: 1.4250\n",
      "Epoch [2/5], Step [30/125], Loss: 1.4181\n",
      "Epoch [2/5], Step [40/125], Loss: 1.4003\n",
      "Epoch [2/5], Step [50/125], Loss: 1.3924\n",
      "Epoch [2/5], Step [60/125], Loss: 1.4475\n",
      "Epoch [2/5], Step [70/125], Loss: 1.4023\n",
      "Epoch [2/5], Step [80/125], Loss: 1.3769\n",
      "Epoch [2/5], Step [90/125], Loss: 1.3902\n",
      "Epoch [2/5], Step [100/125], Loss: 1.3648\n",
      "Epoch [2/5], Step [110/125], Loss: 1.3901\n",
      "Epoch [2/5], Step [120/125], Loss: 1.4466\n",
      "Epoch [2/5], Average Loss: 1.3980, Accuracy: 0.2442\n",
      "Class 0: Precision = 0.2419, Recall = 0.1860, F1 Score = 0.2103\n",
      "Class 1: Precision = 0.2600, Recall = 0.3170, F1 Score = 0.2857\n",
      "Class 2: Precision = 0.2321, Recall = 0.2720, F1 Score = 0.2505\n",
      "Class 3: Precision = 0.2405, Recall = 0.2020, F1 Score = 0.2196\n",
      "Class 4: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 5: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 6: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Epoch [3/5], Step [10/125], Loss: 1.3965\n",
      "Epoch [3/5], Step [20/125], Loss: 1.4067\n",
      "Epoch [3/5], Step [30/125], Loss: 1.4372\n",
      "Epoch [3/5], Step [40/125], Loss: 1.3988\n",
      "Epoch [3/5], Step [50/125], Loss: 1.4224\n",
      "Epoch [3/5], Step [60/125], Loss: 1.4288\n",
      "Epoch [3/5], Step [70/125], Loss: 1.3775\n",
      "Epoch [3/5], Step [80/125], Loss: 1.3709\n",
      "Epoch [3/5], Step [90/125], Loss: 1.4357\n",
      "Epoch [3/5], Step [100/125], Loss: 1.3982\n",
      "Epoch [3/5], Step [110/125], Loss: 1.3827\n",
      "Epoch [3/5], Step [120/125], Loss: 1.4219\n",
      "Epoch [3/5], Average Loss: 1.3983, Accuracy: 0.2422\n",
      "Class 0: Precision = 0.2425, Recall = 0.1370, F1 Score = 0.1751\n",
      "Class 1: Precision = 0.2425, Recall = 0.2590, F1 Score = 0.2505\n",
      "Class 2: Precision = 0.2417, Recall = 0.2710, F1 Score = 0.2555\n",
      "Class 3: Precision = 0.2424, Recall = 0.3020, F1 Score = 0.2689\n",
      "Class 4: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 5: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 6: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Epoch [4/5], Step [10/125], Loss: 1.3762\n",
      "Epoch [4/5], Step [20/125], Loss: 1.3960\n",
      "Epoch [4/5], Step [30/125], Loss: 1.3727\n",
      "Epoch [4/5], Step [40/125], Loss: 1.3861\n",
      "Epoch [4/5], Step [50/125], Loss: 1.3590\n",
      "Epoch [4/5], Step [60/125], Loss: 1.3927\n",
      "Epoch [4/5], Step [70/125], Loss: 1.3765\n",
      "Epoch [4/5], Step [80/125], Loss: 1.3982\n",
      "Epoch [4/5], Step [90/125], Loss: 1.3882\n",
      "Epoch [4/5], Step [100/125], Loss: 1.3888\n",
      "Epoch [4/5], Step [110/125], Loss: 1.4000\n",
      "Epoch [4/5], Step [120/125], Loss: 1.4140\n",
      "Epoch [4/5], Average Loss: 1.3919, Accuracy: 0.2525\n",
      "Class 0: Precision = 0.2495, Recall = 0.1340, F1 Score = 0.1744\n",
      "Class 1: Precision = 0.2470, Recall = 0.2450, F1 Score = 0.2460\n",
      "Class 2: Precision = 0.2419, Recall = 0.1950, F1 Score = 0.2159\n",
      "Class 3: Precision = 0.2619, Recall = 0.4360, F1 Score = 0.3272\n",
      "Class 4: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 5: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 6: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Epoch [5/5], Step [10/125], Loss: 1.3834\n",
      "Epoch [5/5], Step [20/125], Loss: 1.3814\n",
      "Epoch [5/5], Step [30/125], Loss: 1.3697\n",
      "Epoch [5/5], Step [40/125], Loss: 1.3903\n",
      "Epoch [5/5], Step [50/125], Loss: 1.3950\n",
      "Epoch [5/5], Step [60/125], Loss: 1.4203\n",
      "Epoch [5/5], Step [70/125], Loss: 1.3840\n",
      "Epoch [5/5], Step [80/125], Loss: 1.4088\n",
      "Epoch [5/5], Step [90/125], Loss: 1.3735\n",
      "Epoch [5/5], Step [100/125], Loss: 1.3631\n",
      "Epoch [5/5], Step [110/125], Loss: 1.3940\n",
      "Epoch [5/5], Step [120/125], Loss: 1.4205\n",
      "Epoch [5/5], Average Loss: 1.3933, Accuracy: 0.2600\n",
      "Class 0: Precision = 0.2581, Recall = 0.1990, F1 Score = 0.2247\n",
      "Class 1: Precision = 0.2683, Recall = 0.2670, F1 Score = 0.2677\n",
      "Class 2: Precision = 0.2612, Recall = 0.3160, F1 Score = 0.2860\n",
      "Class 3: Precision = 0.2520, Recall = 0.2580, F1 Score = 0.2549\n",
      "Class 4: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 5: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 6: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Constants\n",
    "NumPerElement = 1000  # Limit the number of samples to 100 per folder\n",
    "sequence_length = 3600  # The length of each sequence (number of time steps)\n",
    "input_size = 2  # Real and Imaginary components\n",
    "\n",
    "# Assuming you have 7 classes\n",
    "num_classes = 7\n",
    "hidden_size = 128  # Number of hidden units in LSTM\n",
    "\n",
    "# Folder paths and corresponding labels\n",
    "folders_and_labels = [\n",
    "    ('/Users/anuraagthakur/Desktop/RawData/FSK2', 0),\n",
    "    ('/Users/anuraagthakur/Desktop/RawData/FSK4', 1),\n",
    "    ('/Users/anuraagthakur/Desktop/RawData/FSK8', 2),\n",
    "    ('/Users/anuraagthakur/Desktop/RawData/FSK16', 3),\n",
    "    # Add other folders here if needed\n",
    "]\n",
    "\n",
    "# Prepare data tensors\n",
    "im = torch.zeros(NumPerElement * len(folders_and_labels), sequence_length, input_size)  # (samples, time_steps, features)\n",
    "label = torch.zeros(NumPerElement * len(folders_and_labels))\n",
    "\n",
    "# Counters for the samples processed\n",
    "count = -1\n",
    "\n",
    "# Loop through each folder and load the data\n",
    "for folder_dir, folder_label in folders_and_labels:\n",
    "    count_in_folder = 0\n",
    "    for images in os.listdir(folder_dir):\n",
    "        if images != '.DS_Store' and count_in_folder < NumPerElement:\n",
    "            # Load the .mat file\n",
    "            AA = sio.loadmat(os.path.join(folder_dir, images))\n",
    "            count_in_folder += 1\n",
    "            count += 1\n",
    "            for key, value in AA.items():\n",
    "                # Store real and imaginary parts into the tensor\n",
    "                im[count, :, 0] = torch.from_numpy(value.real[0, :])  # Real part\n",
    "                im[count, :, 1] = torch.from_numpy(value.imag[0, :].copy())  # Imaginary part\n",
    "            label[count] = folder_label\n",
    "\n",
    "        if count_in_folder >= NumPerElement:  # Stop processing after 1000 files per folder\n",
    "            break\n",
    "\n",
    "# Verify the data shape and labels\n",
    "print(f\"Total samples loaded: {count + 1}\")\n",
    "print(f\"Labels for first few samples: {label[:10]}\")\n",
    "\n",
    "# Define the LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM expects input of shape (batch_size, sequence_length, input_size)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Only take the output from the last time step\n",
    "        last_hidden_state = lstm_out[:, -1, :]\n",
    "        out = self.fc(last_hidden_state)\n",
    "        return out\n",
    "\n",
    "# Define the device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to the device\n",
    "model = LSTMModel(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes).to(device)\n",
    "\n",
    "# Convert to dataset and DataLoader\n",
    "dataset = TensorDataset(im, label)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Ensure the model is in training mode\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i, (data, labels) in enumerate(dataloader):\n",
    "        # Move data and labels to the same device as the model\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels.long())  # Ensure labels are of type long for classification\n",
    "        total_loss += loss.item() * data.size(0)  # Accumulate loss (weighted by batch size)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        # Collect all predictions and labels for metrics\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "        total_samples += data.size(0)\n",
    "\n",
    "        if (i+1) % 10 == 0:  # Print every 10 batches\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Convert all_preds and all_labels to numpy arrays\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    # Calculate Precision, Recall, F1 for each class, with zero_division set to 0\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average=None, labels=np.arange(num_classes), zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average=None, labels=np.arange(num_classes), zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average=None, labels=np.arange(num_classes), zero_division=0)\n",
    "\n",
    "    # Print the detailed metrics for each class\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {total_loss / total_samples:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    for i in range(num_classes):\n",
    "        print(f\"Class {i}: Precision = {precision[i]:.4f}, Recall = {recall[i]:.4f}, F1 Score = {f1[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c300d01a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87ad3f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7ba3aa9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: /Users/anuraagthakur/Desktop/RawData/FSK2, Label: 0\n",
      "Assigned label 0 to sample 1 in folder /Users/anuraagthakur/Desktop/RawData/FSK2\n",
      "Assigned label 0 to sample 2 in folder /Users/anuraagthakur/Desktop/RawData/FSK2\n",
      "Assigned label 0 to sample 3 in folder /Users/anuraagthakur/Desktop/RawData/FSK2\n",
      "Assigned label 0 to sample 4 in folder /Users/anuraagthakur/Desktop/RawData/FSK2\n",
      "Assigned label 0 to sample 5 in folder /Users/anuraagthakur/Desktop/RawData/FSK2\n",
      "Processing folder: /Users/anuraagthakur/Desktop/RawData/FSK4, Label: 1\n",
      "Assigned label 1 to sample 1 in folder /Users/anuraagthakur/Desktop/RawData/FSK4\n",
      "Assigned label 1 to sample 2 in folder /Users/anuraagthakur/Desktop/RawData/FSK4\n",
      "Assigned label 1 to sample 3 in folder /Users/anuraagthakur/Desktop/RawData/FSK4\n",
      "Assigned label 1 to sample 4 in folder /Users/anuraagthakur/Desktop/RawData/FSK4\n",
      "Assigned label 1 to sample 5 in folder /Users/anuraagthakur/Desktop/RawData/FSK4\n",
      "Processing folder: /Users/anuraagthakur/Desktop/RawData/FSK8, Label: 2\n",
      "Assigned label 2 to sample 1 in folder /Users/anuraagthakur/Desktop/RawData/FSK8\n",
      "Assigned label 2 to sample 2 in folder /Users/anuraagthakur/Desktop/RawData/FSK8\n",
      "Assigned label 2 to sample 3 in folder /Users/anuraagthakur/Desktop/RawData/FSK8\n",
      "Assigned label 2 to sample 4 in folder /Users/anuraagthakur/Desktop/RawData/FSK8\n",
      "Assigned label 2 to sample 5 in folder /Users/anuraagthakur/Desktop/RawData/FSK8\n",
      "Processing folder: /Users/anuraagthakur/Desktop/RawData/FSK16, Label: 3\n",
      "Assigned label 3 to sample 1 in folder /Users/anuraagthakur/Desktop/RawData/FSK16\n",
      "Assigned label 3 to sample 2 in folder /Users/anuraagthakur/Desktop/RawData/FSK16\n",
      "Assigned label 3 to sample 3 in folder /Users/anuraagthakur/Desktop/RawData/FSK16\n",
      "Assigned label 3 to sample 4 in folder /Users/anuraagthakur/Desktop/RawData/FSK16\n",
      "Assigned label 3 to sample 5 in folder /Users/anuraagthakur/Desktop/RawData/FSK16\n",
      "Total samples loaded: 100\n",
      "Labels for first few samples: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Epoch [1/5], Average Loss: 1.9213, Accuracy: 0.2400\n",
      "Class 0: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 1: Precision = 0.2424, Recall = 0.9600, F1 Score = 0.3871\n",
      "Class 2: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 3: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 4: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 5: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 6: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Epoch [2/5], Average Loss: 1.8730, Accuracy: 0.2500\n",
      "Class 0: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 1: Precision = 0.2500, Recall = 1.0000, F1 Score = 0.4000\n",
      "Class 2: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 3: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 4: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 5: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 6: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Epoch [3/5], Average Loss: 1.8107, Accuracy: 0.2500\n",
      "Class 0: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 1: Precision = 0.2500, Recall = 1.0000, F1 Score = 0.4000\n",
      "Class 2: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 3: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 4: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 5: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 6: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Epoch [4/5], Average Loss: 1.6809, Accuracy: 0.2500\n",
      "Class 0: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 1: Precision = 0.2500, Recall = 1.0000, F1 Score = 0.4000\n",
      "Class 2: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 3: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 4: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 5: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 6: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Epoch [5/5], Average Loss: 1.4528, Accuracy: 0.2500\n",
      "Class 0: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 1: Precision = 0.2500, Recall = 1.0000, F1 Score = 0.4000\n",
      "Class 2: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 3: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 4: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 5: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
      "Class 6: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Constants\n",
    "NumPerElement = 25  # Limit the number of samples to 100 per folder\n",
    "sequence_length = 3600  # The length of each sequence (number of time steps)\n",
    "input_size = 2  # Real and Imaginary components\n",
    "\n",
    "# Assuming you have 7 classes\n",
    "num_classes = 7\n",
    "hidden_size = 128  # Number of hidden units in LSTM\n",
    "\n",
    "# Folder paths and corresponding labels\n",
    "folders_and_labels = [\n",
    "    ('/Users/anuraagthakur/Desktop/RawData/FSK2', 0),\n",
    "    ('/Users/anuraagthakur/Desktop/RawData/FSK4', 1),\n",
    "    ('/Users/anuraagthakur/Desktop/RawData/FSK8', 2),\n",
    "    ('/Users/anuraagthakur/Desktop/RawData/FSK16', 3),\n",
    "    # Add other folders here if needed\n",
    "]\n",
    "\n",
    "# Prepare data tensors\n",
    "im = torch.zeros(NumPerElement * len(folders_and_labels), sequence_length, input_size)  # (samples, time_steps, features)\n",
    "label = torch.zeros(NumPerElement * len(folders_and_labels))\n",
    "\n",
    "# Counters for the samples processed\n",
    "count = -1\n",
    "\n",
    "# Loop through each folder and load the data\n",
    "for folder_dir, folder_label in folders_and_labels:\n",
    "    count_in_folder = 0\n",
    "    print(f\"Processing folder: {folder_dir}, Label: {folder_label}\")  # Debugging print\n",
    "    for images in os.listdir(folder_dir):\n",
    "        if images != '.DS_Store' and count_in_folder < NumPerElement:\n",
    "            # Load the .mat file\n",
    "            AA = sio.loadmat(os.path.join(folder_dir, images))\n",
    "            count_in_folder += 1\n",
    "            count += 1\n",
    "            for key, value in AA.items():\n",
    "                # Store real and imaginary parts into the tensor\n",
    "                im[count, :, 0] = torch.from_numpy(value.real[0, :])  # Real part\n",
    "                im[count, :, 1] = torch.from_numpy(value.imag[0, :].copy())  # Imaginary part\n",
    "            label[count] = folder_label\n",
    "\n",
    "            # Debugging: Print label assignment for the current sample\n",
    "            if count_in_folder <= 5:  # Print the first few samples for verification\n",
    "                print(f\"Assigned label {folder_label} to sample {count_in_folder} in folder {folder_dir}\")\n",
    "\n",
    "        if count_in_folder >= NumPerElement:  # Stop processing after 5000 files per folder\n",
    "            break\n",
    "\n",
    "# Verify the data shape and labels\n",
    "print(f\"Total samples loaded: {count + 1}\")\n",
    "print(f\"Labels for first few samples: {label[:10]}\")\n",
    "\n",
    "# Define the LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM expects input of shape (batch_size, sequence_length, input_size)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Only take the output from the last time step\n",
    "        last_hidden_state = lstm_out[:, -1, :]\n",
    "        out = self.fc(last_hidden_state)\n",
    "        return out\n",
    "\n",
    "# Define the device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to the device\n",
    "model = LSTMModel(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes).to(device)\n",
    "\n",
    "# Convert to dataset and DataLoader\n",
    "dataset = TensorDataset(im, label)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Ensure the model is in training mode\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i, (data, labels) in enumerate(dataloader):\n",
    "        # Move data and labels to the same device as the model\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels.long())  # Ensure labels are of type long for classification\n",
    "        total_loss += loss.item() * data.size(0)  # Accumulate loss (weighted by batch size)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        # Collect all predictions and labels for metrics\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "        total_samples += data.size(0)\n",
    "\n",
    "        if (i+1) % 10 == 0:  # Print every 10 batches\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Convert all_preds and all_labels to numpy arrays\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    # Calculate Precision, Recall, F1 for each class, with zero_division set to 0\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average=None, labels=np.arange(num_classes), zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average=None, labels=np.arange(num_classes), zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average=None, labels=np.arange(num_classes), zero_division=0)\n",
    "\n",
    "    # Print the detailed metrics for each class\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {total_loss / total_samples:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    for i in range(num_classes):\n",
    "        print(f\"Class {i}: Precision = {precision[i]:.4f}, Recall = {recall[i]:.4f}, F1 Score = {f1[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3c7d61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f67fbb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ec01fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d82137d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples loaded: 35000\n",
      "Labels for first few samples: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Epoch [1/5], Step [10/1094], Loss: 1.8670\n",
      "Epoch [1/5], Step [20/1094], Loss: 1.3991\n",
      "Epoch [1/5], Step [30/1094], Loss: 1.1393\n",
      "Epoch [1/5], Step [40/1094], Loss: 0.7881\n",
      "Epoch [1/5], Step [50/1094], Loss: 0.7944\n",
      "Epoch [1/5], Step [60/1094], Loss: 0.6789\n",
      "Epoch [1/5], Step [70/1094], Loss: 1.0009\n",
      "Epoch [1/5], Step [80/1094], Loss: 0.5479\n",
      "Epoch [1/5], Step [90/1094], Loss: 0.5807\n",
      "Epoch [1/5], Step [100/1094], Loss: 0.5564\n",
      "Epoch [1/5], Step [110/1094], Loss: 0.5584\n",
      "Epoch [1/5], Step [120/1094], Loss: 0.5950\n",
      "Epoch [1/5], Step [130/1094], Loss: 0.5901\n",
      "Epoch [1/5], Step [140/1094], Loss: 0.4778\n",
      "Epoch [1/5], Step [150/1094], Loss: 0.4623\n",
      "Epoch [1/5], Step [160/1094], Loss: 0.7225\n",
      "Epoch [1/5], Step [170/1094], Loss: 0.2783\n",
      "Epoch [1/5], Step [180/1094], Loss: 0.6150\n",
      "Epoch [1/5], Step [190/1094], Loss: 0.5647\n",
      "Epoch [1/5], Step [200/1094], Loss: 0.6302\n",
      "Epoch [1/5], Step [210/1094], Loss: 0.6970\n",
      "Epoch [1/5], Step [220/1094], Loss: 0.5098\n",
      "Epoch [1/5], Step [230/1094], Loss: 0.4183\n",
      "Epoch [1/5], Step [240/1094], Loss: 0.3843\n",
      "Epoch [1/5], Step [250/1094], Loss: 0.8966\n",
      "Epoch [1/5], Step [260/1094], Loss: 0.6504\n",
      "Epoch [1/5], Step [270/1094], Loss: 0.6773\n",
      "Epoch [1/5], Step [280/1094], Loss: 0.5889\n",
      "Epoch [1/5], Step [290/1094], Loss: 0.7831\n",
      "Epoch [1/5], Step [300/1094], Loss: 0.6638\n",
      "Epoch [1/5], Step [310/1094], Loss: 0.5601\n",
      "Epoch [1/5], Step [320/1094], Loss: 0.5611\n",
      "Epoch [1/5], Step [330/1094], Loss: 0.5137\n",
      "Epoch [1/5], Step [340/1094], Loss: 0.4279\n",
      "Epoch [1/5], Step [350/1094], Loss: 0.5853\n",
      "Epoch [1/5], Step [360/1094], Loss: 0.3826\n",
      "Epoch [1/5], Step [370/1094], Loss: 0.7748\n",
      "Epoch [1/5], Step [380/1094], Loss: 0.5668\n",
      "Epoch [1/5], Step [390/1094], Loss: 0.5198\n",
      "Epoch [1/5], Step [400/1094], Loss: 0.4582\n",
      "Epoch [1/5], Step [410/1094], Loss: 0.4320\n",
      "Epoch [1/5], Step [420/1094], Loss: 0.5731\n",
      "Epoch [1/5], Step [430/1094], Loss: 0.3156\n",
      "Epoch [1/5], Step [440/1094], Loss: 0.2611\n",
      "Epoch [1/5], Step [450/1094], Loss: 0.2087\n",
      "Epoch [1/5], Step [460/1094], Loss: 0.5209\n",
      "Epoch [1/5], Step [470/1094], Loss: 0.3269\n",
      "Epoch [1/5], Step [480/1094], Loss: 0.2823\n",
      "Epoch [1/5], Step [490/1094], Loss: 0.5770\n",
      "Epoch [1/5], Step [500/1094], Loss: 0.4894\n",
      "Epoch [1/5], Step [510/1094], Loss: 0.3065\n",
      "Epoch [1/5], Step [520/1094], Loss: 0.7792\n",
      "Epoch [1/5], Step [530/1094], Loss: 0.5312\n",
      "Epoch [1/5], Step [540/1094], Loss: 0.3239\n",
      "Epoch [1/5], Step [550/1094], Loss: 0.3386\n",
      "Epoch [1/5], Step [560/1094], Loss: 0.6006\n",
      "Epoch [1/5], Step [570/1094], Loss: 0.4238\n",
      "Epoch [1/5], Step [580/1094], Loss: 0.9126\n",
      "Epoch [1/5], Step [590/1094], Loss: 0.7718\n",
      "Epoch [1/5], Step [600/1094], Loss: 0.4856\n",
      "Epoch [1/5], Step [610/1094], Loss: 0.2808\n",
      "Epoch [1/5], Step [620/1094], Loss: 0.5055\n",
      "Epoch [1/5], Step [630/1094], Loss: 0.4459\n",
      "Epoch [1/5], Step [640/1094], Loss: 0.4878\n",
      "Epoch [1/5], Step [650/1094], Loss: 0.4055\n",
      "Epoch [1/5], Step [660/1094], Loss: 0.3560\n",
      "Epoch [1/5], Step [670/1094], Loss: 0.4245\n",
      "Epoch [1/5], Step [680/1094], Loss: 0.3406\n",
      "Epoch [1/5], Step [690/1094], Loss: 0.6606\n",
      "Epoch [1/5], Step [700/1094], Loss: 0.4290\n",
      "Epoch [1/5], Step [710/1094], Loss: 0.2437\n",
      "Epoch [1/5], Step [720/1094], Loss: 0.3519\n",
      "Epoch [1/5], Step [730/1094], Loss: 0.8574\n",
      "Epoch [1/5], Step [740/1094], Loss: 0.2435\n",
      "Epoch [1/5], Step [750/1094], Loss: 0.8504\n",
      "Epoch [1/5], Step [760/1094], Loss: 0.4300\n",
      "Epoch [1/5], Step [770/1094], Loss: 1.3930\n",
      "Epoch [1/5], Step [780/1094], Loss: 1.0979\n",
      "Epoch [1/5], Step [790/1094], Loss: 0.8263\n",
      "Epoch [1/5], Step [800/1094], Loss: 0.8270\n",
      "Epoch [1/5], Step [810/1094], Loss: 0.7261\n",
      "Epoch [1/5], Step [820/1094], Loss: 0.7851\n",
      "Epoch [1/5], Step [830/1094], Loss: 1.0622\n",
      "Epoch [1/5], Step [840/1094], Loss: 0.6740\n",
      "Epoch [1/5], Step [850/1094], Loss: 0.7797\n",
      "Epoch [1/5], Step [860/1094], Loss: 0.7983\n",
      "Epoch [1/5], Step [870/1094], Loss: 0.9123\n",
      "Epoch [1/5], Step [880/1094], Loss: 0.8221\n",
      "Epoch [1/5], Step [890/1094], Loss: 0.8265\n",
      "Epoch [1/5], Step [900/1094], Loss: 0.7302\n",
      "Epoch [1/5], Step [910/1094], Loss: 0.7954\n",
      "Epoch [1/5], Step [920/1094], Loss: 0.7984\n",
      "Epoch [1/5], Step [930/1094], Loss: 0.5660\n",
      "Epoch [1/5], Step [940/1094], Loss: 0.6707\n",
      "Epoch [1/5], Step [950/1094], Loss: 0.6623\n",
      "Epoch [1/5], Step [960/1094], Loss: 0.7572\n",
      "Epoch [1/5], Step [970/1094], Loss: 0.5745\n",
      "Epoch [1/5], Step [980/1094], Loss: 1.3976\n",
      "Epoch [1/5], Step [990/1094], Loss: 1.3285\n",
      "Epoch [1/5], Step [1000/1094], Loss: 1.2373\n",
      "Epoch [1/5], Step [1010/1094], Loss: 1.1687\n",
      "Epoch [1/5], Step [1020/1094], Loss: 0.9025\n",
      "Epoch [1/5], Step [1030/1094], Loss: 1.2093\n",
      "Epoch [1/5], Step [1040/1094], Loss: 0.7175\n",
      "Epoch [1/5], Step [1050/1094], Loss: 0.7394\n",
      "Epoch [1/5], Step [1060/1094], Loss: 1.6720\n",
      "Epoch [1/5], Step [1070/1094], Loss: 1.3897\n",
      "Epoch [1/5], Step [1080/1094], Loss: 0.8791\n",
      "Epoch [1/5], Step [1090/1094], Loss: 0.8429\n",
      "Epoch [1/5], Average Loss: 0.6872, Accuracy: 0.7431\n",
      "Class 0: Precision = 0.6947, Recall = 0.6622, F1 Score = 0.6781\n",
      "Class 1: Precision = 0.8029, Recall = 0.8660, F1 Score = 0.8333\n",
      "Class 2: Precision = 0.7196, Recall = 0.7900, F1 Score = 0.7532\n",
      "Class 3: Precision = 0.6727, Recall = 0.7566, F1 Score = 0.7122\n",
      "Class 4: Precision = 0.7225, Recall = 0.7750, F1 Score = 0.7479\n",
      "Class 5: Precision = 0.7398, Recall = 0.5816, F1 Score = 0.6512\n",
      "Class 6: Precision = 0.8685, Recall = 0.7702, F1 Score = 0.8164\n",
      "Epoch [2/5], Step [10/1094], Loss: 0.7393\n",
      "Epoch [2/5], Step [20/1094], Loss: 0.7159\n",
      "Epoch [2/5], Step [30/1094], Loss: 0.5399\n",
      "Epoch [2/5], Step [40/1094], Loss: 0.5843\n",
      "Epoch [2/5], Step [50/1094], Loss: 0.5856\n",
      "Epoch [2/5], Step [60/1094], Loss: 0.4267\n",
      "Epoch [2/5], Step [70/1094], Loss: 0.8294\n",
      "Epoch [2/5], Step [80/1094], Loss: 0.6373\n",
      "Epoch [2/5], Step [90/1094], Loss: 0.4443\n",
      "Epoch [2/5], Step [100/1094], Loss: 0.3362\n",
      "Epoch [2/5], Step [110/1094], Loss: 0.4437\n",
      "Epoch [2/5], Step [120/1094], Loss: 0.4904\n",
      "Epoch [2/5], Step [130/1094], Loss: 0.2921\n",
      "Epoch [2/5], Step [140/1094], Loss: 0.6210\n",
      "Epoch [2/5], Step [150/1094], Loss: 0.6534\n",
      "Epoch [2/5], Step [160/1094], Loss: 0.8553\n",
      "Epoch [2/5], Step [170/1094], Loss: 0.3213\n",
      "Epoch [2/5], Step [180/1094], Loss: 0.4390\n",
      "Epoch [2/5], Step [190/1094], Loss: 0.5396\n",
      "Epoch [2/5], Step [200/1094], Loss: 0.4494\n",
      "Epoch [2/5], Step [210/1094], Loss: 0.3279\n",
      "Epoch [2/5], Step [220/1094], Loss: 0.5958\n",
      "Epoch [2/5], Step [230/1094], Loss: 0.6231\n",
      "Epoch [2/5], Step [240/1094], Loss: 0.3791\n",
      "Epoch [2/5], Step [250/1094], Loss: 0.4515\n",
      "Epoch [2/5], Step [260/1094], Loss: 0.4214\n",
      "Epoch [2/5], Step [270/1094], Loss: 0.5068\n",
      "Epoch [2/5], Step [280/1094], Loss: 0.3767\n",
      "Epoch [2/5], Step [290/1094], Loss: 0.4670\n",
      "Epoch [2/5], Step [300/1094], Loss: 0.4154\n",
      "Epoch [2/5], Step [310/1094], Loss: 0.3748\n",
      "Epoch [2/5], Step [320/1094], Loss: 0.2440\n",
      "Epoch [2/5], Step [330/1094], Loss: 0.3936\n",
      "Epoch [2/5], Step [340/1094], Loss: 0.4686\n",
      "Epoch [2/5], Step [350/1094], Loss: 0.4308\n",
      "Epoch [2/5], Step [360/1094], Loss: 0.4697\n",
      "Epoch [2/5], Step [370/1094], Loss: 0.3145\n",
      "Epoch [2/5], Step [380/1094], Loss: 0.2584\n",
      "Epoch [2/5], Step [390/1094], Loss: 0.4084\n",
      "Epoch [2/5], Step [400/1094], Loss: 0.5034\n",
      "Epoch [2/5], Step [410/1094], Loss: 0.5115\n",
      "Epoch [2/5], Step [420/1094], Loss: 0.6132\n",
      "Epoch [2/5], Step [430/1094], Loss: 0.3526\n",
      "Epoch [2/5], Step [440/1094], Loss: 0.5738\n",
      "Epoch [2/5], Step [450/1094], Loss: 0.7380\n",
      "Epoch [2/5], Step [460/1094], Loss: 0.3174\n",
      "Epoch [2/5], Step [470/1094], Loss: 0.3196\n",
      "Epoch [2/5], Step [480/1094], Loss: 0.3832\n",
      "Epoch [2/5], Step [490/1094], Loss: 0.5107\n",
      "Epoch [2/5], Step [500/1094], Loss: 0.1794\n",
      "Epoch [2/5], Step [510/1094], Loss: 0.3843\n",
      "Epoch [2/5], Step [520/1094], Loss: 0.3804\n",
      "Epoch [2/5], Step [530/1094], Loss: 0.5568\n",
      "Epoch [2/5], Step [540/1094], Loss: 0.2705\n",
      "Epoch [2/5], Step [550/1094], Loss: 0.5996\n",
      "Epoch [2/5], Step [560/1094], Loss: 0.2319\n",
      "Epoch [2/5], Step [570/1094], Loss: 0.3512\n",
      "Epoch [2/5], Step [580/1094], Loss: 0.2552\n",
      "Epoch [2/5], Step [590/1094], Loss: 0.2967\n",
      "Epoch [2/5], Step [600/1094], Loss: 0.4126\n",
      "Epoch [2/5], Step [610/1094], Loss: 0.5968\n",
      "Epoch [2/5], Step [620/1094], Loss: 0.4470\n",
      "Epoch [2/5], Step [630/1094], Loss: 0.8877\n",
      "Epoch [2/5], Step [640/1094], Loss: 0.5649\n",
      "Epoch [2/5], Step [650/1094], Loss: 0.1975\n",
      "Epoch [2/5], Step [660/1094], Loss: 0.3935\n",
      "Epoch [2/5], Step [670/1094], Loss: 0.6419\n",
      "Epoch [2/5], Step [680/1094], Loss: 0.2845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [690/1094], Loss: 0.4831\n",
      "Epoch [2/5], Step [700/1094], Loss: 0.7619\n",
      "Epoch [2/5], Step [710/1094], Loss: 0.4106\n",
      "Epoch [2/5], Step [720/1094], Loss: 0.3815\n",
      "Epoch [2/5], Step [730/1094], Loss: 0.4959\n",
      "Epoch [2/5], Step [740/1094], Loss: 0.3357\n",
      "Epoch [2/5], Step [750/1094], Loss: 0.5090\n",
      "Epoch [2/5], Step [760/1094], Loss: 0.4047\n",
      "Epoch [2/5], Step [770/1094], Loss: 0.5701\n",
      "Epoch [2/5], Step [780/1094], Loss: 0.6265\n",
      "Epoch [2/5], Step [790/1094], Loss: 0.4955\n",
      "Epoch [2/5], Step [800/1094], Loss: 0.6111\n",
      "Epoch [2/5], Step [810/1094], Loss: 0.5579\n",
      "Epoch [2/5], Step [820/1094], Loss: 0.8465\n",
      "Epoch [2/5], Step [830/1094], Loss: 0.2990\n",
      "Epoch [2/5], Step [840/1094], Loss: 0.3088\n",
      "Epoch [2/5], Step [850/1094], Loss: 0.1843\n",
      "Epoch [2/5], Step [860/1094], Loss: 0.4186\n",
      "Epoch [2/5], Step [870/1094], Loss: 0.3510\n",
      "Epoch [2/5], Step [880/1094], Loss: 0.3838\n",
      "Epoch [2/5], Step [890/1094], Loss: 0.2429\n",
      "Epoch [2/5], Step [900/1094], Loss: 0.6500\n",
      "Epoch [2/5], Step [910/1094], Loss: 0.3150\n",
      "Epoch [2/5], Step [920/1094], Loss: 0.3173\n",
      "Epoch [2/5], Step [930/1094], Loss: 0.3824\n",
      "Epoch [2/5], Step [940/1094], Loss: 0.4988\n",
      "Epoch [2/5], Step [950/1094], Loss: 0.2045\n",
      "Epoch [2/5], Step [960/1094], Loss: 0.5508\n",
      "Epoch [2/5], Step [970/1094], Loss: 0.3377\n",
      "Epoch [2/5], Step [980/1094], Loss: 0.5107\n",
      "Epoch [2/5], Step [990/1094], Loss: 0.6235\n",
      "Epoch [2/5], Step [1000/1094], Loss: 0.5516\n",
      "Epoch [2/5], Step [1010/1094], Loss: 0.3238\n",
      "Epoch [2/5], Step [1020/1094], Loss: 0.3734\n",
      "Epoch [2/5], Step [1030/1094], Loss: 0.2868\n",
      "Epoch [2/5], Step [1040/1094], Loss: 0.7144\n",
      "Epoch [2/5], Step [1050/1094], Loss: 0.2124\n",
      "Epoch [2/5], Step [1060/1094], Loss: 0.3882\n",
      "Epoch [2/5], Step [1070/1094], Loss: 0.3975\n",
      "Epoch [2/5], Step [1080/1094], Loss: 0.3108\n",
      "Epoch [2/5], Step [1090/1094], Loss: 0.6463\n",
      "Epoch [2/5], Average Loss: 0.4632, Accuracy: 0.8437\n",
      "Class 0: Precision = 0.7734, Recall = 0.7876, F1 Score = 0.7804\n",
      "Class 1: Precision = 0.9613, Recall = 0.9596, F1 Score = 0.9605\n",
      "Class 2: Precision = 0.8567, Recall = 0.8538, F1 Score = 0.8553\n",
      "Class 3: Precision = 0.7296, Recall = 0.8288, F1 Score = 0.7760\n",
      "Class 4: Precision = 0.8496, Recall = 0.8950, F1 Score = 0.8717\n",
      "Class 5: Precision = 0.8083, Recall = 0.6738, F1 Score = 0.7349\n",
      "Class 6: Precision = 0.9411, Recall = 0.9070, F1 Score = 0.9237\n",
      "Epoch [3/5], Step [10/1094], Loss: 0.5326\n",
      "Epoch [3/5], Step [20/1094], Loss: 0.4703\n",
      "Epoch [3/5], Step [30/1094], Loss: 0.2984\n",
      "Epoch [3/5], Step [40/1094], Loss: 0.3273\n",
      "Epoch [3/5], Step [50/1094], Loss: 0.3159\n",
      "Epoch [3/5], Step [60/1094], Loss: 0.3105\n",
      "Epoch [3/5], Step [70/1094], Loss: 0.4732\n",
      "Epoch [3/5], Step [80/1094], Loss: 0.5673\n",
      "Epoch [3/5], Step [90/1094], Loss: 0.2783\n",
      "Epoch [3/5], Step [100/1094], Loss: 0.4423\n",
      "Epoch [3/5], Step [110/1094], Loss: 0.3447\n",
      "Epoch [3/5], Step [120/1094], Loss: 0.3643\n",
      "Epoch [3/5], Step [130/1094], Loss: 0.5212\n",
      "Epoch [3/5], Step [140/1094], Loss: 0.5097\n",
      "Epoch [3/5], Step [150/1094], Loss: 0.3226\n",
      "Epoch [3/5], Step [160/1094], Loss: 0.8264\n",
      "Epoch [3/5], Step [170/1094], Loss: 0.2743\n",
      "Epoch [3/5], Step [180/1094], Loss: 0.2987\n",
      "Epoch [3/5], Step [190/1094], Loss: 0.3981\n",
      "Epoch [3/5], Step [200/1094], Loss: 0.4564\n",
      "Epoch [3/5], Step [210/1094], Loss: 0.6085\n",
      "Epoch [3/5], Step [220/1094], Loss: 0.4286\n",
      "Epoch [3/5], Step [230/1094], Loss: 0.4095\n",
      "Epoch [3/5], Step [240/1094], Loss: 0.3824\n",
      "Epoch [3/5], Step [250/1094], Loss: 0.2241\n",
      "Epoch [3/5], Step [260/1094], Loss: 0.2583\n",
      "Epoch [3/5], Step [270/1094], Loss: 0.3138\n",
      "Epoch [3/5], Step [280/1094], Loss: 0.2372\n",
      "Epoch [3/5], Step [290/1094], Loss: 0.4670\n",
      "Epoch [3/5], Step [300/1094], Loss: 0.4279\n",
      "Epoch [3/5], Step [310/1094], Loss: 0.2352\n",
      "Epoch [3/5], Step [320/1094], Loss: 0.7266\n",
      "Epoch [3/5], Step [330/1094], Loss: 0.7033\n",
      "Epoch [3/5], Step [340/1094], Loss: 0.5893\n",
      "Epoch [3/5], Step [350/1094], Loss: 0.6160\n",
      "Epoch [3/5], Step [360/1094], Loss: 0.4192\n",
      "Epoch [3/5], Step [370/1094], Loss: 0.4233\n",
      "Epoch [3/5], Step [380/1094], Loss: 0.3680\n",
      "Epoch [3/5], Step [390/1094], Loss: 0.4687\n",
      "Epoch [3/5], Step [400/1094], Loss: 0.4836\n",
      "Epoch [3/5], Step [410/1094], Loss: 0.5892\n",
      "Epoch [3/5], Step [420/1094], Loss: 0.5178\n",
      "Epoch [3/5], Step [430/1094], Loss: 0.6936\n",
      "Epoch [3/5], Step [440/1094], Loss: 0.3568\n",
      "Epoch [3/5], Step [450/1094], Loss: 0.4189\n",
      "Epoch [3/5], Step [460/1094], Loss: 0.5139\n",
      "Epoch [3/5], Step [470/1094], Loss: 0.4193\n",
      "Epoch [3/5], Step [480/1094], Loss: 0.3959\n",
      "Epoch [3/5], Step [490/1094], Loss: 0.3623\n",
      "Epoch [3/5], Step [500/1094], Loss: 0.8045\n",
      "Epoch [3/5], Step [510/1094], Loss: 0.5608\n",
      "Epoch [3/5], Step [520/1094], Loss: 0.4861\n",
      "Epoch [3/5], Step [530/1094], Loss: 0.2570\n",
      "Epoch [3/5], Step [540/1094], Loss: 0.4371\n",
      "Epoch [3/5], Step [550/1094], Loss: 0.6966\n",
      "Epoch [3/5], Step [560/1094], Loss: 0.5045\n",
      "Epoch [3/5], Step [570/1094], Loss: 0.4539\n",
      "Epoch [3/5], Step [580/1094], Loss: 0.3289\n",
      "Epoch [3/5], Step [590/1094], Loss: 0.4134\n",
      "Epoch [3/5], Step [600/1094], Loss: 0.3934\n",
      "Epoch [3/5], Step [610/1094], Loss: 0.3807\n",
      "Epoch [3/5], Step [620/1094], Loss: 0.5494\n",
      "Epoch [3/5], Step [630/1094], Loss: 0.3302\n",
      "Epoch [3/5], Step [640/1094], Loss: 0.2094\n",
      "Epoch [3/5], Step [650/1094], Loss: 0.5720\n",
      "Epoch [3/5], Step [660/1094], Loss: 0.2801\n",
      "Epoch [3/5], Step [670/1094], Loss: 0.2974\n",
      "Epoch [3/5], Step [680/1094], Loss: 0.3536\n",
      "Epoch [3/5], Step [690/1094], Loss: 0.2342\n",
      "Epoch [3/5], Step [700/1094], Loss: 0.2708\n",
      "Epoch [3/5], Step [710/1094], Loss: 0.3691\n",
      "Epoch [3/5], Step [720/1094], Loss: 0.6553\n",
      "Epoch [3/5], Step [730/1094], Loss: 0.5927\n",
      "Epoch [3/5], Step [740/1094], Loss: 0.3247\n",
      "Epoch [3/5], Step [750/1094], Loss: 0.1375\n",
      "Epoch [3/5], Step [760/1094], Loss: 0.3557\n",
      "Epoch [3/5], Step [770/1094], Loss: 0.4484\n",
      "Epoch [3/5], Step [780/1094], Loss: 0.4021\n",
      "Epoch [3/5], Step [790/1094], Loss: 0.5177\n",
      "Epoch [3/5], Step [800/1094], Loss: 0.3032\n",
      "Epoch [3/5], Step [810/1094], Loss: 0.4091\n",
      "Epoch [3/5], Step [820/1094], Loss: 0.6448\n",
      "Epoch [3/5], Step [830/1094], Loss: 0.6534\n",
      "Epoch [3/5], Step [840/1094], Loss: 0.4073\n",
      "Epoch [3/5], Step [850/1094], Loss: 0.3538\n",
      "Epoch [3/5], Step [860/1094], Loss: 0.3116\n",
      "Epoch [3/5], Step [870/1094], Loss: 0.2404\n",
      "Epoch [3/5], Step [880/1094], Loss: 0.2491\n",
      "Epoch [3/5], Step [890/1094], Loss: 0.3325\n",
      "Epoch [3/5], Step [900/1094], Loss: 0.5076\n",
      "Epoch [3/5], Step [910/1094], Loss: 0.3215\n",
      "Epoch [3/5], Step [920/1094], Loss: 1.1851\n",
      "Epoch [3/5], Step [930/1094], Loss: 0.4364\n",
      "Epoch [3/5], Step [940/1094], Loss: 0.4920\n",
      "Epoch [3/5], Step [950/1094], Loss: 0.3097\n",
      "Epoch [3/5], Step [960/1094], Loss: 0.3561\n",
      "Epoch [3/5], Step [970/1094], Loss: 0.7386\n",
      "Epoch [3/5], Step [980/1094], Loss: 0.5355\n",
      "Epoch [3/5], Step [990/1094], Loss: 0.6412\n",
      "Epoch [3/5], Step [1000/1094], Loss: 0.5174\n",
      "Epoch [3/5], Step [1010/1094], Loss: 0.3688\n",
      "Epoch [3/5], Step [1020/1094], Loss: 0.3347\n",
      "Epoch [3/5], Step [1030/1094], Loss: 0.5954\n",
      "Epoch [3/5], Step [1040/1094], Loss: 0.7063\n",
      "Epoch [3/5], Step [1050/1094], Loss: 0.2745\n",
      "Epoch [3/5], Step [1060/1094], Loss: 0.3720\n",
      "Epoch [3/5], Step [1070/1094], Loss: 0.4907\n",
      "Epoch [3/5], Step [1080/1094], Loss: 0.4330\n",
      "Epoch [3/5], Step [1090/1094], Loss: 0.4647\n",
      "Epoch [3/5], Average Loss: 0.4434, Accuracy: 0.8422\n",
      "Class 0: Precision = 0.7897, Recall = 0.7796, F1 Score = 0.7846\n",
      "Class 1: Precision = 0.9683, Recall = 0.9722, F1 Score = 0.9703\n",
      "Class 2: Precision = 0.8601, Recall = 0.8702, F1 Score = 0.8651\n",
      "Class 3: Precision = 0.7435, Recall = 0.8170, F1 Score = 0.7785\n",
      "Class 4: Precision = 0.8403, Recall = 0.8662, F1 Score = 0.8531\n",
      "Class 5: Precision = 0.7929, Recall = 0.6878, F1 Score = 0.7366\n",
      "Class 6: Precision = 0.9026, Recall = 0.9026, F1 Score = 0.9026\n",
      "Epoch [4/5], Step [10/1094], Loss: 0.3659\n",
      "Epoch [4/5], Step [20/1094], Loss: 0.3940\n",
      "Epoch [4/5], Step [30/1094], Loss: 0.4393\n",
      "Epoch [4/5], Step [40/1094], Loss: 0.2397\n",
      "Epoch [4/5], Step [50/1094], Loss: 0.2855\n",
      "Epoch [4/5], Step [60/1094], Loss: 0.2230\n",
      "Epoch [4/5], Step [70/1094], Loss: 0.4048\n",
      "Epoch [4/5], Step [80/1094], Loss: 0.7771\n",
      "Epoch [4/5], Step [90/1094], Loss: 0.6040\n",
      "Epoch [4/5], Step [100/1094], Loss: 0.2917\n",
      "Epoch [4/5], Step [110/1094], Loss: 0.3298\n",
      "Epoch [4/5], Step [120/1094], Loss: 0.2708\n",
      "Epoch [4/5], Step [130/1094], Loss: 0.4808\n",
      "Epoch [4/5], Step [140/1094], Loss: 0.5313\n",
      "Epoch [4/5], Step [150/1094], Loss: 0.2887\n",
      "Epoch [4/5], Step [160/1094], Loss: 0.4567\n",
      "Epoch [4/5], Step [170/1094], Loss: 0.5267\n",
      "Epoch [4/5], Step [180/1094], Loss: 0.2742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [190/1094], Loss: 0.2228\n",
      "Epoch [4/5], Step [200/1094], Loss: 0.3952\n",
      "Epoch [4/5], Step [210/1094], Loss: 0.3376\n",
      "Epoch [4/5], Step [220/1094], Loss: 0.1851\n",
      "Epoch [4/5], Step [230/1094], Loss: 0.3663\n",
      "Epoch [4/5], Step [240/1094], Loss: 0.4113\n",
      "Epoch [4/5], Step [250/1094], Loss: 0.3138\n",
      "Epoch [4/5], Step [260/1094], Loss: 0.3214\n",
      "Epoch [4/5], Step [270/1094], Loss: 0.2849\n",
      "Epoch [4/5], Step [280/1094], Loss: 0.1517\n",
      "Epoch [4/5], Step [290/1094], Loss: 0.6189\n",
      "Epoch [4/5], Step [300/1094], Loss: 0.3751\n",
      "Epoch [4/5], Step [310/1094], Loss: 0.2923\n",
      "Epoch [4/5], Step [320/1094], Loss: 0.6171\n",
      "Epoch [4/5], Step [330/1094], Loss: 0.4108\n",
      "Epoch [4/5], Step [340/1094], Loss: 0.1566\n",
      "Epoch [4/5], Step [350/1094], Loss: 0.2633\n",
      "Epoch [4/5], Step [360/1094], Loss: 0.5087\n",
      "Epoch [4/5], Step [370/1094], Loss: 0.6518\n",
      "Epoch [4/5], Step [380/1094], Loss: 0.4854\n",
      "Epoch [4/5], Step [390/1094], Loss: 0.2003\n",
      "Epoch [4/5], Step [400/1094], Loss: 0.3742\n",
      "Epoch [4/5], Step [410/1094], Loss: 0.4563\n",
      "Epoch [4/5], Step [420/1094], Loss: 0.6355\n",
      "Epoch [4/5], Step [430/1094], Loss: 0.6545\n",
      "Epoch [4/5], Step [440/1094], Loss: 0.2547\n",
      "Epoch [4/5], Step [450/1094], Loss: 0.3761\n",
      "Epoch [4/5], Step [460/1094], Loss: 0.3134\n",
      "Epoch [4/5], Step [470/1094], Loss: 0.5822\n",
      "Epoch [4/5], Step [480/1094], Loss: 0.5220\n",
      "Epoch [4/5], Step [490/1094], Loss: 0.4351\n",
      "Epoch [4/5], Step [500/1094], Loss: 0.2225\n",
      "Epoch [4/5], Step [510/1094], Loss: 0.5638\n",
      "Epoch [4/5], Step [520/1094], Loss: 0.3862\n",
      "Epoch [4/5], Step [530/1094], Loss: 0.4203\n",
      "Epoch [4/5], Step [540/1094], Loss: 0.3364\n",
      "Epoch [4/5], Step [550/1094], Loss: 0.4897\n",
      "Epoch [4/5], Step [560/1094], Loss: 0.4772\n",
      "Epoch [4/5], Step [570/1094], Loss: 0.2940\n",
      "Epoch [4/5], Step [580/1094], Loss: 0.1705\n",
      "Epoch [4/5], Step [590/1094], Loss: 0.5129\n",
      "Epoch [4/5], Step [600/1094], Loss: 0.1569\n",
      "Epoch [4/5], Step [610/1094], Loss: 0.2513\n",
      "Epoch [4/5], Step [620/1094], Loss: 0.5878\n",
      "Epoch [4/5], Step [630/1094], Loss: 0.7316\n",
      "Epoch [4/5], Step [640/1094], Loss: 0.4416\n",
      "Epoch [4/5], Step [650/1094], Loss: 0.8659\n",
      "Epoch [4/5], Step [660/1094], Loss: 0.3946\n",
      "Epoch [4/5], Step [670/1094], Loss: 0.5467\n",
      "Epoch [4/5], Step [680/1094], Loss: 0.2400\n",
      "Epoch [4/5], Step [690/1094], Loss: 0.3055\n",
      "Epoch [4/5], Step [700/1094], Loss: 0.2023\n",
      "Epoch [4/5], Step [710/1094], Loss: 0.2896\n",
      "Epoch [4/5], Step [720/1094], Loss: 0.6038\n",
      "Epoch [4/5], Step [730/1094], Loss: 0.6288\n",
      "Epoch [4/5], Step [740/1094], Loss: 0.3855\n",
      "Epoch [4/5], Step [750/1094], Loss: 0.2240\n",
      "Epoch [4/5], Step [760/1094], Loss: 0.4749\n",
      "Epoch [4/5], Step [770/1094], Loss: 0.3610\n",
      "Epoch [4/5], Step [780/1094], Loss: 0.2530\n",
      "Epoch [4/5], Step [790/1094], Loss: 0.5297\n",
      "Epoch [4/5], Step [800/1094], Loss: 0.1834\n",
      "Epoch [4/5], Step [810/1094], Loss: 0.4212\n",
      "Epoch [4/5], Step [820/1094], Loss: 0.4660\n",
      "Epoch [4/5], Step [830/1094], Loss: 0.7184\n",
      "Epoch [4/5], Step [840/1094], Loss: 0.8912\n",
      "Epoch [4/5], Step [850/1094], Loss: 0.5372\n",
      "Epoch [4/5], Step [860/1094], Loss: 0.5940\n",
      "Epoch [4/5], Step [870/1094], Loss: 0.3243\n",
      "Epoch [4/5], Step [880/1094], Loss: 0.4884\n",
      "Epoch [4/5], Step [890/1094], Loss: 0.2138\n",
      "Epoch [4/5], Step [900/1094], Loss: 0.2283\n",
      "Epoch [4/5], Step [910/1094], Loss: 0.3618\n",
      "Epoch [4/5], Step [920/1094], Loss: 0.4499\n",
      "Epoch [4/5], Step [930/1094], Loss: 0.4000\n",
      "Epoch [4/5], Step [940/1094], Loss: 0.2582\n",
      "Epoch [4/5], Step [950/1094], Loss: 0.6153\n",
      "Epoch [4/5], Step [960/1094], Loss: 0.2760\n",
      "Epoch [4/5], Step [970/1094], Loss: 0.5045\n",
      "Epoch [4/5], Step [980/1094], Loss: 0.5087\n",
      "Epoch [4/5], Step [990/1094], Loss: 0.2297\n",
      "Epoch [4/5], Step [1000/1094], Loss: 0.6322\n",
      "Epoch [4/5], Step [1010/1094], Loss: 0.6430\n",
      "Epoch [4/5], Step [1020/1094], Loss: 0.7414\n",
      "Epoch [4/5], Step [1030/1094], Loss: 0.1964\n",
      "Epoch [4/5], Step [1040/1094], Loss: 0.3686\n",
      "Epoch [4/5], Step [1050/1094], Loss: 0.5207\n",
      "Epoch [4/5], Step [1060/1094], Loss: 0.5727\n",
      "Epoch [4/5], Step [1070/1094], Loss: 0.2530\n",
      "Epoch [4/5], Step [1080/1094], Loss: 0.2888\n",
      "Epoch [4/5], Step [1090/1094], Loss: 0.3355\n",
      "Epoch [4/5], Average Loss: 0.4159, Accuracy: 0.8507\n",
      "Class 0: Precision = 0.7895, Recall = 0.7830, F1 Score = 0.7862\n",
      "Class 1: Precision = 0.9701, Recall = 0.9738, F1 Score = 0.9720\n",
      "Class 2: Precision = 0.8561, Recall = 0.8652, F1 Score = 0.8606\n",
      "Class 3: Precision = 0.7484, Recall = 0.8166, F1 Score = 0.7810\n",
      "Class 4: Precision = 0.8692, Recall = 0.9040, F1 Score = 0.8863\n",
      "Class 5: Precision = 0.7888, Recall = 0.6894, F1 Score = 0.7358\n",
      "Class 6: Precision = 0.9336, Recall = 0.9230, F1 Score = 0.9283\n",
      "Epoch [5/5], Step [10/1094], Loss: 0.3111\n",
      "Epoch [5/5], Step [20/1094], Loss: 0.3944\n",
      "Epoch [5/5], Step [30/1094], Loss: 0.4980\n",
      "Epoch [5/5], Step [40/1094], Loss: 0.6448\n",
      "Epoch [5/5], Step [50/1094], Loss: 0.4218\n",
      "Epoch [5/5], Step [60/1094], Loss: 1.0415\n",
      "Epoch [5/5], Step [70/1094], Loss: 0.3230\n",
      "Epoch [5/5], Step [80/1094], Loss: 0.4675\n",
      "Epoch [5/5], Step [90/1094], Loss: 0.6164\n",
      "Epoch [5/5], Step [100/1094], Loss: 0.5748\n",
      "Epoch [5/5], Step [110/1094], Loss: 0.4911\n",
      "Epoch [5/5], Step [120/1094], Loss: 0.2879\n",
      "Epoch [5/5], Step [130/1094], Loss: 0.3439\n",
      "Epoch [5/5], Step [140/1094], Loss: 0.2544\n",
      "Epoch [5/5], Step [150/1094], Loss: 0.4814\n",
      "Epoch [5/5], Step [160/1094], Loss: 0.3942\n",
      "Epoch [5/5], Step [170/1094], Loss: 0.1893\n",
      "Epoch [5/5], Step [180/1094], Loss: 0.3615\n",
      "Epoch [5/5], Step [190/1094], Loss: 0.3458\n",
      "Epoch [5/5], Step [200/1094], Loss: 0.2745\n",
      "Epoch [5/5], Step [210/1094], Loss: 0.3114\n",
      "Epoch [5/5], Step [220/1094], Loss: 0.4472\n",
      "Epoch [5/5], Step [230/1094], Loss: 0.3437\n",
      "Epoch [5/5], Step [240/1094], Loss: 0.6784\n",
      "Epoch [5/5], Step [250/1094], Loss: 0.5684\n",
      "Epoch [5/5], Step [260/1094], Loss: 0.3352\n",
      "Epoch [5/5], Step [270/1094], Loss: 0.4721\n",
      "Epoch [5/5], Step [280/1094], Loss: 0.4878\n",
      "Epoch [5/5], Step [290/1094], Loss: 0.4253\n",
      "Epoch [5/5], Step [300/1094], Loss: 0.4727\n",
      "Epoch [5/5], Step [310/1094], Loss: 0.6589\n",
      "Epoch [5/5], Step [320/1094], Loss: 0.3389\n",
      "Epoch [5/5], Step [330/1094], Loss: 0.4148\n",
      "Epoch [5/5], Step [340/1094], Loss: 0.3750\n",
      "Epoch [5/5], Step [350/1094], Loss: 0.2986\n",
      "Epoch [5/5], Step [360/1094], Loss: 0.6619\n",
      "Epoch [5/5], Step [370/1094], Loss: 0.5602\n",
      "Epoch [5/5], Step [380/1094], Loss: 0.3501\n",
      "Epoch [5/5], Step [390/1094], Loss: 0.4397\n",
      "Epoch [5/5], Step [400/1094], Loss: 0.3498\n",
      "Epoch [5/5], Step [410/1094], Loss: 0.3110\n",
      "Epoch [5/5], Step [420/1094], Loss: 0.3147\n",
      "Epoch [5/5], Step [430/1094], Loss: 0.4246\n",
      "Epoch [5/5], Step [440/1094], Loss: 0.4142\n",
      "Epoch [5/5], Step [450/1094], Loss: 0.3411\n",
      "Epoch [5/5], Step [460/1094], Loss: 0.4523\n",
      "Epoch [5/5], Step [470/1094], Loss: 0.2501\n",
      "Epoch [5/5], Step [480/1094], Loss: 0.4178\n",
      "Epoch [5/5], Step [490/1094], Loss: 0.3868\n",
      "Epoch [5/5], Step [500/1094], Loss: 0.4246\n",
      "Epoch [5/5], Step [510/1094], Loss: 0.3628\n",
      "Epoch [5/5], Step [520/1094], Loss: 0.6452\n",
      "Epoch [5/5], Step [530/1094], Loss: 0.4871\n",
      "Epoch [5/5], Step [540/1094], Loss: 0.4732\n",
      "Epoch [5/5], Step [550/1094], Loss: 0.1871\n",
      "Epoch [5/5], Step [560/1094], Loss: 0.5381\n",
      "Epoch [5/5], Step [570/1094], Loss: 0.3209\n",
      "Epoch [5/5], Step [580/1094], Loss: 0.5407\n",
      "Epoch [5/5], Step [590/1094], Loss: 0.2826\n",
      "Epoch [5/5], Step [600/1094], Loss: 0.4487\n",
      "Epoch [5/5], Step [610/1094], Loss: 0.2900\n",
      "Epoch [5/5], Step [620/1094], Loss: 0.3358\n",
      "Epoch [5/5], Step [630/1094], Loss: 0.4012\n",
      "Epoch [5/5], Step [640/1094], Loss: 0.5428\n",
      "Epoch [5/5], Step [650/1094], Loss: 0.8256\n",
      "Epoch [5/5], Step [660/1094], Loss: 0.2843\n",
      "Epoch [5/5], Step [670/1094], Loss: 0.2212\n",
      "Epoch [5/5], Step [680/1094], Loss: 0.3758\n",
      "Epoch [5/5], Step [690/1094], Loss: 0.3808\n",
      "Epoch [5/5], Step [700/1094], Loss: 0.4182\n",
      "Epoch [5/5], Step [710/1094], Loss: 0.3091\n",
      "Epoch [5/5], Step [720/1094], Loss: 0.3023\n",
      "Epoch [5/5], Step [730/1094], Loss: 0.1971\n",
      "Epoch [5/5], Step [740/1094], Loss: 0.2622\n",
      "Epoch [5/5], Step [750/1094], Loss: 0.4324\n",
      "Epoch [5/5], Step [760/1094], Loss: 0.3513\n",
      "Epoch [5/5], Step [770/1094], Loss: 0.3037\n",
      "Epoch [5/5], Step [780/1094], Loss: 0.2578\n",
      "Epoch [5/5], Step [790/1094], Loss: 0.1872\n",
      "Epoch [5/5], Step [800/1094], Loss: 0.5885\n",
      "Epoch [5/5], Step [810/1094], Loss: 0.5350\n",
      "Epoch [5/5], Step [820/1094], Loss: 0.3057\n",
      "Epoch [5/5], Step [830/1094], Loss: 0.2744\n",
      "Epoch [5/5], Step [840/1094], Loss: 0.5746\n",
      "Epoch [5/5], Step [850/1094], Loss: 0.5345\n",
      "Epoch [5/5], Step [860/1094], Loss: 0.8775\n",
      "Epoch [5/5], Step [870/1094], Loss: 0.4832\n",
      "Epoch [5/5], Step [880/1094], Loss: 0.4534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [890/1094], Loss: 0.2800\n",
      "Epoch [5/5], Step [900/1094], Loss: 0.4499\n",
      "Epoch [5/5], Step [910/1094], Loss: 0.5055\n",
      "Epoch [5/5], Step [920/1094], Loss: 0.4368\n",
      "Epoch [5/5], Step [930/1094], Loss: 0.3402\n",
      "Epoch [5/5], Step [940/1094], Loss: 0.4653\n",
      "Epoch [5/5], Step [950/1094], Loss: 0.3260\n",
      "Epoch [5/5], Step [960/1094], Loss: 0.4947\n",
      "Epoch [5/5], Step [970/1094], Loss: 0.4295\n",
      "Epoch [5/5], Step [980/1094], Loss: 0.4105\n",
      "Epoch [5/5], Step [990/1094], Loss: 0.2305\n",
      "Epoch [5/5], Step [1000/1094], Loss: 0.3259\n",
      "Epoch [5/5], Step [1010/1094], Loss: 0.5336\n",
      "Epoch [5/5], Step [1020/1094], Loss: 0.5123\n",
      "Epoch [5/5], Step [1030/1094], Loss: 0.3820\n",
      "Epoch [5/5], Step [1040/1094], Loss: 0.3015\n",
      "Epoch [5/5], Step [1050/1094], Loss: 0.4996\n",
      "Epoch [5/5], Step [1060/1094], Loss: 0.3119\n",
      "Epoch [5/5], Step [1070/1094], Loss: 0.5029\n",
      "Epoch [5/5], Step [1080/1094], Loss: 0.5742\n",
      "Epoch [5/5], Step [1090/1094], Loss: 0.6207\n",
      "Epoch [5/5], Average Loss: 0.4097, Accuracy: 0.8517\n",
      "Class 0: Precision = 0.7930, Recall = 0.7822, F1 Score = 0.7876\n",
      "Class 1: Precision = 0.9716, Recall = 0.9732, F1 Score = 0.9724\n",
      "Class 2: Precision = 0.8579, Recall = 0.8680, F1 Score = 0.8629\n",
      "Class 3: Precision = 0.7509, Recall = 0.8288, F1 Score = 0.7879\n",
      "Class 4: Precision = 0.8637, Recall = 0.8998, F1 Score = 0.8814\n",
      "Class 5: Precision = 0.8034, Recall = 0.6896, F1 Score = 0.7421\n",
      "Class 6: Precision = 0.9241, Recall = 0.9206, F1 Score = 0.9224\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Constants\n",
    "NumPerElement = 1000  # Limit the number of samples to 10 per folder (for testing)\n",
    "sequence_length = 300  # The length of each sequence (number of time steps)\n",
    "\n",
    "input_size = 2  # Real and Imaginary components\n",
    "hidden_size = 128  # Number of hidden units per LSTM layer\n",
    "num_classes = 7  # Number of output classes\n",
    "num_layers = 3  # Number of stacked LSTM layers\n",
    "\n",
    "\n",
    "# Folder paths and corresponding labels\n",
    "folders_and_labels = [\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/4QAM/AWGN',0),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/4QAM/ImpulseNoise',0),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/4QAM/PinkNoise',0),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/4QAM/RayleighNoise',0),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/4QAM/RicianNoise',0),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM/AWGN',1),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM/ImpulseNoise',1),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM/PinkNoise',1),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM/RayleighNoise',1),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM/RicianNoise',1),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM-Flipped/AWGN',2),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM-Flipped/ImpulseNoise',2),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM-Flipped/PinkNoise',2),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM-Flipped/RayleighNoise',2),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM-Flipped/RicianNoise',2),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QPSK/AWGN',3),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QPSK/ImpulseNoise',3),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QPSK/PinkNoise',3),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QPSK/RayleighNoise',3),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QPSK/RicianNoise',3),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK/AWGN',4),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK/ImpulseNoise',4),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK/PinkNoise',4),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK/RayleighNoise',4),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK/RicianNoise',4),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK-Flipped/AWGN',5),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK-Flipped/ImpulseNoise',5),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK-Flipped/PinkNoise',5),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK-Flipped/RayleighNoise',5),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK-Flipped/RicianNoise',5),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/QPSK/AWGN',6),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/QPSK/ImpulseNoise',6),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/QPSK/PinkNoise',6),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/QPSK/RayleighNoise',6),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/QPSK/RicianNoise',6),\n",
    "]\n",
    "\n",
    "# Prepare data tensors\n",
    "im = torch.zeros(NumPerElement * len(folders_and_labels), sequence_length, input_size)  # (samples, time_steps, features)\n",
    "label = torch.zeros(NumPerElement * len(folders_and_labels))\n",
    "\n",
    "# Counters for the samples processed\n",
    "count = -1\n",
    "\n",
    "# Loop through each folder and load the data\n",
    "for folder_dir, folder_label in folders_and_labels:\n",
    "    count_in_folder = 0\n",
    "    for images in os.listdir(folder_dir):\n",
    "        if images != '.DS_Store' and count_in_folder < NumPerElement:\n",
    "            # Load the .mat file\n",
    "            AA = sio.loadmat(os.path.join(folder_dir, images))\n",
    "            count_in_folder += 1\n",
    "            count += 1\n",
    "            for key, value in AA.items():\n",
    "                if isinstance(value, np.ndarray) and np.iscomplexobj(value):\n",
    "                    im[count, :, 0] = torch.from_numpy(value.real[0, :])  # Real part\n",
    "                    im[count, :, 1] = torch.from_numpy(value.imag[0, :].copy())  # Imaginary part\n",
    "                label[count] = folder_label\n",
    "\n",
    "        if count_in_folder >= NumPerElement:  # Stop processing after 10 files per folder\n",
    "            break\n",
    "\n",
    "# Verify the data shape and labels\n",
    "print(f\"Total samples loaded: {count + 1}\")\n",
    "print(f\"Labels for first few samples: {label[:10]}\")\n",
    "\n",
    "# Define the Sequential LSTM Model\n",
    "model = nn.Sequential(\n",
    "    nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True),  # Stacked LSTM layers without dropout\n",
    "    nn.Linear(hidden_size, num_classes)  # Fully connected layer for classification\n",
    ")\n",
    "\n",
    "# Define the device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to the device\n",
    "model = model.to(device)\n",
    "\n",
    "# Convert to dataset and DataLoader\n",
    "dataset = TensorDataset(im, label)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Ensure the model is in training mode\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i, (data, labels) in enumerate(dataloader):\n",
    "        # Move data and labels to the same device as the model\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        lstm_out, _ = model[0](data)  # Only pass through LSTM layer\n",
    "        outputs = model[1](lstm_out[:, -1, :])  # Use output from last time step (last hidden state)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels.long())  # Ensure labels are of type long for classification\n",
    "        total_loss += loss.item() * data.size(0)  # Accumulate loss (weighted by batch size)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        # Collect all predictions and labels for metrics\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "        total_samples += data.size(0)\n",
    "\n",
    "        if (i+1) % 10 == 0:  # Print every 10 batches\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Convert all_preds and all_labels to numpy arrays\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    # Calculate Precision, Recall, F1 for each class, with zero_division set to 0\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average=None, labels=np.arange(num_classes), zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average=None, labels=np.arange(num_classes), zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average=None, labels=np.arange(num_classes), zero_division=0)\n",
    "\n",
    "    # Print the detailed metrics for each class\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {total_loss / total_samples:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    for i in range(num_classes):\n",
    "        print(f\"Class {i}: Precision = {precision[i]:.4f}, Recall = {recall[i]:.4f}, F1 Score = {f1[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6327902a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdef9d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7576b18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples loaded: 35000\n",
      "Labels for first few samples: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Epoch [1/10], Step [10/1094], Loss: 1.8991\n",
      "Epoch [1/10], Step [20/1094], Loss: 1.3588\n",
      "Epoch [1/10], Step [30/1094], Loss: 1.0488\n",
      "Epoch [1/10], Step [40/1094], Loss: 0.8190\n",
      "Epoch [1/10], Step [50/1094], Loss: 0.6895\n",
      "Epoch [1/10], Step [60/1094], Loss: 0.6875\n",
      "Epoch [1/10], Step [70/1094], Loss: 0.8302\n",
      "Epoch [1/10], Step [80/1094], Loss: 0.6546\n",
      "Epoch [1/10], Step [90/1094], Loss: 0.4984\n",
      "Epoch [1/10], Step [100/1094], Loss: 0.4419\n",
      "Epoch [1/10], Step [110/1094], Loss: 0.4563\n",
      "Epoch [1/10], Step [120/1094], Loss: 0.5440\n",
      "Epoch [1/10], Step [130/1094], Loss: 0.4542\n",
      "Epoch [1/10], Step [140/1094], Loss: 0.7560\n",
      "Epoch [1/10], Step [150/1094], Loss: 0.5661\n",
      "Epoch [1/10], Step [160/1094], Loss: 0.4915\n",
      "Epoch [1/10], Step [170/1094], Loss: 0.5513\n",
      "Epoch [1/10], Step [180/1094], Loss: 0.5827\n",
      "Epoch [1/10], Step [190/1094], Loss: 0.7946\n",
      "Epoch [1/10], Step [200/1094], Loss: 0.4629\n",
      "Epoch [1/10], Step [210/1094], Loss: 0.6095\n",
      "Epoch [1/10], Step [220/1094], Loss: 0.5438\n",
      "Epoch [1/10], Step [230/1094], Loss: 0.5050\n",
      "Epoch [1/10], Step [240/1094], Loss: 0.9404\n",
      "Epoch [1/10], Step [250/1094], Loss: 0.4527\n",
      "Epoch [1/10], Step [260/1094], Loss: 0.6014\n",
      "Epoch [1/10], Step [270/1094], Loss: 0.3999\n",
      "Epoch [1/10], Step [280/1094], Loss: 0.2462\n",
      "Epoch [1/10], Step [290/1094], Loss: 0.5311\n",
      "Epoch [1/10], Step [300/1094], Loss: 0.6206\n",
      "Epoch [1/10], Step [310/1094], Loss: 0.4894\n",
      "Epoch [1/10], Step [320/1094], Loss: 0.4941\n",
      "Epoch [1/10], Step [330/1094], Loss: 0.4593\n",
      "Epoch [1/10], Step [340/1094], Loss: 0.4068\n",
      "Epoch [1/10], Step [350/1094], Loss: 0.3058\n",
      "Epoch [1/10], Step [360/1094], Loss: 0.6058\n",
      "Epoch [1/10], Step [370/1094], Loss: 0.2831\n",
      "Epoch [1/10], Step [380/1094], Loss: 0.4936\n",
      "Epoch [1/10], Step [390/1094], Loss: 0.7132\n",
      "Epoch [1/10], Step [400/1094], Loss: 0.9219\n",
      "Epoch [1/10], Step [410/1094], Loss: 0.4513\n",
      "Epoch [1/10], Step [420/1094], Loss: 0.3931\n",
      "Epoch [1/10], Step [430/1094], Loss: 0.5592\n",
      "Epoch [1/10], Step [440/1094], Loss: 0.6934\n",
      "Epoch [1/10], Step [450/1094], Loss: 0.3418\n",
      "Epoch [1/10], Step [460/1094], Loss: 0.3300\n",
      "Epoch [1/10], Step [470/1094], Loss: 0.6512\n",
      "Epoch [1/10], Step [480/1094], Loss: 0.2775\n",
      "Epoch [1/10], Step [490/1094], Loss: 0.4044\n",
      "Epoch [1/10], Step [500/1094], Loss: 0.7109\n",
      "Epoch [1/10], Step [510/1094], Loss: 0.5596\n",
      "Epoch [1/10], Step [520/1094], Loss: 0.4775\n",
      "Epoch [1/10], Step [530/1094], Loss: 0.6400\n",
      "Epoch [1/10], Step [540/1094], Loss: 0.3882\n",
      "Epoch [1/10], Step [550/1094], Loss: 0.5680\n",
      "Epoch [1/10], Step [560/1094], Loss: 0.4756\n",
      "Epoch [1/10], Step [570/1094], Loss: 0.2823\n",
      "Epoch [1/10], Step [580/1094], Loss: 0.5153\n",
      "Epoch [1/10], Step [590/1094], Loss: 0.2616\n",
      "Epoch [1/10], Step [600/1094], Loss: 0.8019\n",
      "Epoch [1/10], Step [610/1094], Loss: 0.5680\n",
      "Epoch [1/10], Step [620/1094], Loss: 0.3872\n",
      "Epoch [1/10], Step [630/1094], Loss: 0.4986\n",
      "Epoch [1/10], Step [640/1094], Loss: 0.7208\n",
      "Epoch [1/10], Step [650/1094], Loss: 0.3684\n",
      "Epoch [1/10], Step [660/1094], Loss: 0.3653\n",
      "Epoch [1/10], Step [670/1094], Loss: 0.6578\n",
      "Epoch [1/10], Step [680/1094], Loss: 0.5618\n",
      "Epoch [1/10], Step [690/1094], Loss: 0.5502\n",
      "Epoch [1/10], Step [700/1094], Loss: 0.3781\n",
      "Epoch [1/10], Step [710/1094], Loss: 0.5578\n",
      "Epoch [1/10], Step [720/1094], Loss: 0.4002\n",
      "Epoch [1/10], Step [730/1094], Loss: 0.5574\n",
      "Epoch [1/10], Step [740/1094], Loss: 0.5300\n",
      "Epoch [1/10], Step [750/1094], Loss: 0.3379\n",
      "Epoch [1/10], Step [760/1094], Loss: 0.3535\n",
      "Epoch [1/10], Step [770/1094], Loss: 0.6682\n",
      "Epoch [1/10], Step [780/1094], Loss: 0.4049\n",
      "Epoch [1/10], Step [790/1094], Loss: 0.3093\n",
      "Epoch [1/10], Step [800/1094], Loss: 0.5653\n",
      "Epoch [1/10], Step [810/1094], Loss: 0.5388\n",
      "Epoch [1/10], Step [820/1094], Loss: 0.2785\n",
      "Epoch [1/10], Step [830/1094], Loss: 0.3473\n",
      "Epoch [1/10], Step [840/1094], Loss: 0.3183\n",
      "Epoch [1/10], Step [850/1094], Loss: 0.3156\n",
      "Epoch [1/10], Step [860/1094], Loss: 0.4402\n",
      "Epoch [1/10], Step [870/1094], Loss: 0.3581\n",
      "Epoch [1/10], Step [880/1094], Loss: 0.2812\n",
      "Epoch [1/10], Step [890/1094], Loss: 0.4791\n",
      "Epoch [1/10], Step [900/1094], Loss: 0.3823\n",
      "Epoch [1/10], Step [910/1094], Loss: 0.2416\n",
      "Epoch [1/10], Step [920/1094], Loss: 0.2478\n",
      "Epoch [1/10], Step [930/1094], Loss: 0.9115\n",
      "Epoch [1/10], Step [940/1094], Loss: 0.6868\n",
      "Epoch [1/10], Step [950/1094], Loss: 0.5685\n",
      "Epoch [1/10], Step [960/1094], Loss: 0.4217\n",
      "Epoch [1/10], Step [970/1094], Loss: 0.6045\n",
      "Epoch [1/10], Step [980/1094], Loss: 0.5314\n",
      "Epoch [1/10], Step [990/1094], Loss: 0.1253\n",
      "Epoch [1/10], Step [1000/1094], Loss: 0.4370\n",
      "Epoch [1/10], Step [1010/1094], Loss: 0.3666\n",
      "Epoch [1/10], Step [1020/1094], Loss: 0.7064\n",
      "Epoch [1/10], Step [1030/1094], Loss: 0.5211\n",
      "Epoch [1/10], Step [1040/1094], Loss: 0.4168\n",
      "Epoch [1/10], Step [1050/1094], Loss: 0.5421\n",
      "Epoch [1/10], Step [1060/1094], Loss: 0.2911\n",
      "Epoch [1/10], Step [1070/1094], Loss: 0.3335\n",
      "Epoch [1/10], Step [1080/1094], Loss: 0.4451\n",
      "Epoch [1/10], Step [1090/1094], Loss: 0.5674\n",
      "Epoch [1/10], Average Loss: 0.5346, Accuracy: 0.8102\n",
      "Class 0: Precision = 0.7156, Recall = 0.7454, F1 Score = 0.7302\n",
      "Class 1: Precision = 0.9263, Recall = 0.9530, F1 Score = 0.9395\n",
      "Class 2: Precision = 0.8238, Recall = 0.8144, F1 Score = 0.8191\n",
      "Class 3: Precision = 0.7163, Recall = 0.7836, F1 Score = 0.7484\n",
      "Class 4: Precision = 0.8263, Recall = 0.8570, F1 Score = 0.8414\n",
      "Class 5: Precision = 0.7714, Recall = 0.6404, F1 Score = 0.6998\n",
      "Class 6: Precision = 0.8961, Recall = 0.8778, F1 Score = 0.8868\n",
      "Epoch [2/10], Step [10/1094], Loss: 0.2568\n",
      "Epoch [2/10], Step [20/1094], Loss: 0.6475\n",
      "Epoch [2/10], Step [30/1094], Loss: 0.3266\n",
      "Epoch [2/10], Step [40/1094], Loss: 0.4110\n",
      "Epoch [2/10], Step [50/1094], Loss: 0.3408\n",
      "Epoch [2/10], Step [60/1094], Loss: 0.5806\n",
      "Epoch [2/10], Step [70/1094], Loss: 0.2938\n",
      "Epoch [2/10], Step [80/1094], Loss: 0.5470\n",
      "Epoch [2/10], Step [90/1094], Loss: 0.3187\n",
      "Epoch [2/10], Step [100/1094], Loss: 0.4421\n",
      "Epoch [2/10], Step [110/1094], Loss: 0.6966\n",
      "Epoch [2/10], Step [120/1094], Loss: 0.5383\n",
      "Epoch [2/10], Step [130/1094], Loss: 0.4080\n",
      "Epoch [2/10], Step [140/1094], Loss: 0.4002\n",
      "Epoch [2/10], Step [150/1094], Loss: 0.3212\n",
      "Epoch [2/10], Step [160/1094], Loss: 0.3947\n",
      "Epoch [2/10], Step [170/1094], Loss: 0.6119\n",
      "Epoch [2/10], Step [180/1094], Loss: 0.2734\n",
      "Epoch [2/10], Step [190/1094], Loss: 0.3879\n",
      "Epoch [2/10], Step [200/1094], Loss: 0.4709\n",
      "Epoch [2/10], Step [210/1094], Loss: 0.4474\n",
      "Epoch [2/10], Step [220/1094], Loss: 0.3365\n",
      "Epoch [2/10], Step [230/1094], Loss: 0.6152\n",
      "Epoch [2/10], Step [240/1094], Loss: 0.3385\n",
      "Epoch [2/10], Step [250/1094], Loss: 0.3884\n",
      "Epoch [2/10], Step [260/1094], Loss: 0.3123\n",
      "Epoch [2/10], Step [270/1094], Loss: 1.7181\n",
      "Epoch [2/10], Step [280/1094], Loss: 0.5197\n",
      "Epoch [2/10], Step [290/1094], Loss: 0.7282\n",
      "Epoch [2/10], Step [300/1094], Loss: 0.7478\n",
      "Epoch [2/10], Step [310/1094], Loss: 0.9608\n",
      "Epoch [2/10], Step [320/1094], Loss: 0.4924\n",
      "Epoch [2/10], Step [330/1094], Loss: 0.2383\n",
      "Epoch [2/10], Step [340/1094], Loss: 0.4520\n",
      "Epoch [2/10], Step [350/1094], Loss: 0.3270\n",
      "Epoch [2/10], Step [360/1094], Loss: 0.9830\n",
      "Epoch [2/10], Step [370/1094], Loss: 0.6348\n",
      "Epoch [2/10], Step [380/1094], Loss: 0.3149\n",
      "Epoch [2/10], Step [390/1094], Loss: 0.5710\n",
      "Epoch [2/10], Step [400/1094], Loss: 0.6877\n",
      "Epoch [2/10], Step [410/1094], Loss: 0.3223\n",
      "Epoch [2/10], Step [420/1094], Loss: 0.4762\n",
      "Epoch [2/10], Step [430/1094], Loss: 0.6022\n",
      "Epoch [2/10], Step [440/1094], Loss: 0.5329\n",
      "Epoch [2/10], Step [450/1094], Loss: 0.4466\n",
      "Epoch [2/10], Step [460/1094], Loss: 0.6670\n",
      "Epoch [2/10], Step [470/1094], Loss: 0.4337\n",
      "Epoch [2/10], Step [480/1094], Loss: 0.3746\n",
      "Epoch [2/10], Step [490/1094], Loss: 0.4174\n",
      "Epoch [2/10], Step [500/1094], Loss: 0.2636\n",
      "Epoch [2/10], Step [510/1094], Loss: 0.4535\n",
      "Epoch [2/10], Step [520/1094], Loss: 0.6169\n",
      "Epoch [2/10], Step [530/1094], Loss: 0.2572\n",
      "Epoch [2/10], Step [540/1094], Loss: 0.3203\n",
      "Epoch [2/10], Step [550/1094], Loss: 0.6926\n",
      "Epoch [2/10], Step [560/1094], Loss: 0.5939\n",
      "Epoch [2/10], Step [570/1094], Loss: 0.3670\n",
      "Epoch [2/10], Step [580/1094], Loss: 0.5150\n",
      "Epoch [2/10], Step [590/1094], Loss: 0.6453\n",
      "Epoch [2/10], Step [600/1094], Loss: 0.3331\n",
      "Epoch [2/10], Step [610/1094], Loss: 0.6645\n",
      "Epoch [2/10], Step [620/1094], Loss: 0.3500\n",
      "Epoch [2/10], Step [630/1094], Loss: 0.5329\n",
      "Epoch [2/10], Step [640/1094], Loss: 0.3636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Step [650/1094], Loss: 0.6089\n",
      "Epoch [2/10], Step [660/1094], Loss: 0.5007\n",
      "Epoch [2/10], Step [670/1094], Loss: 1.5829\n",
      "Epoch [2/10], Step [680/1094], Loss: 0.7758\n",
      "Epoch [2/10], Step [690/1094], Loss: 0.6286\n",
      "Epoch [2/10], Step [700/1094], Loss: 0.7564\n",
      "Epoch [2/10], Step [710/1094], Loss: 0.6565\n",
      "Epoch [2/10], Step [720/1094], Loss: 0.7567\n",
      "Epoch [2/10], Step [730/1094], Loss: 0.5518\n",
      "Epoch [2/10], Step [740/1094], Loss: 0.6753\n",
      "Epoch [2/10], Step [750/1094], Loss: 0.6112\n",
      "Epoch [2/10], Step [760/1094], Loss: 0.6604\n",
      "Epoch [2/10], Step [770/1094], Loss: 0.5033\n",
      "Epoch [2/10], Step [780/1094], Loss: 0.8037\n",
      "Epoch [2/10], Step [790/1094], Loss: 0.4376\n",
      "Epoch [2/10], Step [800/1094], Loss: 0.6568\n",
      "Epoch [2/10], Step [810/1094], Loss: 0.5485\n",
      "Epoch [2/10], Step [820/1094], Loss: 0.5548\n",
      "Epoch [2/10], Step [830/1094], Loss: 0.6067\n",
      "Epoch [2/10], Step [840/1094], Loss: 0.6742\n",
      "Epoch [2/10], Step [850/1094], Loss: 0.4557\n",
      "Epoch [2/10], Step [860/1094], Loss: 0.6092\n",
      "Epoch [2/10], Step [870/1094], Loss: 0.5219\n",
      "Epoch [2/10], Step [880/1094], Loss: 0.7045\n",
      "Epoch [2/10], Step [890/1094], Loss: 0.4707\n",
      "Epoch [2/10], Step [900/1094], Loss: 0.7472\n",
      "Epoch [2/10], Step [910/1094], Loss: 0.5325\n",
      "Epoch [2/10], Step [920/1094], Loss: 0.4987\n",
      "Epoch [2/10], Step [930/1094], Loss: 0.3990\n",
      "Epoch [2/10], Step [940/1094], Loss: 0.3450\n",
      "Epoch [2/10], Step [950/1094], Loss: 0.3724\n",
      "Epoch [2/10], Step [960/1094], Loss: 0.5390\n",
      "Epoch [2/10], Step [970/1094], Loss: 0.4757\n",
      "Epoch [2/10], Step [980/1094], Loss: 0.1968\n",
      "Epoch [2/10], Step [990/1094], Loss: 0.2641\n",
      "Epoch [2/10], Step [1000/1094], Loss: 0.5074\n",
      "Epoch [2/10], Step [1010/1094], Loss: 0.6662\n",
      "Epoch [2/10], Step [1020/1094], Loss: 0.5163\n",
      "Epoch [2/10], Step [1030/1094], Loss: 0.5411\n",
      "Epoch [2/10], Step [1040/1094], Loss: 0.3470\n",
      "Epoch [2/10], Step [1050/1094], Loss: 0.2139\n",
      "Epoch [2/10], Step [1060/1094], Loss: 0.6662\n",
      "Epoch [2/10], Step [1070/1094], Loss: 0.4359\n",
      "Epoch [2/10], Step [1080/1094], Loss: 0.4098\n",
      "Epoch [2/10], Step [1090/1094], Loss: 0.1979\n",
      "Epoch [2/10], Average Loss: 0.5036, Accuracy: 0.8245\n",
      "Class 0: Precision = 0.7865, Recall = 0.7720, F1 Score = 0.7792\n",
      "Class 1: Precision = 0.9188, Recall = 0.9636, F1 Score = 0.9406\n",
      "Class 2: Precision = 0.8538, Recall = 0.8586, F1 Score = 0.8562\n",
      "Class 3: Precision = 0.7202, Recall = 0.7818, F1 Score = 0.7497\n",
      "Class 4: Precision = 0.8456, Recall = 0.8410, F1 Score = 0.8433\n",
      "Class 5: Precision = 0.7331, Recall = 0.6690, F1 Score = 0.6996\n",
      "Class 6: Precision = 0.9117, Recall = 0.8854, F1 Score = 0.8983\n",
      "Epoch [3/10], Step [10/1094], Loss: 0.6322\n",
      "Epoch [3/10], Step [20/1094], Loss: 0.8510\n",
      "Epoch [3/10], Step [30/1094], Loss: 0.5980\n",
      "Epoch [3/10], Step [40/1094], Loss: 0.2838\n",
      "Epoch [3/10], Step [50/1094], Loss: 0.3162\n",
      "Epoch [3/10], Step [60/1094], Loss: 0.6159\n",
      "Epoch [3/10], Step [70/1094], Loss: 0.3305\n",
      "Epoch [3/10], Step [80/1094], Loss: 0.3628\n",
      "Epoch [3/10], Step [90/1094], Loss: 0.3172\n",
      "Epoch [3/10], Step [100/1094], Loss: 0.4968\n",
      "Epoch [3/10], Step [110/1094], Loss: 0.6357\n",
      "Epoch [3/10], Step [120/1094], Loss: 0.3455\n",
      "Epoch [3/10], Step [130/1094], Loss: 0.3556\n",
      "Epoch [3/10], Step [140/1094], Loss: 0.4725\n",
      "Epoch [3/10], Step [150/1094], Loss: 0.5322\n",
      "Epoch [3/10], Step [160/1094], Loss: 0.3278\n",
      "Epoch [3/10], Step [170/1094], Loss: 0.4458\n",
      "Epoch [3/10], Step [180/1094], Loss: 0.3689\n",
      "Epoch [3/10], Step [190/1094], Loss: 0.5787\n",
      "Epoch [3/10], Step [200/1094], Loss: 0.4563\n",
      "Epoch [3/10], Step [210/1094], Loss: 0.4754\n",
      "Epoch [3/10], Step [220/1094], Loss: 0.2752\n",
      "Epoch [3/10], Step [230/1094], Loss: 0.6240\n",
      "Epoch [3/10], Step [240/1094], Loss: 0.4630\n",
      "Epoch [3/10], Step [250/1094], Loss: 0.3399\n",
      "Epoch [3/10], Step [260/1094], Loss: 0.3934\n",
      "Epoch [3/10], Step [270/1094], Loss: 0.2842\n",
      "Epoch [3/10], Step [280/1094], Loss: 0.5510\n",
      "Epoch [3/10], Step [290/1094], Loss: 0.4189\n",
      "Epoch [3/10], Step [300/1094], Loss: 0.8405\n",
      "Epoch [3/10], Step [310/1094], Loss: 0.2379\n",
      "Epoch [3/10], Step [320/1094], Loss: 0.5192\n",
      "Epoch [3/10], Step [330/1094], Loss: 0.6559\n",
      "Epoch [3/10], Step [340/1094], Loss: 0.4157\n",
      "Epoch [3/10], Step [350/1094], Loss: 0.7599\n",
      "Epoch [3/10], Step [360/1094], Loss: 0.5142\n",
      "Epoch [3/10], Step [370/1094], Loss: 0.3521\n",
      "Epoch [3/10], Step [380/1094], Loss: 0.5215\n",
      "Epoch [3/10], Step [390/1094], Loss: 0.2405\n",
      "Epoch [3/10], Step [400/1094], Loss: 0.4011\n",
      "Epoch [3/10], Step [410/1094], Loss: 0.4601\n",
      "Epoch [3/10], Step [420/1094], Loss: 0.4812\n",
      "Epoch [3/10], Step [430/1094], Loss: 0.3281\n",
      "Epoch [3/10], Step [440/1094], Loss: 0.5373\n",
      "Epoch [3/10], Step [450/1094], Loss: 0.4587\n",
      "Epoch [3/10], Step [460/1094], Loss: 0.4397\n",
      "Epoch [3/10], Step [470/1094], Loss: 0.4551\n",
      "Epoch [3/10], Step [480/1094], Loss: 0.4153\n",
      "Epoch [3/10], Step [490/1094], Loss: 0.2841\n",
      "Epoch [3/10], Step [500/1094], Loss: 0.5397\n",
      "Epoch [3/10], Step [510/1094], Loss: 0.5511\n",
      "Epoch [3/10], Step [520/1094], Loss: 0.4358\n",
      "Epoch [3/10], Step [530/1094], Loss: 0.3548\n",
      "Epoch [3/10], Step [540/1094], Loss: 0.4195\n",
      "Epoch [3/10], Step [550/1094], Loss: 0.3060\n",
      "Epoch [3/10], Step [560/1094], Loss: 0.3502\n",
      "Epoch [3/10], Step [570/1094], Loss: 0.2300\n",
      "Epoch [3/10], Step [580/1094], Loss: 0.3873\n",
      "Epoch [3/10], Step [590/1094], Loss: 0.5302\n",
      "Epoch [3/10], Step [600/1094], Loss: 0.5306\n",
      "Epoch [3/10], Step [610/1094], Loss: 0.3635\n",
      "Epoch [3/10], Step [620/1094], Loss: 0.4988\n",
      "Epoch [3/10], Step [630/1094], Loss: 0.1397\n",
      "Epoch [3/10], Step [640/1094], Loss: 0.2589\n",
      "Epoch [3/10], Step [650/1094], Loss: 0.1883\n",
      "Epoch [3/10], Step [660/1094], Loss: 0.4022\n",
      "Epoch [3/10], Step [670/1094], Loss: 0.4098\n",
      "Epoch [3/10], Step [680/1094], Loss: 0.6434\n",
      "Epoch [3/10], Step [690/1094], Loss: 0.2767\n",
      "Epoch [3/10], Step [700/1094], Loss: 0.2732\n",
      "Epoch [3/10], Step [710/1094], Loss: 0.3325\n",
      "Epoch [3/10], Step [720/1094], Loss: 0.5363\n",
      "Epoch [3/10], Step [730/1094], Loss: 0.4889\n",
      "Epoch [3/10], Step [740/1094], Loss: 0.3629\n",
      "Epoch [3/10], Step [750/1094], Loss: 0.6705\n",
      "Epoch [3/10], Step [760/1094], Loss: 0.4418\n",
      "Epoch [3/10], Step [770/1094], Loss: 0.3268\n",
      "Epoch [3/10], Step [780/1094], Loss: 0.3211\n",
      "Epoch [3/10], Step [790/1094], Loss: 0.4547\n",
      "Epoch [3/10], Step [800/1094], Loss: 0.3831\n",
      "Epoch [3/10], Step [810/1094], Loss: 0.3450\n",
      "Epoch [3/10], Step [820/1094], Loss: 0.4115\n",
      "Epoch [3/10], Step [830/1094], Loss: 0.5025\n",
      "Epoch [3/10], Step [840/1094], Loss: 0.4333\n",
      "Epoch [3/10], Step [850/1094], Loss: 0.7876\n",
      "Epoch [3/10], Step [860/1094], Loss: 0.6379\n",
      "Epoch [3/10], Step [870/1094], Loss: 0.2907\n",
      "Epoch [3/10], Step [880/1094], Loss: 0.2337\n",
      "Epoch [3/10], Step [890/1094], Loss: 0.6102\n",
      "Epoch [3/10], Step [900/1094], Loss: 0.3125\n",
      "Epoch [3/10], Step [910/1094], Loss: 0.3581\n",
      "Epoch [3/10], Step [920/1094], Loss: 0.2416\n",
      "Epoch [3/10], Step [930/1094], Loss: 0.3576\n",
      "Epoch [3/10], Step [940/1094], Loss: 0.4797\n",
      "Epoch [3/10], Step [950/1094], Loss: 0.4594\n",
      "Epoch [3/10], Step [960/1094], Loss: 0.3707\n",
      "Epoch [3/10], Step [970/1094], Loss: 0.2886\n",
      "Epoch [3/10], Step [980/1094], Loss: 0.5317\n",
      "Epoch [3/10], Step [990/1094], Loss: 0.4052\n",
      "Epoch [3/10], Step [1000/1094], Loss: 0.2627\n",
      "Epoch [3/10], Step [1010/1094], Loss: 0.5988\n",
      "Epoch [3/10], Step [1020/1094], Loss: 0.4475\n",
      "Epoch [3/10], Step [1030/1094], Loss: 0.2979\n",
      "Epoch [3/10], Step [1040/1094], Loss: 0.6084\n",
      "Epoch [3/10], Step [1050/1094], Loss: 0.5513\n",
      "Epoch [3/10], Step [1060/1094], Loss: 0.4596\n",
      "Epoch [3/10], Step [1070/1094], Loss: 0.5995\n",
      "Epoch [3/10], Step [1080/1094], Loss: 0.2495\n",
      "Epoch [3/10], Step [1090/1094], Loss: 0.4436\n",
      "Epoch [3/10], Average Loss: 0.4287, Accuracy: 0.8515\n",
      "Class 0: Precision = 0.7825, Recall = 0.7894, F1 Score = 0.7859\n",
      "Class 1: Precision = 0.9657, Recall = 0.9734, F1 Score = 0.9695\n",
      "Class 2: Precision = 0.8588, Recall = 0.8554, F1 Score = 0.8571\n",
      "Class 3: Precision = 0.7481, Recall = 0.8244, F1 Score = 0.7844\n",
      "Class 4: Precision = 0.8710, Recall = 0.9036, F1 Score = 0.8870\n",
      "Class 5: Precision = 0.8021, Recall = 0.6892, F1 Score = 0.7414\n",
      "Class 6: Precision = 0.9355, Recall = 0.9248, F1 Score = 0.9301\n",
      "Epoch [4/10], Step [10/1094], Loss: 0.6088\n",
      "Epoch [4/10], Step [20/1094], Loss: 0.4131\n",
      "Epoch [4/10], Step [30/1094], Loss: 0.3496\n",
      "Epoch [4/10], Step [40/1094], Loss: 0.4625\n",
      "Epoch [4/10], Step [50/1094], Loss: 0.3000\n",
      "Epoch [4/10], Step [60/1094], Loss: 0.3056\n",
      "Epoch [4/10], Step [70/1094], Loss: 0.4506\n",
      "Epoch [4/10], Step [80/1094], Loss: 0.5206\n",
      "Epoch [4/10], Step [90/1094], Loss: 0.4310\n",
      "Epoch [4/10], Step [100/1094], Loss: 0.2907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Step [110/1094], Loss: 0.3965\n",
      "Epoch [4/10], Step [120/1094], Loss: 0.6074\n",
      "Epoch [4/10], Step [130/1094], Loss: 0.4563\n",
      "Epoch [4/10], Step [140/1094], Loss: 0.2877\n",
      "Epoch [4/10], Step [150/1094], Loss: 0.2364\n",
      "Epoch [4/10], Step [160/1094], Loss: 0.4800\n",
      "Epoch [4/10], Step [170/1094], Loss: 0.4053\n",
      "Epoch [4/10], Step [180/1094], Loss: 0.3927\n",
      "Epoch [4/10], Step [190/1094], Loss: 0.2620\n",
      "Epoch [4/10], Step [200/1094], Loss: 0.3501\n",
      "Epoch [4/10], Step [210/1094], Loss: 0.4240\n",
      "Epoch [4/10], Step [220/1094], Loss: 0.3138\n",
      "Epoch [4/10], Step [230/1094], Loss: 0.3414\n",
      "Epoch [4/10], Step [240/1094], Loss: 0.1195\n",
      "Epoch [4/10], Step [250/1094], Loss: 0.6797\n",
      "Epoch [4/10], Step [260/1094], Loss: 0.6759\n",
      "Epoch [4/10], Step [270/1094], Loss: 0.3606\n",
      "Epoch [4/10], Step [280/1094], Loss: 0.6565\n",
      "Epoch [4/10], Step [290/1094], Loss: 0.5052\n",
      "Epoch [4/10], Step [300/1094], Loss: 0.3337\n",
      "Epoch [4/10], Step [310/1094], Loss: 0.3782\n",
      "Epoch [4/10], Step [320/1094], Loss: 0.3714\n",
      "Epoch [4/10], Step [330/1094], Loss: 0.5830\n",
      "Epoch [4/10], Step [340/1094], Loss: 0.3032\n",
      "Epoch [4/10], Step [350/1094], Loss: 0.4132\n",
      "Epoch [4/10], Step [360/1094], Loss: 0.2128\n",
      "Epoch [4/10], Step [370/1094], Loss: 0.5362\n",
      "Epoch [4/10], Step [380/1094], Loss: 0.6400\n",
      "Epoch [4/10], Step [390/1094], Loss: 0.4167\n",
      "Epoch [4/10], Step [400/1094], Loss: 0.2434\n",
      "Epoch [4/10], Step [410/1094], Loss: 0.1643\n",
      "Epoch [4/10], Step [420/1094], Loss: 0.4205\n",
      "Epoch [4/10], Step [430/1094], Loss: 0.2312\n",
      "Epoch [4/10], Step [440/1094], Loss: 0.6414\n",
      "Epoch [4/10], Step [450/1094], Loss: 0.4760\n",
      "Epoch [4/10], Step [460/1094], Loss: 0.2171\n",
      "Epoch [4/10], Step [470/1094], Loss: 0.5922\n",
      "Epoch [4/10], Step [480/1094], Loss: 0.4182\n",
      "Epoch [4/10], Step [490/1094], Loss: 0.5520\n",
      "Epoch [4/10], Step [500/1094], Loss: 0.1968\n",
      "Epoch [4/10], Step [510/1094], Loss: 0.2709\n",
      "Epoch [4/10], Step [520/1094], Loss: 0.4797\n",
      "Epoch [4/10], Step [530/1094], Loss: 0.4028\n",
      "Epoch [4/10], Step [540/1094], Loss: 0.4773\n",
      "Epoch [4/10], Step [550/1094], Loss: 0.2603\n",
      "Epoch [4/10], Step [560/1094], Loss: 0.6320\n",
      "Epoch [4/10], Step [570/1094], Loss: 0.5818\n",
      "Epoch [4/10], Step [580/1094], Loss: 0.4549\n",
      "Epoch [4/10], Step [590/1094], Loss: 0.2671\n",
      "Epoch [4/10], Step [600/1094], Loss: 0.5815\n",
      "Epoch [4/10], Step [610/1094], Loss: 0.7700\n",
      "Epoch [4/10], Step [620/1094], Loss: 0.2316\n",
      "Epoch [4/10], Step [630/1094], Loss: 0.5304\n",
      "Epoch [4/10], Step [640/1094], Loss: 0.5004\n",
      "Epoch [4/10], Step [650/1094], Loss: 0.4352\n",
      "Epoch [4/10], Step [660/1094], Loss: 0.4460\n",
      "Epoch [4/10], Step [670/1094], Loss: 0.3614\n",
      "Epoch [4/10], Step [680/1094], Loss: 0.5245\n",
      "Epoch [4/10], Step [690/1094], Loss: 0.2835\n",
      "Epoch [4/10], Step [700/1094], Loss: 0.4299\n",
      "Epoch [4/10], Step [710/1094], Loss: 0.5237\n",
      "Epoch [4/10], Step [720/1094], Loss: 0.5304\n",
      "Epoch [4/10], Step [730/1094], Loss: 0.1996\n",
      "Epoch [4/10], Step [740/1094], Loss: 0.2393\n",
      "Epoch [4/10], Step [750/1094], Loss: 0.3501\n",
      "Epoch [4/10], Step [760/1094], Loss: 0.3904\n",
      "Epoch [4/10], Step [770/1094], Loss: 0.4956\n",
      "Epoch [4/10], Step [780/1094], Loss: 0.4413\n",
      "Epoch [4/10], Step [790/1094], Loss: 0.6051\n",
      "Epoch [4/10], Step [800/1094], Loss: 0.3823\n",
      "Epoch [4/10], Step [810/1094], Loss: 0.3337\n",
      "Epoch [4/10], Step [820/1094], Loss: 0.4249\n",
      "Epoch [4/10], Step [830/1094], Loss: 0.4346\n",
      "Epoch [4/10], Step [840/1094], Loss: 0.2932\n",
      "Epoch [4/10], Step [850/1094], Loss: 0.3577\n",
      "Epoch [4/10], Step [860/1094], Loss: 0.7769\n",
      "Epoch [4/10], Step [870/1094], Loss: 0.3620\n",
      "Epoch [4/10], Step [880/1094], Loss: 0.3210\n",
      "Epoch [4/10], Step [890/1094], Loss: 0.3191\n",
      "Epoch [4/10], Step [900/1094], Loss: 0.2410\n",
      "Epoch [4/10], Step [910/1094], Loss: 0.5390\n",
      "Epoch [4/10], Step [920/1094], Loss: 0.4194\n",
      "Epoch [4/10], Step [930/1094], Loss: 0.4894\n",
      "Epoch [4/10], Step [940/1094], Loss: 0.3690\n",
      "Epoch [4/10], Step [950/1094], Loss: 0.5923\n",
      "Epoch [4/10], Step [960/1094], Loss: 0.3548\n",
      "Epoch [4/10], Step [970/1094], Loss: 0.2911\n",
      "Epoch [4/10], Step [980/1094], Loss: 0.3930\n",
      "Epoch [4/10], Step [990/1094], Loss: 0.4156\n",
      "Epoch [4/10], Step [1000/1094], Loss: 0.6187\n",
      "Epoch [4/10], Step [1010/1094], Loss: 0.2067\n",
      "Epoch [4/10], Step [1020/1094], Loss: 0.2766\n",
      "Epoch [4/10], Step [1030/1094], Loss: 0.4148\n",
      "Epoch [4/10], Step [1040/1094], Loss: 0.3477\n",
      "Epoch [4/10], Step [1050/1094], Loss: 0.3206\n",
      "Epoch [4/10], Step [1060/1094], Loss: 0.3467\n",
      "Epoch [4/10], Step [1070/1094], Loss: 0.2231\n",
      "Epoch [4/10], Step [1080/1094], Loss: 0.3761\n",
      "Epoch [4/10], Step [1090/1094], Loss: 0.2688\n",
      "Epoch [4/10], Average Loss: 0.4140, Accuracy: 0.8520\n",
      "Class 0: Precision = 0.7841, Recall = 0.7836, F1 Score = 0.7838\n",
      "Class 1: Precision = 0.9728, Recall = 0.9716, F1 Score = 0.9722\n",
      "Class 2: Precision = 0.8549, Recall = 0.8626, F1 Score = 0.8587\n",
      "Class 3: Precision = 0.7518, Recall = 0.8274, F1 Score = 0.7878\n",
      "Class 4: Precision = 0.8674, Recall = 0.9042, F1 Score = 0.8854\n",
      "Class 5: Precision = 0.8086, Recall = 0.6912, F1 Score = 0.7453\n",
      "Class 6: Precision = 0.9280, Recall = 0.9234, F1 Score = 0.9257\n",
      "Epoch [5/10], Step [10/1094], Loss: 0.6087\n",
      "Epoch [5/10], Step [20/1094], Loss: 0.4243\n",
      "Epoch [5/10], Step [30/1094], Loss: 0.4824\n",
      "Epoch [5/10], Step [40/1094], Loss: 0.3734\n",
      "Epoch [5/10], Step [50/1094], Loss: 0.6192\n",
      "Epoch [5/10], Step [60/1094], Loss: 0.3807\n",
      "Epoch [5/10], Step [70/1094], Loss: 0.8109\n",
      "Epoch [5/10], Step [80/1094], Loss: 0.3697\n",
      "Epoch [5/10], Step [90/1094], Loss: 0.3387\n",
      "Epoch [5/10], Step [100/1094], Loss: 0.5158\n",
      "Epoch [5/10], Step [110/1094], Loss: 0.8888\n",
      "Epoch [5/10], Step [120/1094], Loss: 0.1753\n",
      "Epoch [5/10], Step [130/1094], Loss: 0.5497\n",
      "Epoch [5/10], Step [140/1094], Loss: 0.4844\n",
      "Epoch [5/10], Step [150/1094], Loss: 0.3708\n",
      "Epoch [5/10], Step [160/1094], Loss: 0.4757\n",
      "Epoch [5/10], Step [170/1094], Loss: 0.3866\n",
      "Epoch [5/10], Step [180/1094], Loss: 0.6455\n",
      "Epoch [5/10], Step [190/1094], Loss: 0.3368\n",
      "Epoch [5/10], Step [200/1094], Loss: 0.3533\n",
      "Epoch [5/10], Step [210/1094], Loss: 0.2188\n",
      "Epoch [5/10], Step [220/1094], Loss: 0.3082\n",
      "Epoch [5/10], Step [230/1094], Loss: 0.4743\n",
      "Epoch [5/10], Step [240/1094], Loss: 0.6351\n",
      "Epoch [5/10], Step [250/1094], Loss: 0.4017\n",
      "Epoch [5/10], Step [260/1094], Loss: 0.2799\n",
      "Epoch [5/10], Step [270/1094], Loss: 0.3946\n",
      "Epoch [5/10], Step [280/1094], Loss: 0.5496\n",
      "Epoch [5/10], Step [290/1094], Loss: 0.4155\n",
      "Epoch [5/10], Step [300/1094], Loss: 0.3554\n",
      "Epoch [5/10], Step [310/1094], Loss: 0.4190\n",
      "Epoch [5/10], Step [320/1094], Loss: 0.2854\n",
      "Epoch [5/10], Step [330/1094], Loss: 0.5124\n",
      "Epoch [5/10], Step [340/1094], Loss: 0.7124\n",
      "Epoch [5/10], Step [350/1094], Loss: 0.9238\n",
      "Epoch [5/10], Step [360/1094], Loss: 0.4665\n",
      "Epoch [5/10], Step [370/1094], Loss: 0.6358\n",
      "Epoch [5/10], Step [380/1094], Loss: 0.4651\n",
      "Epoch [5/10], Step [390/1094], Loss: 0.3246\n",
      "Epoch [5/10], Step [400/1094], Loss: 0.2999\n",
      "Epoch [5/10], Step [410/1094], Loss: 0.3521\n",
      "Epoch [5/10], Step [420/1094], Loss: 0.4769\n",
      "Epoch [5/10], Step [430/1094], Loss: 0.4872\n",
      "Epoch [5/10], Step [440/1094], Loss: 0.2265\n",
      "Epoch [5/10], Step [450/1094], Loss: 0.6365\n",
      "Epoch [5/10], Step [460/1094], Loss: 0.5551\n",
      "Epoch [5/10], Step [470/1094], Loss: 0.7430\n",
      "Epoch [5/10], Step [480/1094], Loss: 0.3677\n",
      "Epoch [5/10], Step [490/1094], Loss: 0.5987\n",
      "Epoch [5/10], Step [500/1094], Loss: 0.3084\n",
      "Epoch [5/10], Step [510/1094], Loss: 0.3792\n",
      "Epoch [5/10], Step [520/1094], Loss: 0.3597\n",
      "Epoch [5/10], Step [530/1094], Loss: 0.2761\n",
      "Epoch [5/10], Step [540/1094], Loss: 0.4364\n",
      "Epoch [5/10], Step [550/1094], Loss: 0.5412\n",
      "Epoch [5/10], Step [560/1094], Loss: 0.3653\n",
      "Epoch [5/10], Step [570/1094], Loss: 0.6029\n",
      "Epoch [5/10], Step [580/1094], Loss: 0.5723\n",
      "Epoch [5/10], Step [590/1094], Loss: 0.4010\n",
      "Epoch [5/10], Step [600/1094], Loss: 0.5304\n",
      "Epoch [5/10], Step [610/1094], Loss: 0.5172\n",
      "Epoch [5/10], Step [620/1094], Loss: 0.5109\n",
      "Epoch [5/10], Step [630/1094], Loss: 0.7279\n",
      "Epoch [5/10], Step [640/1094], Loss: 0.5693\n",
      "Epoch [5/10], Step [650/1094], Loss: 0.3570\n",
      "Epoch [5/10], Step [660/1094], Loss: 0.4744\n",
      "Epoch [5/10], Step [670/1094], Loss: 1.0266\n",
      "Epoch [5/10], Step [680/1094], Loss: 1.1612\n",
      "Epoch [5/10], Step [690/1094], Loss: 0.9541\n",
      "Epoch [5/10], Step [700/1094], Loss: 0.5529\n",
      "Epoch [5/10], Step [710/1094], Loss: 0.4334\n",
      "Epoch [5/10], Step [720/1094], Loss: 1.3183\n",
      "Epoch [5/10], Step [730/1094], Loss: 0.9625\n",
      "Epoch [5/10], Step [740/1094], Loss: 0.6106\n",
      "Epoch [5/10], Step [750/1094], Loss: 0.6718\n",
      "Epoch [5/10], Step [760/1094], Loss: 0.7993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Step [770/1094], Loss: 0.4065\n",
      "Epoch [5/10], Step [780/1094], Loss: 0.5657\n",
      "Epoch [5/10], Step [790/1094], Loss: 0.3637\n",
      "Epoch [5/10], Step [800/1094], Loss: 0.4585\n",
      "Epoch [5/10], Step [810/1094], Loss: 0.5363\n",
      "Epoch [5/10], Step [820/1094], Loss: 0.8376\n",
      "Epoch [5/10], Step [830/1094], Loss: 0.4516\n",
      "Epoch [5/10], Step [840/1094], Loss: 0.6136\n",
      "Epoch [5/10], Step [850/1094], Loss: 0.5768\n",
      "Epoch [5/10], Step [860/1094], Loss: 0.5629\n",
      "Epoch [5/10], Step [870/1094], Loss: 0.2585\n",
      "Epoch [5/10], Step [880/1094], Loss: 0.3910\n",
      "Epoch [5/10], Step [890/1094], Loss: 0.3210\n",
      "Epoch [5/10], Step [900/1094], Loss: 0.3130\n",
      "Epoch [5/10], Step [910/1094], Loss: 0.2564\n",
      "Epoch [5/10], Step [920/1094], Loss: 0.5883\n",
      "Epoch [5/10], Step [930/1094], Loss: 0.1863\n",
      "Epoch [5/10], Step [940/1094], Loss: 0.7749\n",
      "Epoch [5/10], Step [950/1094], Loss: 0.9210\n",
      "Epoch [5/10], Step [960/1094], Loss: 0.6624\n",
      "Epoch [5/10], Step [970/1094], Loss: 0.3340\n",
      "Epoch [5/10], Step [980/1094], Loss: 0.2739\n",
      "Epoch [5/10], Step [990/1094], Loss: 0.6674\n",
      "Epoch [5/10], Step [1000/1094], Loss: 0.6491\n",
      "Epoch [5/10], Step [1010/1094], Loss: 1.2003\n",
      "Epoch [5/10], Step [1020/1094], Loss: 1.3839\n",
      "Epoch [5/10], Step [1030/1094], Loss: 1.1374\n",
      "Epoch [5/10], Step [1040/1094], Loss: 0.9882\n",
      "Epoch [5/10], Step [1050/1094], Loss: 0.6617\n",
      "Epoch [5/10], Step [1060/1094], Loss: 1.0973\n",
      "Epoch [5/10], Step [1070/1094], Loss: 0.6749\n",
      "Epoch [5/10], Step [1080/1094], Loss: 0.7397\n",
      "Epoch [5/10], Step [1090/1094], Loss: 0.9750\n",
      "Epoch [5/10], Average Loss: 0.5489, Accuracy: 0.8055\n",
      "Class 0: Precision = 0.7423, Recall = 0.7208, F1 Score = 0.7314\n",
      "Class 1: Precision = 0.9557, Recall = 0.9582, F1 Score = 0.9570\n",
      "Class 2: Precision = 0.8373, Recall = 0.8326, F1 Score = 0.8349\n",
      "Class 3: Precision = 0.7302, Recall = 0.8020, F1 Score = 0.7644\n",
      "Class 4: Precision = 0.8048, Recall = 0.8272, F1 Score = 0.8159\n",
      "Class 5: Precision = 0.7442, Recall = 0.6308, F1 Score = 0.6828\n",
      "Class 6: Precision = 0.8189, Recall = 0.8666, F1 Score = 0.8421\n",
      "Epoch [6/10], Step [10/1094], Loss: 0.6797\n",
      "Epoch [6/10], Step [20/1094], Loss: 0.8912\n",
      "Epoch [6/10], Step [30/1094], Loss: 0.6984\n",
      "Epoch [6/10], Step [40/1094], Loss: 0.4914\n",
      "Epoch [6/10], Step [50/1094], Loss: 0.7304\n",
      "Epoch [6/10], Step [60/1094], Loss: 0.5260\n",
      "Epoch [6/10], Step [70/1094], Loss: 0.4209\n",
      "Epoch [6/10], Step [80/1094], Loss: 0.7678\n",
      "Epoch [6/10], Step [90/1094], Loss: 0.7186\n",
      "Epoch [6/10], Step [100/1094], Loss: 0.5834\n",
      "Epoch [6/10], Step [110/1094], Loss: 0.8320\n",
      "Epoch [6/10], Step [120/1094], Loss: 0.4580\n",
      "Epoch [6/10], Step [130/1094], Loss: 0.8084\n",
      "Epoch [6/10], Step [140/1094], Loss: 0.6982\n",
      "Epoch [6/10], Step [150/1094], Loss: 0.7605\n",
      "Epoch [6/10], Step [160/1094], Loss: 0.7310\n",
      "Epoch [6/10], Step [170/1094], Loss: 0.7829\n",
      "Epoch [6/10], Step [180/1094], Loss: 0.5860\n",
      "Epoch [6/10], Step [190/1094], Loss: 0.4828\n",
      "Epoch [6/10], Step [200/1094], Loss: 0.6287\n",
      "Epoch [6/10], Step [210/1094], Loss: 0.7734\n",
      "Epoch [6/10], Step [220/1094], Loss: 0.7438\n",
      "Epoch [6/10], Step [230/1094], Loss: 0.5396\n",
      "Epoch [6/10], Step [240/1094], Loss: 0.3925\n",
      "Epoch [6/10], Step [250/1094], Loss: 0.5797\n",
      "Epoch [6/10], Step [260/1094], Loss: 0.6199\n",
      "Epoch [6/10], Step [270/1094], Loss: 0.4984\n",
      "Epoch [6/10], Step [280/1094], Loss: 0.5714\n",
      "Epoch [6/10], Step [290/1094], Loss: 0.5964\n",
      "Epoch [6/10], Step [300/1094], Loss: 0.4447\n",
      "Epoch [6/10], Step [310/1094], Loss: 0.3936\n",
      "Epoch [6/10], Step [320/1094], Loss: 0.5004\n",
      "Epoch [6/10], Step [330/1094], Loss: 0.4662\n",
      "Epoch [6/10], Step [340/1094], Loss: 0.5687\n",
      "Epoch [6/10], Step [350/1094], Loss: 0.4822\n",
      "Epoch [6/10], Step [360/1094], Loss: 0.3614\n",
      "Epoch [6/10], Step [370/1094], Loss: 0.4947\n",
      "Epoch [6/10], Step [380/1094], Loss: 0.5436\n",
      "Epoch [6/10], Step [390/1094], Loss: 0.6722\n",
      "Epoch [6/10], Step [400/1094], Loss: 0.6330\n",
      "Epoch [6/10], Step [410/1094], Loss: 0.5858\n",
      "Epoch [6/10], Step [420/1094], Loss: 0.3878\n",
      "Epoch [6/10], Step [430/1094], Loss: 0.2435\n",
      "Epoch [6/10], Step [440/1094], Loss: 0.5355\n",
      "Epoch [6/10], Step [450/1094], Loss: 0.5576\n",
      "Epoch [6/10], Step [460/1094], Loss: 0.6303\n",
      "Epoch [6/10], Step [470/1094], Loss: 0.5685\n",
      "Epoch [6/10], Step [480/1094], Loss: 0.5139\n",
      "Epoch [6/10], Step [490/1094], Loss: 0.6353\n",
      "Epoch [6/10], Step [500/1094], Loss: 0.6130\n",
      "Epoch [6/10], Step [510/1094], Loss: 0.7106\n",
      "Epoch [6/10], Step [520/1094], Loss: 0.5955\n",
      "Epoch [6/10], Step [530/1094], Loss: 0.5325\n",
      "Epoch [6/10], Step [540/1094], Loss: 0.3075\n",
      "Epoch [6/10], Step [550/1094], Loss: 0.2639\n",
      "Epoch [6/10], Step [560/1094], Loss: 0.4472\n",
      "Epoch [6/10], Step [570/1094], Loss: 0.4645\n",
      "Epoch [6/10], Step [580/1094], Loss: 0.5374\n",
      "Epoch [6/10], Step [590/1094], Loss: 0.4242\n",
      "Epoch [6/10], Step [600/1094], Loss: 0.3824\n",
      "Epoch [6/10], Step [610/1094], Loss: 0.7281\n",
      "Epoch [6/10], Step [620/1094], Loss: 0.3224\n",
      "Epoch [6/10], Step [630/1094], Loss: 0.3408\n",
      "Epoch [6/10], Step [640/1094], Loss: 0.6477\n",
      "Epoch [6/10], Step [650/1094], Loss: 1.0374\n",
      "Epoch [6/10], Step [660/1094], Loss: 0.7303\n",
      "Epoch [6/10], Step [670/1094], Loss: 0.8120\n",
      "Epoch [6/10], Step [680/1094], Loss: 0.6999\n",
      "Epoch [6/10], Step [690/1094], Loss: 0.8586\n",
      "Epoch [6/10], Step [700/1094], Loss: 0.6037\n",
      "Epoch [6/10], Step [710/1094], Loss: 0.7761\n",
      "Epoch [6/10], Step [720/1094], Loss: 0.8864\n",
      "Epoch [6/10], Step [730/1094], Loss: 0.7893\n",
      "Epoch [6/10], Step [740/1094], Loss: 0.9068\n",
      "Epoch [6/10], Step [750/1094], Loss: 0.7878\n",
      "Epoch [6/10], Step [760/1094], Loss: 0.7741\n",
      "Epoch [6/10], Step [770/1094], Loss: 0.7611\n",
      "Epoch [6/10], Step [780/1094], Loss: 0.9839\n",
      "Epoch [6/10], Step [790/1094], Loss: 0.6439\n",
      "Epoch [6/10], Step [800/1094], Loss: 0.6202\n",
      "Epoch [6/10], Step [810/1094], Loss: 1.0463\n",
      "Epoch [6/10], Step [820/1094], Loss: 0.7743\n",
      "Epoch [6/10], Step [830/1094], Loss: 0.8102\n",
      "Epoch [6/10], Step [840/1094], Loss: 0.6332\n",
      "Epoch [6/10], Step [850/1094], Loss: 0.6997\n",
      "Epoch [6/10], Step [860/1094], Loss: 0.5286\n",
      "Epoch [6/10], Step [870/1094], Loss: 0.7530\n",
      "Epoch [6/10], Step [880/1094], Loss: 0.3874\n",
      "Epoch [6/10], Step [890/1094], Loss: 0.4871\n",
      "Epoch [6/10], Step [900/1094], Loss: 0.5855\n",
      "Epoch [6/10], Step [910/1094], Loss: 0.8632\n",
      "Epoch [6/10], Step [920/1094], Loss: 0.6017\n",
      "Epoch [6/10], Step [930/1094], Loss: 0.7880\n",
      "Epoch [6/10], Step [940/1094], Loss: 0.6672\n",
      "Epoch [6/10], Step [950/1094], Loss: 0.9379\n",
      "Epoch [6/10], Step [960/1094], Loss: 0.8581\n",
      "Epoch [6/10], Step [970/1094], Loss: 0.7259\n",
      "Epoch [6/10], Step [980/1094], Loss: 0.7462\n",
      "Epoch [6/10], Step [990/1094], Loss: 0.6285\n",
      "Epoch [6/10], Step [1000/1094], Loss: 0.4858\n",
      "Epoch [6/10], Step [1010/1094], Loss: 0.5185\n",
      "Epoch [6/10], Step [1020/1094], Loss: 0.6869\n",
      "Epoch [6/10], Step [1030/1094], Loss: 0.5611\n",
      "Epoch [6/10], Step [1040/1094], Loss: 0.7565\n",
      "Epoch [6/10], Step [1050/1094], Loss: 0.5999\n",
      "Epoch [6/10], Step [1060/1094], Loss: 0.5035\n",
      "Epoch [6/10], Step [1070/1094], Loss: 0.5642\n",
      "Epoch [6/10], Step [1080/1094], Loss: 0.7262\n",
      "Epoch [6/10], Step [1090/1094], Loss: 0.4775\n",
      "Epoch [6/10], Average Loss: 0.6398, Accuracy: 0.7695\n",
      "Class 0: Precision = 0.6619, Recall = 0.6530, F1 Score = 0.6574\n",
      "Class 1: Precision = 0.9727, Recall = 0.9638, F1 Score = 0.9683\n",
      "Class 2: Precision = 0.7497, Recall = 0.7458, F1 Score = 0.7477\n",
      "Class 3: Precision = 0.7380, Recall = 0.8372, F1 Score = 0.7845\n",
      "Class 4: Precision = 0.8347, Recall = 0.8280, F1 Score = 0.8313\n",
      "Class 5: Precision = 0.7565, Recall = 0.4846, F1 Score = 0.5908\n",
      "Class 6: Precision = 0.6931, Recall = 0.8738, F1 Score = 0.7730\n",
      "Epoch [7/10], Step [10/1094], Loss: 0.8256\n",
      "Epoch [7/10], Step [20/1094], Loss: 0.6136\n",
      "Epoch [7/10], Step [30/1094], Loss: 0.3534\n",
      "Epoch [7/10], Step [40/1094], Loss: 0.4611\n",
      "Epoch [7/10], Step [50/1094], Loss: 0.8595\n",
      "Epoch [7/10], Step [60/1094], Loss: 0.6747\n",
      "Epoch [7/10], Step [70/1094], Loss: 0.8541\n",
      "Epoch [7/10], Step [80/1094], Loss: 0.7383\n",
      "Epoch [7/10], Step [90/1094], Loss: 0.6748\n",
      "Epoch [7/10], Step [100/1094], Loss: 0.4548\n",
      "Epoch [7/10], Step [110/1094], Loss: 0.6411\n",
      "Epoch [7/10], Step [120/1094], Loss: 0.6878\n",
      "Epoch [7/10], Step [130/1094], Loss: 0.4325\n",
      "Epoch [7/10], Step [140/1094], Loss: 0.6356\n",
      "Epoch [7/10], Step [150/1094], Loss: 0.7138\n",
      "Epoch [7/10], Step [160/1094], Loss: 0.7570\n",
      "Epoch [7/10], Step [170/1094], Loss: 0.4999\n",
      "Epoch [7/10], Step [180/1094], Loss: 0.5257\n",
      "Epoch [7/10], Step [190/1094], Loss: 0.7716\n",
      "Epoch [7/10], Step [200/1094], Loss: 0.7038\n",
      "Epoch [7/10], Step [210/1094], Loss: 0.7395\n",
      "Epoch [7/10], Step [220/1094], Loss: 0.7096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Step [230/1094], Loss: 0.5787\n",
      "Epoch [7/10], Step [240/1094], Loss: 0.5514\n",
      "Epoch [7/10], Step [250/1094], Loss: 0.3602\n",
      "Epoch [7/10], Step [260/1094], Loss: 0.5248\n",
      "Epoch [7/10], Step [270/1094], Loss: 0.7202\n",
      "Epoch [7/10], Step [280/1094], Loss: 0.6539\n",
      "Epoch [7/10], Step [290/1094], Loss: 0.6250\n",
      "Epoch [7/10], Step [300/1094], Loss: 0.7763\n",
      "Epoch [7/10], Step [310/1094], Loss: 1.0352\n",
      "Epoch [7/10], Step [320/1094], Loss: 1.0310\n",
      "Epoch [7/10], Step [330/1094], Loss: 0.5838\n",
      "Epoch [7/10], Step [340/1094], Loss: 0.3581\n",
      "Epoch [7/10], Step [350/1094], Loss: 0.5044\n",
      "Epoch [7/10], Step [360/1094], Loss: 0.5294\n",
      "Epoch [7/10], Step [370/1094], Loss: 0.5002\n",
      "Epoch [7/10], Step [380/1094], Loss: 0.4186\n",
      "Epoch [7/10], Step [390/1094], Loss: 0.6228\n",
      "Epoch [7/10], Step [400/1094], Loss: 0.5671\n",
      "Epoch [7/10], Step [410/1094], Loss: 0.7191\n",
      "Epoch [7/10], Step [420/1094], Loss: 0.3253\n",
      "Epoch [7/10], Step [430/1094], Loss: 0.5023\n",
      "Epoch [7/10], Step [440/1094], Loss: 0.5832\n",
      "Epoch [7/10], Step [450/1094], Loss: 0.5714\n",
      "Epoch [7/10], Step [460/1094], Loss: 0.6395\n",
      "Epoch [7/10], Step [470/1094], Loss: 0.5780\n",
      "Epoch [7/10], Step [480/1094], Loss: 0.4811\n",
      "Epoch [7/10], Step [490/1094], Loss: 0.3271\n",
      "Epoch [7/10], Step [500/1094], Loss: 0.6528\n",
      "Epoch [7/10], Step [510/1094], Loss: 0.4013\n",
      "Epoch [7/10], Step [520/1094], Loss: 0.4234\n",
      "Epoch [7/10], Step [530/1094], Loss: 0.4125\n",
      "Epoch [7/10], Step [540/1094], Loss: 0.4142\n",
      "Epoch [7/10], Step [550/1094], Loss: 0.6602\n",
      "Epoch [7/10], Step [560/1094], Loss: 0.3592\n",
      "Epoch [7/10], Step [570/1094], Loss: 0.3086\n",
      "Epoch [7/10], Step [580/1094], Loss: 1.1011\n",
      "Epoch [7/10], Step [590/1094], Loss: 0.4276\n",
      "Epoch [7/10], Step [600/1094], Loss: 0.7317\n",
      "Epoch [7/10], Step [610/1094], Loss: 0.2684\n",
      "Epoch [7/10], Step [620/1094], Loss: 0.4280\n",
      "Epoch [7/10], Step [630/1094], Loss: 0.5342\n",
      "Epoch [7/10], Step [640/1094], Loss: 0.4524\n",
      "Epoch [7/10], Step [650/1094], Loss: 0.7181\n",
      "Epoch [7/10], Step [660/1094], Loss: 0.7233\n",
      "Epoch [7/10], Step [670/1094], Loss: 0.5076\n",
      "Epoch [7/10], Step [680/1094], Loss: 0.5597\n",
      "Epoch [7/10], Step [690/1094], Loss: 0.3205\n",
      "Epoch [7/10], Step [700/1094], Loss: 0.4890\n",
      "Epoch [7/10], Step [710/1094], Loss: 0.5979\n",
      "Epoch [7/10], Step [720/1094], Loss: 0.4182\n",
      "Epoch [7/10], Step [730/1094], Loss: 0.7326\n",
      "Epoch [7/10], Step [740/1094], Loss: 0.6045\n",
      "Epoch [7/10], Step [750/1094], Loss: 0.5615\n",
      "Epoch [7/10], Step [760/1094], Loss: 0.6110\n",
      "Epoch [7/10], Step [770/1094], Loss: 0.7501\n",
      "Epoch [7/10], Step [780/1094], Loss: 0.4389\n",
      "Epoch [7/10], Step [790/1094], Loss: 0.7483\n",
      "Epoch [7/10], Step [800/1094], Loss: 0.5069\n",
      "Epoch [7/10], Step [810/1094], Loss: 0.6341\n",
      "Epoch [7/10], Step [820/1094], Loss: 0.4690\n",
      "Epoch [7/10], Step [830/1094], Loss: 0.6988\n",
      "Epoch [7/10], Step [840/1094], Loss: 0.8524\n",
      "Epoch [7/10], Step [850/1094], Loss: 0.6339\n",
      "Epoch [7/10], Step [860/1094], Loss: 0.3934\n",
      "Epoch [7/10], Step [870/1094], Loss: 0.4576\n",
      "Epoch [7/10], Step [880/1094], Loss: 0.7623\n",
      "Epoch [7/10], Step [890/1094], Loss: 0.5126\n",
      "Epoch [7/10], Step [900/1094], Loss: 0.4656\n",
      "Epoch [7/10], Step [910/1094], Loss: 0.3553\n",
      "Epoch [7/10], Step [920/1094], Loss: 0.4455\n",
      "Epoch [7/10], Step [930/1094], Loss: 0.3322\n",
      "Epoch [7/10], Step [940/1094], Loss: 0.6493\n",
      "Epoch [7/10], Step [950/1094], Loss: 0.4869\n",
      "Epoch [7/10], Step [960/1094], Loss: 0.5780\n",
      "Epoch [7/10], Step [970/1094], Loss: 0.3067\n",
      "Epoch [7/10], Step [980/1094], Loss: 0.4013\n",
      "Epoch [7/10], Step [990/1094], Loss: 0.5618\n",
      "Epoch [7/10], Step [1000/1094], Loss: 0.3640\n",
      "Epoch [7/10], Step [1010/1094], Loss: 0.4356\n",
      "Epoch [7/10], Step [1020/1094], Loss: 0.6906\n",
      "Epoch [7/10], Step [1030/1094], Loss: 0.6240\n",
      "Epoch [7/10], Step [1040/1094], Loss: 0.2904\n",
      "Epoch [7/10], Step [1050/1094], Loss: 0.5183\n",
      "Epoch [7/10], Step [1060/1094], Loss: 0.5484\n",
      "Epoch [7/10], Step [1070/1094], Loss: 0.5191\n",
      "Epoch [7/10], Step [1080/1094], Loss: 0.6145\n",
      "Epoch [7/10], Step [1090/1094], Loss: 0.4502\n",
      "Epoch [7/10], Average Loss: 0.5375, Accuracy: 0.8146\n",
      "Class 0: Precision = 0.7683, Recall = 0.7966, F1 Score = 0.7822\n",
      "Class 1: Precision = 0.9741, Recall = 0.9618, F1 Score = 0.9679\n",
      "Class 2: Precision = 0.8581, Recall = 0.8300, F1 Score = 0.8438\n",
      "Class 3: Precision = 0.7516, Recall = 0.8304, F1 Score = 0.7891\n",
      "Class 4: Precision = 0.8540, Recall = 0.7384, F1 Score = 0.7920\n",
      "Class 5: Precision = 0.7679, Recall = 0.6258, F1 Score = 0.6896\n",
      "Class 6: Precision = 0.7509, Recall = 0.9192, F1 Score = 0.8265\n",
      "Epoch [8/10], Step [10/1094], Loss: 0.3808\n",
      "Epoch [8/10], Step [20/1094], Loss: 0.5901\n",
      "Epoch [8/10], Step [30/1094], Loss: 0.7654\n",
      "Epoch [8/10], Step [40/1094], Loss: 0.4050\n",
      "Epoch [8/10], Step [50/1094], Loss: 0.5451\n",
      "Epoch [8/10], Step [60/1094], Loss: 0.4035\n",
      "Epoch [8/10], Step [70/1094], Loss: 0.6495\n",
      "Epoch [8/10], Step [80/1094], Loss: 0.4316\n",
      "Epoch [8/10], Step [90/1094], Loss: 0.5151\n",
      "Epoch [8/10], Step [100/1094], Loss: 0.5045\n",
      "Epoch [8/10], Step [110/1094], Loss: 0.2136\n",
      "Epoch [8/10], Step [120/1094], Loss: 0.3701\n",
      "Epoch [8/10], Step [130/1094], Loss: 0.3555\n",
      "Epoch [8/10], Step [140/1094], Loss: 0.3662\n",
      "Epoch [8/10], Step [150/1094], Loss: 0.2918\n",
      "Epoch [8/10], Step [160/1094], Loss: 0.4425\n",
      "Epoch [8/10], Step [170/1094], Loss: 0.4063\n",
      "Epoch [8/10], Step [180/1094], Loss: 0.4133\n",
      "Epoch [8/10], Step [190/1094], Loss: 0.5223\n",
      "Epoch [8/10], Step [200/1094], Loss: 1.1984\n",
      "Epoch [8/10], Step [210/1094], Loss: 0.6973\n",
      "Epoch [8/10], Step [220/1094], Loss: 0.5685\n",
      "Epoch [8/10], Step [230/1094], Loss: 0.5509\n",
      "Epoch [8/10], Step [240/1094], Loss: 0.5251\n",
      "Epoch [8/10], Step [250/1094], Loss: 0.3671\n",
      "Epoch [8/10], Step [260/1094], Loss: 0.6027\n",
      "Epoch [8/10], Step [270/1094], Loss: 0.7514\n",
      "Epoch [8/10], Step [280/1094], Loss: 0.6424\n",
      "Epoch [8/10], Step [290/1094], Loss: 0.3273\n",
      "Epoch [8/10], Step [300/1094], Loss: 0.3207\n",
      "Epoch [8/10], Step [310/1094], Loss: 0.7696\n",
      "Epoch [8/10], Step [320/1094], Loss: 0.6390\n",
      "Epoch [8/10], Step [330/1094], Loss: 0.6304\n",
      "Epoch [8/10], Step [340/1094], Loss: 0.7723\n",
      "Epoch [8/10], Step [350/1094], Loss: 0.4323\n",
      "Epoch [8/10], Step [360/1094], Loss: 0.5181\n",
      "Epoch [8/10], Step [370/1094], Loss: 0.5713\n",
      "Epoch [8/10], Step [380/1094], Loss: 0.2641\n",
      "Epoch [8/10], Step [390/1094], Loss: 0.2738\n",
      "Epoch [8/10], Step [400/1094], Loss: 0.4099\n",
      "Epoch [8/10], Step [410/1094], Loss: 0.5750\n",
      "Epoch [8/10], Step [420/1094], Loss: 0.5346\n",
      "Epoch [8/10], Step [430/1094], Loss: 0.4473\n",
      "Epoch [8/10], Step [440/1094], Loss: 0.4131\n",
      "Epoch [8/10], Step [450/1094], Loss: 0.3523\n",
      "Epoch [8/10], Step [460/1094], Loss: 0.4214\n",
      "Epoch [8/10], Step [470/1094], Loss: 0.5471\n",
      "Epoch [8/10], Step [480/1094], Loss: 0.7003\n",
      "Epoch [8/10], Step [490/1094], Loss: 0.4865\n",
      "Epoch [8/10], Step [500/1094], Loss: 0.4043\n",
      "Epoch [8/10], Step [510/1094], Loss: 0.3161\n",
      "Epoch [8/10], Step [520/1094], Loss: 0.3852\n",
      "Epoch [8/10], Step [530/1094], Loss: 0.4659\n",
      "Epoch [8/10], Step [540/1094], Loss: 0.5019\n",
      "Epoch [8/10], Step [550/1094], Loss: 0.6102\n",
      "Epoch [8/10], Step [560/1094], Loss: 0.5470\n",
      "Epoch [8/10], Step [570/1094], Loss: 0.6334\n",
      "Epoch [8/10], Step [580/1094], Loss: 0.5541\n",
      "Epoch [8/10], Step [590/1094], Loss: 0.3024\n",
      "Epoch [8/10], Step [600/1094], Loss: 0.5439\n",
      "Epoch [8/10], Step [610/1094], Loss: 0.6403\n",
      "Epoch [8/10], Step [620/1094], Loss: 0.4075\n",
      "Epoch [8/10], Step [630/1094], Loss: 0.6450\n",
      "Epoch [8/10], Step [640/1094], Loss: 0.6703\n",
      "Epoch [8/10], Step [650/1094], Loss: 0.6679\n",
      "Epoch [8/10], Step [660/1094], Loss: 0.5625\n",
      "Epoch [8/10], Step [670/1094], Loss: 0.4982\n",
      "Epoch [8/10], Step [680/1094], Loss: 0.5640\n",
      "Epoch [8/10], Step [690/1094], Loss: 0.4321\n",
      "Epoch [8/10], Step [700/1094], Loss: 0.6792\n",
      "Epoch [8/10], Step [710/1094], Loss: 0.3503\n",
      "Epoch [8/10], Step [720/1094], Loss: 0.3976\n",
      "Epoch [8/10], Step [730/1094], Loss: 0.2971\n",
      "Epoch [8/10], Step [740/1094], Loss: 0.5409\n",
      "Epoch [8/10], Step [750/1094], Loss: 1.0190\n",
      "Epoch [8/10], Step [760/1094], Loss: 0.2937\n",
      "Epoch [8/10], Step [770/1094], Loss: 0.7218\n",
      "Epoch [8/10], Step [780/1094], Loss: 0.2854\n",
      "Epoch [8/10], Step [790/1094], Loss: 0.5031\n",
      "Epoch [8/10], Step [800/1094], Loss: 0.3509\n",
      "Epoch [8/10], Step [810/1094], Loss: 0.7993\n",
      "Epoch [8/10], Step [820/1094], Loss: 0.3994\n",
      "Epoch [8/10], Step [830/1094], Loss: 0.5119\n",
      "Epoch [8/10], Step [840/1094], Loss: 0.4198\n",
      "Epoch [8/10], Step [850/1094], Loss: 0.3792\n",
      "Epoch [8/10], Step [860/1094], Loss: 0.5408\n",
      "Epoch [8/10], Step [870/1094], Loss: 0.7663\n",
      "Epoch [8/10], Step [880/1094], Loss: 0.4098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Step [890/1094], Loss: 0.4312\n",
      "Epoch [8/10], Step [900/1094], Loss: 0.2721\n",
      "Epoch [8/10], Step [910/1094], Loss: 0.3672\n",
      "Epoch [8/10], Step [920/1094], Loss: 0.4203\n",
      "Epoch [8/10], Step [930/1094], Loss: 0.5314\n",
      "Epoch [8/10], Step [940/1094], Loss: 0.4168\n",
      "Epoch [8/10], Step [950/1094], Loss: 0.7801\n",
      "Epoch [8/10], Step [960/1094], Loss: 0.2023\n",
      "Epoch [8/10], Step [970/1094], Loss: 0.6936\n",
      "Epoch [8/10], Step [980/1094], Loss: 0.4022\n",
      "Epoch [8/10], Step [990/1094], Loss: 0.5211\n",
      "Epoch [8/10], Step [1000/1094], Loss: 0.5092\n",
      "Epoch [8/10], Step [1010/1094], Loss: 0.3709\n",
      "Epoch [8/10], Step [1020/1094], Loss: 0.4139\n",
      "Epoch [8/10], Step [1030/1094], Loss: 0.4089\n",
      "Epoch [8/10], Step [1040/1094], Loss: 0.3444\n",
      "Epoch [8/10], Step [1050/1094], Loss: 0.8351\n",
      "Epoch [8/10], Step [1060/1094], Loss: 0.5433\n",
      "Epoch [8/10], Step [1070/1094], Loss: 0.1871\n",
      "Epoch [8/10], Step [1080/1094], Loss: 0.7076\n",
      "Epoch [8/10], Step [1090/1094], Loss: 0.3049\n",
      "Epoch [8/10], Average Loss: 0.4707, Accuracy: 0.8319\n",
      "Class 0: Precision = 0.7369, Recall = 0.7816, F1 Score = 0.7586\n",
      "Class 1: Precision = 0.9760, Recall = 0.9584, F1 Score = 0.9671\n",
      "Class 2: Precision = 0.8361, Recall = 0.8000, F1 Score = 0.8177\n",
      "Class 3: Precision = 0.7571, Recall = 0.8146, F1 Score = 0.7848\n",
      "Class 4: Precision = 0.8065, Recall = 0.8934, F1 Score = 0.8477\n",
      "Class 5: Precision = 0.8234, Recall = 0.6806, F1 Score = 0.7452\n",
      "Class 6: Precision = 0.9035, Recall = 0.8946, F1 Score = 0.8990\n",
      "Epoch [9/10], Step [10/1094], Loss: 0.2534\n",
      "Epoch [9/10], Step [20/1094], Loss: 0.5452\n",
      "Epoch [9/10], Step [30/1094], Loss: 0.4689\n",
      "Epoch [9/10], Step [40/1094], Loss: 0.6316\n",
      "Epoch [9/10], Step [50/1094], Loss: 0.3802\n",
      "Epoch [9/10], Step [60/1094], Loss: 0.4579\n",
      "Epoch [9/10], Step [70/1094], Loss: 0.2426\n",
      "Epoch [9/10], Step [80/1094], Loss: 0.4865\n",
      "Epoch [9/10], Step [90/1094], Loss: 0.4051\n",
      "Epoch [9/10], Step [100/1094], Loss: 0.4592\n",
      "Epoch [9/10], Step [110/1094], Loss: 0.2430\n",
      "Epoch [9/10], Step [120/1094], Loss: 0.3428\n",
      "Epoch [9/10], Step [130/1094], Loss: 0.3716\n",
      "Epoch [9/10], Step [140/1094], Loss: 0.6352\n",
      "Epoch [9/10], Step [150/1094], Loss: 0.2132\n",
      "Epoch [9/10], Step [160/1094], Loss: 0.3099\n",
      "Epoch [9/10], Step [170/1094], Loss: 0.3978\n",
      "Epoch [9/10], Step [180/1094], Loss: 0.5109\n",
      "Epoch [9/10], Step [190/1094], Loss: 0.4304\n",
      "Epoch [9/10], Step [200/1094], Loss: 0.3354\n",
      "Epoch [9/10], Step [210/1094], Loss: 0.4499\n",
      "Epoch [9/10], Step [220/1094], Loss: 0.4650\n",
      "Epoch [9/10], Step [230/1094], Loss: 0.4063\n",
      "Epoch [9/10], Step [240/1094], Loss: 0.3570\n",
      "Epoch [9/10], Step [250/1094], Loss: 0.5862\n",
      "Epoch [9/10], Step [260/1094], Loss: 0.3769\n",
      "Epoch [9/10], Step [270/1094], Loss: 0.3397\n",
      "Epoch [9/10], Step [280/1094], Loss: 0.5627\n",
      "Epoch [9/10], Step [290/1094], Loss: 0.3244\n",
      "Epoch [9/10], Step [300/1094], Loss: 0.4933\n",
      "Epoch [9/10], Step [310/1094], Loss: 0.5023\n",
      "Epoch [9/10], Step [320/1094], Loss: 0.5622\n",
      "Epoch [9/10], Step [330/1094], Loss: 0.2513\n",
      "Epoch [9/10], Step [340/1094], Loss: 0.4121\n",
      "Epoch [9/10], Step [350/1094], Loss: 0.4334\n",
      "Epoch [9/10], Step [360/1094], Loss: 0.2890\n",
      "Epoch [9/10], Step [370/1094], Loss: 0.6220\n",
      "Epoch [9/10], Step [380/1094], Loss: 0.3683\n",
      "Epoch [9/10], Step [390/1094], Loss: 0.3825\n",
      "Epoch [9/10], Step [400/1094], Loss: 0.2639\n",
      "Epoch [9/10], Step [410/1094], Loss: 0.3992\n",
      "Epoch [9/10], Step [420/1094], Loss: 0.2494\n",
      "Epoch [9/10], Step [430/1094], Loss: 0.3392\n",
      "Epoch [9/10], Step [440/1094], Loss: 0.3179\n",
      "Epoch [9/10], Step [450/1094], Loss: 0.2784\n",
      "Epoch [9/10], Step [460/1094], Loss: 0.3501\n",
      "Epoch [9/10], Step [470/1094], Loss: 0.5837\n",
      "Epoch [9/10], Step [480/1094], Loss: 0.4044\n",
      "Epoch [9/10], Step [490/1094], Loss: 0.3173\n",
      "Epoch [9/10], Step [500/1094], Loss: 0.2937\n",
      "Epoch [9/10], Step [510/1094], Loss: 0.3799\n",
      "Epoch [9/10], Step [520/1094], Loss: 0.2805\n",
      "Epoch [9/10], Step [530/1094], Loss: 0.3219\n",
      "Epoch [9/10], Step [540/1094], Loss: 0.3537\n",
      "Epoch [9/10], Step [550/1094], Loss: 0.4040\n",
      "Epoch [9/10], Step [560/1094], Loss: 0.3585\n",
      "Epoch [9/10], Step [570/1094], Loss: 0.3070\n",
      "Epoch [9/10], Step [580/1094], Loss: 0.6912\n",
      "Epoch [9/10], Step [590/1094], Loss: 0.6041\n",
      "Epoch [9/10], Step [600/1094], Loss: 0.4500\n",
      "Epoch [9/10], Step [610/1094], Loss: 0.6074\n",
      "Epoch [9/10], Step [620/1094], Loss: 0.4731\n",
      "Epoch [9/10], Step [630/1094], Loss: 0.3999\n",
      "Epoch [9/10], Step [640/1094], Loss: 0.3198\n",
      "Epoch [9/10], Step [650/1094], Loss: 0.4676\n",
      "Epoch [9/10], Step [660/1094], Loss: 0.5227\n",
      "Epoch [9/10], Step [670/1094], Loss: 0.2222\n",
      "Epoch [9/10], Step [680/1094], Loss: 0.4406\n",
      "Epoch [9/10], Step [690/1094], Loss: 0.3824\n",
      "Epoch [9/10], Step [700/1094], Loss: 0.4355\n",
      "Epoch [9/10], Step [710/1094], Loss: 0.3587\n",
      "Epoch [9/10], Step [720/1094], Loss: 0.5464\n",
      "Epoch [9/10], Step [730/1094], Loss: 0.4760\n",
      "Epoch [9/10], Step [740/1094], Loss: 0.5129\n",
      "Epoch [9/10], Step [750/1094], Loss: 0.4872\n",
      "Epoch [9/10], Step [760/1094], Loss: 0.2838\n",
      "Epoch [9/10], Step [770/1094], Loss: 0.3413\n",
      "Epoch [9/10], Step [780/1094], Loss: 0.4571\n",
      "Epoch [9/10], Step [790/1094], Loss: 0.4898\n",
      "Epoch [9/10], Step [800/1094], Loss: 0.3294\n",
      "Epoch [9/10], Step [810/1094], Loss: 0.1736\n",
      "Epoch [9/10], Step [820/1094], Loss: 0.4372\n",
      "Epoch [9/10], Step [830/1094], Loss: 0.4009\n",
      "Epoch [9/10], Step [840/1094], Loss: 0.4823\n",
      "Epoch [9/10], Step [850/1094], Loss: 0.3980\n",
      "Epoch [9/10], Step [860/1094], Loss: 0.6251\n",
      "Epoch [9/10], Step [870/1094], Loss: 0.4334\n",
      "Epoch [9/10], Step [880/1094], Loss: 0.3475\n",
      "Epoch [9/10], Step [890/1094], Loss: 0.5565\n",
      "Epoch [9/10], Step [900/1094], Loss: 0.5263\n",
      "Epoch [9/10], Step [910/1094], Loss: 0.2874\n",
      "Epoch [9/10], Step [920/1094], Loss: 0.2636\n",
      "Epoch [9/10], Step [930/1094], Loss: 0.5023\n",
      "Epoch [9/10], Step [940/1094], Loss: 0.5104\n",
      "Epoch [9/10], Step [950/1094], Loss: 0.4415\n",
      "Epoch [9/10], Step [960/1094], Loss: 0.5142\n",
      "Epoch [9/10], Step [970/1094], Loss: 0.3033\n",
      "Epoch [9/10], Step [980/1094], Loss: 0.4856\n",
      "Epoch [9/10], Step [990/1094], Loss: 0.3694\n",
      "Epoch [9/10], Step [1000/1094], Loss: 0.3306\n",
      "Epoch [9/10], Step [1010/1094], Loss: 0.3746\n",
      "Epoch [9/10], Step [1020/1094], Loss: 0.4875\n",
      "Epoch [9/10], Step [1030/1094], Loss: 0.3569\n",
      "Epoch [9/10], Step [1040/1094], Loss: 0.8121\n",
      "Epoch [9/10], Step [1050/1094], Loss: 0.4835\n",
      "Epoch [9/10], Step [1060/1094], Loss: 0.3975\n",
      "Epoch [9/10], Step [1070/1094], Loss: 0.4471\n",
      "Epoch [9/10], Step [1080/1094], Loss: 0.5336\n",
      "Epoch [9/10], Step [1090/1094], Loss: 0.2644\n",
      "Epoch [9/10], Average Loss: 0.4199, Accuracy: 0.8491\n",
      "Class 0: Precision = 0.7814, Recall = 0.8000, F1 Score = 0.7906\n",
      "Class 1: Precision = 0.9768, Recall = 0.9620, F1 Score = 0.9694\n",
      "Class 2: Precision = 0.8636, Recall = 0.8548, F1 Score = 0.8592\n",
      "Class 3: Precision = 0.7570, Recall = 0.8160, F1 Score = 0.7854\n",
      "Class 4: Precision = 0.8419, Recall = 0.9094, F1 Score = 0.8743\n",
      "Class 5: Precision = 0.8059, Recall = 0.6932, F1 Score = 0.7453\n",
      "Class 6: Precision = 0.9241, Recall = 0.9086, F1 Score = 0.9163\n",
      "Epoch [10/10], Step [10/1094], Loss: 0.1614\n",
      "Epoch [10/10], Step [20/1094], Loss: 0.2602\n",
      "Epoch [10/10], Step [30/1094], Loss: 0.3935\n",
      "Epoch [10/10], Step [40/1094], Loss: 0.3153\n",
      "Epoch [10/10], Step [50/1094], Loss: 0.5929\n",
      "Epoch [10/10], Step [60/1094], Loss: 0.2841\n",
      "Epoch [10/10], Step [70/1094], Loss: 0.3557\n",
      "Epoch [10/10], Step [80/1094], Loss: 0.6096\n",
      "Epoch [10/10], Step [90/1094], Loss: 0.3623\n",
      "Epoch [10/10], Step [100/1094], Loss: 0.4365\n",
      "Epoch [10/10], Step [110/1094], Loss: 0.3529\n",
      "Epoch [10/10], Step [120/1094], Loss: 0.2674\n",
      "Epoch [10/10], Step [130/1094], Loss: 0.3214\n",
      "Epoch [10/10], Step [140/1094], Loss: 0.4178\n",
      "Epoch [10/10], Step [150/1094], Loss: 0.6048\n",
      "Epoch [10/10], Step [160/1094], Loss: 0.4612\n",
      "Epoch [10/10], Step [170/1094], Loss: 0.4725\n",
      "Epoch [10/10], Step [180/1094], Loss: 0.7280\n",
      "Epoch [10/10], Step [190/1094], Loss: 0.2706\n",
      "Epoch [10/10], Step [200/1094], Loss: 0.4647\n",
      "Epoch [10/10], Step [210/1094], Loss: 0.2467\n",
      "Epoch [10/10], Step [220/1094], Loss: 0.2573\n",
      "Epoch [10/10], Step [230/1094], Loss: 0.3105\n",
      "Epoch [10/10], Step [240/1094], Loss: 0.5057\n",
      "Epoch [10/10], Step [250/1094], Loss: 0.2268\n",
      "Epoch [10/10], Step [260/1094], Loss: 0.4573\n",
      "Epoch [10/10], Step [270/1094], Loss: 0.4131\n",
      "Epoch [10/10], Step [280/1094], Loss: 0.5933\n",
      "Epoch [10/10], Step [290/1094], Loss: 0.5184\n",
      "Epoch [10/10], Step [300/1094], Loss: 0.4250\n",
      "Epoch [10/10], Step [310/1094], Loss: 0.2288\n",
      "Epoch [10/10], Step [320/1094], Loss: 0.8044\n",
      "Epoch [10/10], Step [330/1094], Loss: 0.3103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Step [340/1094], Loss: 0.3182\n",
      "Epoch [10/10], Step [350/1094], Loss: 0.3191\n",
      "Epoch [10/10], Step [360/1094], Loss: 0.5005\n",
      "Epoch [10/10], Step [370/1094], Loss: 0.6640\n",
      "Epoch [10/10], Step [380/1094], Loss: 0.4195\n",
      "Epoch [10/10], Step [390/1094], Loss: 0.7870\n",
      "Epoch [10/10], Step [400/1094], Loss: 0.5178\n",
      "Epoch [10/10], Step [410/1094], Loss: 0.4687\n",
      "Epoch [10/10], Step [420/1094], Loss: 0.5220\n",
      "Epoch [10/10], Step [430/1094], Loss: 0.4513\n",
      "Epoch [10/10], Step [440/1094], Loss: 0.4466\n",
      "Epoch [10/10], Step [450/1094], Loss: 0.4134\n",
      "Epoch [10/10], Step [460/1094], Loss: 0.5796\n",
      "Epoch [10/10], Step [470/1094], Loss: 0.4052\n",
      "Epoch [10/10], Step [480/1094], Loss: 0.3233\n",
      "Epoch [10/10], Step [490/1094], Loss: 0.4446\n",
      "Epoch [10/10], Step [500/1094], Loss: 0.4643\n",
      "Epoch [10/10], Step [510/1094], Loss: 0.4567\n",
      "Epoch [10/10], Step [520/1094], Loss: 0.5329\n",
      "Epoch [10/10], Step [530/1094], Loss: 0.3009\n",
      "Epoch [10/10], Step [540/1094], Loss: 0.3184\n",
      "Epoch [10/10], Step [550/1094], Loss: 0.3948\n",
      "Epoch [10/10], Step [560/1094], Loss: 0.2567\n",
      "Epoch [10/10], Step [570/1094], Loss: 0.5210\n",
      "Epoch [10/10], Step [580/1094], Loss: 0.1875\n",
      "Epoch [10/10], Step [590/1094], Loss: 0.3178\n",
      "Epoch [10/10], Step [600/1094], Loss: 0.3096\n",
      "Epoch [10/10], Step [610/1094], Loss: 0.3130\n",
      "Epoch [10/10], Step [620/1094], Loss: 0.4689\n",
      "Epoch [10/10], Step [630/1094], Loss: 0.3168\n",
      "Epoch [10/10], Step [640/1094], Loss: 0.3817\n",
      "Epoch [10/10], Step [650/1094], Loss: 0.1446\n",
      "Epoch [10/10], Step [660/1094], Loss: 0.3952\n",
      "Epoch [10/10], Step [670/1094], Loss: 0.1863\n",
      "Epoch [10/10], Step [680/1094], Loss: 0.2051\n",
      "Epoch [10/10], Step [690/1094], Loss: 0.3181\n",
      "Epoch [10/10], Step [700/1094], Loss: 0.2939\n",
      "Epoch [10/10], Step [710/1094], Loss: 0.4892\n",
      "Epoch [10/10], Step [720/1094], Loss: 0.3541\n",
      "Epoch [10/10], Step [730/1094], Loss: 0.3876\n",
      "Epoch [10/10], Step [740/1094], Loss: 0.3425\n",
      "Epoch [10/10], Step [750/1094], Loss: 0.7056\n",
      "Epoch [10/10], Step [760/1094], Loss: 0.6811\n",
      "Epoch [10/10], Step [770/1094], Loss: 0.3074\n",
      "Epoch [10/10], Step [780/1094], Loss: 0.1977\n",
      "Epoch [10/10], Step [790/1094], Loss: 0.2499\n",
      "Epoch [10/10], Step [800/1094], Loss: 0.3353\n",
      "Epoch [10/10], Step [810/1094], Loss: 0.3865\n",
      "Epoch [10/10], Step [820/1094], Loss: 0.3781\n",
      "Epoch [10/10], Step [830/1094], Loss: 0.2930\n",
      "Epoch [10/10], Step [840/1094], Loss: 0.4560\n",
      "Epoch [10/10], Step [850/1094], Loss: 0.4275\n",
      "Epoch [10/10], Step [860/1094], Loss: 0.5188\n",
      "Epoch [10/10], Step [870/1094], Loss: 0.3150\n",
      "Epoch [10/10], Step [880/1094], Loss: 0.2159\n",
      "Epoch [10/10], Step [890/1094], Loss: 0.3565\n",
      "Epoch [10/10], Step [900/1094], Loss: 0.2861\n",
      "Epoch [10/10], Step [910/1094], Loss: 0.4207\n",
      "Epoch [10/10], Step [920/1094], Loss: 0.5435\n",
      "Epoch [10/10], Step [930/1094], Loss: 0.3995\n",
      "Epoch [10/10], Step [940/1094], Loss: 0.4445\n",
      "Epoch [10/10], Step [950/1094], Loss: 0.2694\n",
      "Epoch [10/10], Step [960/1094], Loss: 0.3640\n",
      "Epoch [10/10], Step [970/1094], Loss: 0.1608\n",
      "Epoch [10/10], Step [980/1094], Loss: 0.3571\n",
      "Epoch [10/10], Step [990/1094], Loss: 0.3809\n",
      "Epoch [10/10], Step [1000/1094], Loss: 0.5234\n",
      "Epoch [10/10], Step [1010/1094], Loss: 0.4125\n",
      "Epoch [10/10], Step [1020/1094], Loss: 0.2582\n",
      "Epoch [10/10], Step [1030/1094], Loss: 0.5682\n",
      "Epoch [10/10], Step [1040/1094], Loss: 0.4167\n",
      "Epoch [10/10], Step [1050/1094], Loss: 0.6277\n",
      "Epoch [10/10], Step [1060/1094], Loss: 0.4011\n",
      "Epoch [10/10], Step [1070/1094], Loss: 0.5959\n",
      "Epoch [10/10], Step [1080/1094], Loss: 0.3901\n",
      "Epoch [10/10], Step [1090/1094], Loss: 0.4686\n",
      "Epoch [10/10], Average Loss: 0.4037, Accuracy: 0.8523\n",
      "Class 0: Precision = 0.7854, Recall = 0.7972, F1 Score = 0.7913\n",
      "Class 1: Precision = 0.9761, Recall = 0.9644, F1 Score = 0.9702\n",
      "Class 2: Precision = 0.8603, Recall = 0.8570, F1 Score = 0.8586\n",
      "Class 3: Precision = 0.7662, Recall = 0.8134, F1 Score = 0.7891\n",
      "Class 4: Precision = 0.8539, Recall = 0.9092, F1 Score = 0.8807\n",
      "Class 5: Precision = 0.8063, Recall = 0.7076, F1 Score = 0.7537\n",
      "Class 6: Precision = 0.9203, Recall = 0.9174, F1 Score = 0.9189\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Constants\n",
    "NumPerElement = 1000  # Limit the number of samples to 10 per folder (for testing)\n",
    "sequence_length = 300  # The length of each sequence (number of time steps)\n",
    "\n",
    "input_size = 2  # Real and Imaginary components\n",
    "hidden_size = 128  # Number of hidden units per LSTM layer\n",
    "num_classes = 7  # Number of output classes\n",
    "num_layers = 3  # Number of stacked LSTM layers\n",
    "\n",
    "\n",
    "# Folder paths and corresponding labels\n",
    "folders_and_labels = [\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/4QAM/AWGN',0),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/4QAM/ImpulseNoise',0),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/4QAM/PinkNoise',0),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/4QAM/RayleighNoise',0),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/4QAM/RicianNoise',0),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM/AWGN',1),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM/ImpulseNoise',1),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM/PinkNoise',1),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM/RayleighNoise',1),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM/RicianNoise',1),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM-Flipped/AWGN',2),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM-Flipped/ImpulseNoise',2),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM-Flipped/PinkNoise',2),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM-Flipped/RayleighNoise',2),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM-Flipped/RicianNoise',2),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QPSK/AWGN',3),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QPSK/ImpulseNoise',3),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QPSK/PinkNoise',3),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QPSK/RayleighNoise',3),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QPSK/RicianNoise',3),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK/AWGN',4),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK/ImpulseNoise',4),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK/PinkNoise',4),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK/RayleighNoise',4),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK/RicianNoise',4),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK-Flipped/AWGN',5),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK-Flipped/ImpulseNoise',5),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK-Flipped/PinkNoise',5),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK-Flipped/RayleighNoise',5),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK-Flipped/RicianNoise',5),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/QPSK/AWGN',6),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/QPSK/ImpulseNoise',6),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/QPSK/PinkNoise',6),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/QPSK/RayleighNoise',6),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/QPSK/RicianNoise',6),\n",
    "]\n",
    "\n",
    "# Prepare data tensors\n",
    "im = torch.zeros(NumPerElement * len(folders_and_labels), sequence_length, input_size)  # (samples, time_steps, features)\n",
    "label = torch.zeros(NumPerElement * len(folders_and_labels))\n",
    "\n",
    "# Counters for the samples processed\n",
    "count = -1\n",
    "\n",
    "# Loop through each folder and load the data\n",
    "for folder_dir, folder_label in folders_and_labels:\n",
    "    count_in_folder = 0\n",
    "    for images in os.listdir(folder_dir):\n",
    "        if images != '.DS_Store' and count_in_folder < NumPerElement:\n",
    "            # Load the .mat file\n",
    "            AA = sio.loadmat(os.path.join(folder_dir, images))\n",
    "            count_in_folder += 1\n",
    "            count += 1\n",
    "            for key, value in AA.items():\n",
    "                if isinstance(value, np.ndarray) and np.iscomplexobj(value):\n",
    "                    im[count, :, 0] = torch.from_numpy(value.real[0, :])  # Real part\n",
    "                    im[count, :, 1] = torch.from_numpy(value.imag[0, :].copy())  # Imaginary part\n",
    "                label[count] = folder_label\n",
    "\n",
    "        if count_in_folder >= NumPerElement:  # Stop processing after 10 files per folder\n",
    "            break\n",
    "\n",
    "# Verify the data shape and labels\n",
    "print(f\"Total samples loaded: {count + 1}\")\n",
    "print(f\"Labels for first few samples: {label[:10]}\")\n",
    "\n",
    "# Define the Sequential LSTM Model\n",
    "model = nn.Sequential(\n",
    "    nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True),  # Stacked LSTM layers without dropout\n",
    "    nn.Linear(hidden_size, num_classes)  # Fully connected layer for classification\n",
    ")\n",
    "\n",
    "# Define the device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to the device\n",
    "model = model.to(device)\n",
    "\n",
    "# Convert to dataset and DataLoader\n",
    "dataset = TensorDataset(im, label)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Ensure the model is in training mode\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i, (data, labels) in enumerate(dataloader):\n",
    "        # Move data and labels to the same device as the model\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        lstm_out, _ = model[0](data)  # Only pass through LSTM layer\n",
    "        outputs = model[1](lstm_out[:, -1, :])  # Use output from last time step (last hidden state)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels.long())  # Ensure labels are of type long for classification\n",
    "        total_loss += loss.item() * data.size(0)  # Accumulate loss (weighted by batch size)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        # Collect all predictions and labels for metrics\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "        total_samples += data.size(0)\n",
    "\n",
    "        if (i+1) % 10 == 0:  # Print every 10 batches\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Convert all_preds and all_labels to numpy arrays\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    # Calculate Precision, Recall, F1 for each class, with zero_division set to 0\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average=None, labels=np.arange(num_classes), zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average=None, labels=np.arange(num_classes), zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average=None, labels=np.arange(num_classes), zero_division=0)\n",
    "\n",
    "    # Print the detailed metrics for each class\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {total_loss / total_samples:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    for i in range(num_classes):\n",
    "        print(f\"Class {i}: Precision = {precision[i]:.4f}, Recall = {recall[i]:.4f}, F1 Score = {f1[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfea342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede1ef7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d51cedfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples loaded: 35000\n",
      "Labels for first few samples: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Epoch [1/8], Step [10/1094], Loss: 1.9338\n",
      "Epoch [1/8], Step [20/1094], Loss: 1.8094\n",
      "Epoch [1/8], Step [30/1094], Loss: 1.5211\n",
      "Epoch [1/8], Step [40/1094], Loss: 1.6792\n",
      "Epoch [1/8], Step [50/1094], Loss: 1.4415\n",
      "Epoch [1/8], Step [60/1094], Loss: 1.2855\n",
      "Epoch [1/8], Step [70/1094], Loss: 1.6141\n",
      "Epoch [1/8], Step [80/1094], Loss: 1.6254\n",
      "Epoch [1/8], Step [90/1094], Loss: 1.4762\n",
      "Epoch [1/8], Step [100/1094], Loss: 1.3712\n",
      "Epoch [1/8], Step [110/1094], Loss: 1.7209\n",
      "Epoch [1/8], Step [120/1094], Loss: 1.3735\n",
      "Epoch [1/8], Step [130/1094], Loss: 1.2585\n",
      "Epoch [1/8], Step [140/1094], Loss: 1.2247\n",
      "Epoch [1/8], Step [150/1094], Loss: 0.9469\n",
      "Epoch [1/8], Step [160/1094], Loss: 1.0348\n",
      "Epoch [1/8], Step [170/1094], Loss: 0.9482\n",
      "Epoch [1/8], Step [180/1094], Loss: 0.8139\n",
      "Epoch [1/8], Step [190/1094], Loss: 0.7254\n",
      "Epoch [1/8], Step [200/1094], Loss: 1.3654\n",
      "Epoch [1/8], Step [210/1094], Loss: 0.8587\n",
      "Epoch [1/8], Step [220/1094], Loss: 0.9005\n",
      "Epoch [1/8], Step [230/1094], Loss: 0.8877\n",
      "Epoch [1/8], Step [240/1094], Loss: 0.8895\n",
      "Epoch [1/8], Step [250/1094], Loss: 0.6805\n",
      "Epoch [1/8], Step [260/1094], Loss: 1.1014\n",
      "Epoch [1/8], Step [270/1094], Loss: 0.7749\n",
      "Epoch [1/8], Step [280/1094], Loss: 0.8832\n",
      "Epoch [1/8], Step [290/1094], Loss: 0.9374\n",
      "Epoch [1/8], Step [300/1094], Loss: 0.8657\n",
      "Epoch [1/8], Step [310/1094], Loss: 1.0665\n",
      "Epoch [1/8], Step [320/1094], Loss: 0.8508\n",
      "Epoch [1/8], Step [330/1094], Loss: 0.8624\n",
      "Epoch [1/8], Step [340/1094], Loss: 0.6965\n",
      "Epoch [1/8], Step [350/1094], Loss: 0.8625\n",
      "Epoch [1/8], Step [360/1094], Loss: 0.5549\n",
      "Epoch [1/8], Step [370/1094], Loss: 0.9252\n",
      "Epoch [1/8], Step [380/1094], Loss: 0.6562\n",
      "Epoch [1/8], Step [390/1094], Loss: 0.5771\n",
      "Epoch [1/8], Step [400/1094], Loss: 0.8932\n",
      "Epoch [1/8], Step [410/1094], Loss: 0.5840\n",
      "Epoch [1/8], Step [420/1094], Loss: 1.4336\n",
      "Epoch [1/8], Step [430/1094], Loss: 1.0970\n",
      "Epoch [1/8], Step [440/1094], Loss: 1.0306\n",
      "Epoch [1/8], Step [450/1094], Loss: 1.1609\n",
      "Epoch [1/8], Step [460/1094], Loss: 1.4630\n",
      "Epoch [1/8], Step [470/1094], Loss: 1.4143\n",
      "Epoch [1/8], Step [480/1094], Loss: 1.3792\n",
      "Epoch [1/8], Step [490/1094], Loss: 1.5259\n",
      "Epoch [1/8], Step [500/1094], Loss: 1.4959\n",
      "Epoch [1/8], Step [510/1094], Loss: 1.4048\n",
      "Epoch [1/8], Step [520/1094], Loss: 1.8661\n",
      "Epoch [1/8], Step [530/1094], Loss: 1.7078\n",
      "Epoch [1/8], Step [540/1094], Loss: 1.3581\n",
      "Epoch [1/8], Step [550/1094], Loss: 1.4520\n",
      "Epoch [1/8], Step [560/1094], Loss: 1.6245\n",
      "Epoch [1/8], Step [570/1094], Loss: 1.3468\n",
      "Epoch [1/8], Step [580/1094], Loss: 1.4859\n",
      "Epoch [1/8], Step [590/1094], Loss: 1.6197\n",
      "Epoch [1/8], Step [600/1094], Loss: 1.4733\n",
      "Epoch [1/8], Step [610/1094], Loss: 1.4279\n",
      "Epoch [1/8], Step [620/1094], Loss: 1.4003\n",
      "Epoch [1/8], Step [630/1094], Loss: 1.2738\n",
      "Epoch [1/8], Step [640/1094], Loss: 1.4517\n",
      "Epoch [1/8], Step [650/1094], Loss: 1.3651\n",
      "Epoch [1/8], Step [660/1094], Loss: 1.1654\n",
      "Epoch [1/8], Step [670/1094], Loss: 1.4074\n",
      "Epoch [1/8], Step [680/1094], Loss: 1.4696\n",
      "Epoch [1/8], Step [690/1094], Loss: 1.7832\n",
      "Epoch [1/8], Step [700/1094], Loss: 1.6844\n",
      "Epoch [1/8], Step [710/1094], Loss: 1.3652\n",
      "Epoch [1/8], Step [720/1094], Loss: 1.1766\n",
      "Epoch [1/8], Step [730/1094], Loss: 1.3664\n",
      "Epoch [1/8], Step [740/1094], Loss: 1.2503\n",
      "Epoch [1/8], Step [750/1094], Loss: 1.2271\n",
      "Epoch [1/8], Step [760/1094], Loss: 1.2050\n",
      "Epoch [1/8], Step [770/1094], Loss: 1.3504\n",
      "Epoch [1/8], Step [780/1094], Loss: 1.3295\n",
      "Epoch [1/8], Step [790/1094], Loss: 1.3855\n",
      "Epoch [1/8], Step [800/1094], Loss: 0.9478\n",
      "Epoch [1/8], Step [810/1094], Loss: 1.1911\n",
      "Epoch [1/8], Step [820/1094], Loss: 1.0717\n",
      "Epoch [1/8], Step [830/1094], Loss: 1.3520\n",
      "Epoch [1/8], Step [840/1094], Loss: 1.1084\n",
      "Epoch [1/8], Step [850/1094], Loss: 1.1137\n",
      "Epoch [1/8], Step [860/1094], Loss: 0.9461\n",
      "Epoch [1/8], Step [870/1094], Loss: 0.9567\n",
      "Epoch [1/8], Step [880/1094], Loss: 1.1285\n",
      "Epoch [1/8], Step [890/1094], Loss: 0.9623\n",
      "Epoch [1/8], Step [900/1094], Loss: 1.0274\n",
      "Epoch [1/8], Step [910/1094], Loss: 1.2605\n",
      "Epoch [1/8], Step [920/1094], Loss: 1.3331\n",
      "Epoch [1/8], Step [930/1094], Loss: 1.2631\n",
      "Epoch [1/8], Step [940/1094], Loss: 1.4495\n",
      "Epoch [1/8], Step [950/1094], Loss: 1.4641\n",
      "Epoch [1/8], Step [960/1094], Loss: 1.3163\n",
      "Epoch [1/8], Step [970/1094], Loss: 1.3648\n",
      "Epoch [1/8], Step [980/1094], Loss: 1.2010\n",
      "Epoch [1/8], Step [990/1094], Loss: 1.4040\n",
      "Epoch [1/8], Step [1000/1094], Loss: 1.7542\n",
      "Epoch [1/8], Step [1010/1094], Loss: 1.4941\n",
      "Epoch [1/8], Step [1020/1094], Loss: 1.3827\n",
      "Epoch [1/8], Step [1030/1094], Loss: 1.4622\n",
      "Epoch [1/8], Step [1040/1094], Loss: 1.5882\n",
      "Epoch [1/8], Step [1050/1094], Loss: 1.2652\n",
      "Epoch [1/8], Step [1060/1094], Loss: 1.2944\n",
      "Epoch [1/8], Step [1070/1094], Loss: 1.2848\n",
      "Epoch [1/8], Step [1080/1094], Loss: 1.1739\n",
      "Epoch [1/8], Step [1090/1094], Loss: 1.0051\n",
      "Epoch [1/8], Average Loss: 1.2499, Accuracy: 0.4451\n",
      "Class 0: Precision = 0.6077, Recall = 0.5802, F1 Score = 0.5936\n",
      "Class 1: Precision = 0.3511, Recall = 0.5430, F1 Score = 0.4265\n",
      "Class 2: Precision = 0.7084, Recall = 0.7448, F1 Score = 0.7261\n",
      "Class 3: Precision = 0.3810, Recall = 0.3472, F1 Score = 0.3633\n",
      "Class 4: Precision = 0.2660, Recall = 0.2430, F1 Score = 0.2540\n",
      "Class 5: Precision = 0.4657, Recall = 0.4062, F1 Score = 0.4339\n",
      "Class 6: Precision = 0.3345, Recall = 0.2510, F1 Score = 0.2868\n",
      "Epoch [2/8], Step [10/1094], Loss: 0.8060\n",
      "Epoch [2/8], Step [20/1094], Loss: 0.8416\n",
      "Epoch [2/8], Step [30/1094], Loss: 0.9290\n",
      "Epoch [2/8], Step [40/1094], Loss: 0.8126\n",
      "Epoch [2/8], Step [50/1094], Loss: 0.8588\n",
      "Epoch [2/8], Step [60/1094], Loss: 0.9437\n",
      "Epoch [2/8], Step [70/1094], Loss: 1.1863\n",
      "Epoch [2/8], Step [80/1094], Loss: 0.8552\n",
      "Epoch [2/8], Step [90/1094], Loss: 0.8624\n",
      "Epoch [2/8], Step [100/1094], Loss: 1.0624\n",
      "Epoch [2/8], Step [110/1094], Loss: 1.0154\n",
      "Epoch [2/8], Step [120/1094], Loss: 0.9908\n",
      "Epoch [2/8], Step [130/1094], Loss: 0.7685\n",
      "Epoch [2/8], Step [140/1094], Loss: 0.7751\n",
      "Epoch [2/8], Step [150/1094], Loss: 0.8208\n",
      "Epoch [2/8], Step [160/1094], Loss: 1.0467\n",
      "Epoch [2/8], Step [170/1094], Loss: 0.6667\n",
      "Epoch [2/8], Step [180/1094], Loss: 1.3494\n",
      "Epoch [2/8], Step [190/1094], Loss: 0.8536\n",
      "Epoch [2/8], Step [200/1094], Loss: 0.8267\n",
      "Epoch [2/8], Step [210/1094], Loss: 0.8556\n",
      "Epoch [2/8], Step [220/1094], Loss: 0.9695\n",
      "Epoch [2/8], Step [230/1094], Loss: 0.9350\n",
      "Epoch [2/8], Step [240/1094], Loss: 1.0602\n",
      "Epoch [2/8], Step [250/1094], Loss: 0.8800\n",
      "Epoch [2/8], Step [260/1094], Loss: 1.0792\n",
      "Epoch [2/8], Step [270/1094], Loss: 1.4359\n",
      "Epoch [2/8], Step [280/1094], Loss: 1.0516\n",
      "Epoch [2/8], Step [290/1094], Loss: 1.0484\n",
      "Epoch [2/8], Step [300/1094], Loss: 0.8370\n",
      "Epoch [2/8], Step [310/1094], Loss: 1.0026\n",
      "Epoch [2/8], Step [320/1094], Loss: 1.0192\n",
      "Epoch [2/8], Step [330/1094], Loss: 1.0249\n",
      "Epoch [2/8], Step [340/1094], Loss: 1.1513\n",
      "Epoch [2/8], Step [350/1094], Loss: 1.1120\n",
      "Epoch [2/8], Step [360/1094], Loss: 0.9723\n",
      "Epoch [2/8], Step [370/1094], Loss: 0.8822\n",
      "Epoch [2/8], Step [380/1094], Loss: 1.0521\n",
      "Epoch [2/8], Step [390/1094], Loss: 0.8623\n",
      "Epoch [2/8], Step [400/1094], Loss: 0.9877\n",
      "Epoch [2/8], Step [410/1094], Loss: 0.9747\n",
      "Epoch [2/8], Step [420/1094], Loss: 0.8692\n",
      "Epoch [2/8], Step [430/1094], Loss: 0.9709\n",
      "Epoch [2/8], Step [440/1094], Loss: 0.7535\n",
      "Epoch [2/8], Step [450/1094], Loss: 1.2672\n",
      "Epoch [2/8], Step [460/1094], Loss: 0.9237\n",
      "Epoch [2/8], Step [470/1094], Loss: 0.8148\n",
      "Epoch [2/8], Step [480/1094], Loss: 3.2496\n",
      "Epoch [2/8], Step [490/1094], Loss: 2.1274\n",
      "Epoch [2/8], Step [500/1094], Loss: 1.8924\n",
      "Epoch [2/8], Step [510/1094], Loss: 1.5486\n",
      "Epoch [2/8], Step [520/1094], Loss: 1.7438\n",
      "Epoch [2/8], Step [530/1094], Loss: 1.6495\n",
      "Epoch [2/8], Step [540/1094], Loss: 1.7781\n",
      "Epoch [2/8], Step [550/1094], Loss: 1.8504\n",
      "Epoch [2/8], Step [560/1094], Loss: 1.7768\n",
      "Epoch [2/8], Step [570/1094], Loss: 1.7892\n",
      "Epoch [2/8], Step [580/1094], Loss: 1.7605\n",
      "Epoch [2/8], Step [590/1094], Loss: 1.4256\n",
      "Epoch [2/8], Step [600/1094], Loss: 1.3756\n",
      "Epoch [2/8], Step [610/1094], Loss: 1.6067\n",
      "Epoch [2/8], Step [620/1094], Loss: 1.3534\n",
      "Epoch [2/8], Step [630/1094], Loss: 1.4721\n",
      "Epoch [2/8], Step [640/1094], Loss: 1.5827\n",
      "Epoch [2/8], Step [650/1094], Loss: 1.4649\n",
      "Epoch [2/8], Step [660/1094], Loss: 1.4886\n",
      "Epoch [2/8], Step [670/1094], Loss: 1.5807\n",
      "Epoch [2/8], Step [680/1094], Loss: 1.4608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/8], Step [690/1094], Loss: 1.3184\n",
      "Epoch [2/8], Step [700/1094], Loss: 1.3671\n",
      "Epoch [2/8], Step [710/1094], Loss: 1.5763\n",
      "Epoch [2/8], Step [720/1094], Loss: 1.4702\n",
      "Epoch [2/8], Step [730/1094], Loss: 1.5466\n",
      "Epoch [2/8], Step [740/1094], Loss: 1.4342\n",
      "Epoch [2/8], Step [750/1094], Loss: 1.4468\n",
      "Epoch [2/8], Step [760/1094], Loss: 1.3834\n",
      "Epoch [2/8], Step [770/1094], Loss: 1.3815\n",
      "Epoch [2/8], Step [780/1094], Loss: 1.4819\n",
      "Epoch [2/8], Step [790/1094], Loss: 1.4898\n",
      "Epoch [2/8], Step [800/1094], Loss: 1.3433\n",
      "Epoch [2/8], Step [810/1094], Loss: 1.5867\n",
      "Epoch [2/8], Step [820/1094], Loss: 1.5022\n",
      "Epoch [2/8], Step [830/1094], Loss: 1.4408\n",
      "Epoch [2/8], Step [840/1094], Loss: 1.4163\n",
      "Epoch [2/8], Step [850/1094], Loss: 1.4581\n",
      "Epoch [2/8], Step [860/1094], Loss: 1.4842\n",
      "Epoch [2/8], Step [870/1094], Loss: 1.3534\n",
      "Epoch [2/8], Step [880/1094], Loss: 1.4128\n",
      "Epoch [2/8], Step [890/1094], Loss: 1.4013\n",
      "Epoch [2/8], Step [900/1094], Loss: 1.5630\n",
      "Epoch [2/8], Step [910/1094], Loss: 1.4743\n",
      "Epoch [2/8], Step [920/1094], Loss: 1.3034\n",
      "Epoch [2/8], Step [930/1094], Loss: 1.3276\n",
      "Epoch [2/8], Step [940/1094], Loss: 1.2932\n",
      "Epoch [2/8], Step [950/1094], Loss: 1.3783\n",
      "Epoch [2/8], Step [960/1094], Loss: 1.4661\n",
      "Epoch [2/8], Step [970/1094], Loss: 1.4339\n",
      "Epoch [2/8], Step [980/1094], Loss: 1.2091\n",
      "Epoch [2/8], Step [990/1094], Loss: 1.1263\n",
      "Epoch [2/8], Step [1000/1094], Loss: 1.2209\n",
      "Epoch [2/8], Step [1010/1094], Loss: 1.4267\n",
      "Epoch [2/8], Step [1020/1094], Loss: 1.3862\n",
      "Epoch [2/8], Step [1030/1094], Loss: 1.4966\n",
      "Epoch [2/8], Step [1040/1094], Loss: 1.4096\n",
      "Epoch [2/8], Step [1050/1094], Loss: 1.7159\n",
      "Epoch [2/8], Step [1060/1094], Loss: 1.4177\n",
      "Epoch [2/8], Step [1070/1094], Loss: 1.6617\n",
      "Epoch [2/8], Step [1080/1094], Loss: 1.3595\n",
      "Epoch [2/8], Step [1090/1094], Loss: 1.4731\n",
      "Epoch [2/8], Average Loss: 1.2680, Accuracy: 0.4039\n",
      "Class 0: Precision = 0.5276, Recall = 0.5504, F1 Score = 0.5388\n",
      "Class 1: Precision = 0.2680, Recall = 0.3334, F1 Score = 0.2971\n",
      "Class 2: Precision = 0.5867, Recall = 0.6822, F1 Score = 0.6308\n",
      "Class 3: Precision = 0.4932, Recall = 0.4504, F1 Score = 0.4708\n",
      "Class 4: Precision = 0.2564, Recall = 0.2920, F1 Score = 0.2731\n",
      "Class 5: Precision = 0.5184, Recall = 0.2762, F1 Score = 0.3604\n",
      "Class 6: Precision = 0.2518, Recall = 0.2430, F1 Score = 0.2473\n",
      "Epoch [3/8], Step [10/1094], Loss: 1.4797\n",
      "Epoch [3/8], Step [20/1094], Loss: 1.4947\n",
      "Epoch [3/8], Step [30/1094], Loss: 1.4640\n",
      "Epoch [3/8], Step [40/1094], Loss: 1.3998\n",
      "Epoch [3/8], Step [50/1094], Loss: 1.5002\n",
      "Epoch [3/8], Step [60/1094], Loss: 1.4102\n",
      "Epoch [3/8], Step [70/1094], Loss: 1.6526\n",
      "Epoch [3/8], Step [80/1094], Loss: 1.4503\n",
      "Epoch [3/8], Step [90/1094], Loss: 1.5100\n",
      "Epoch [3/8], Step [100/1094], Loss: 1.4138\n",
      "Epoch [3/8], Step [110/1094], Loss: 1.3358\n",
      "Epoch [3/8], Step [120/1094], Loss: 1.2608\n",
      "Epoch [3/8], Step [130/1094], Loss: 1.4863\n",
      "Epoch [3/8], Step [140/1094], Loss: 1.4361\n",
      "Epoch [3/8], Step [150/1094], Loss: 1.5007\n",
      "Epoch [3/8], Step [160/1094], Loss: 1.3899\n",
      "Epoch [3/8], Step [170/1094], Loss: 1.3931\n",
      "Epoch [3/8], Step [180/1094], Loss: 1.5051\n",
      "Epoch [3/8], Step [190/1094], Loss: 1.4651\n",
      "Epoch [3/8], Step [200/1094], Loss: 1.4758\n",
      "Epoch [3/8], Step [210/1094], Loss: 1.4581\n",
      "Epoch [3/8], Step [220/1094], Loss: 1.6870\n",
      "Epoch [3/8], Step [230/1094], Loss: 1.4619\n",
      "Epoch [3/8], Step [240/1094], Loss: 1.4461\n",
      "Epoch [3/8], Step [250/1094], Loss: 1.3606\n",
      "Epoch [3/8], Step [260/1094], Loss: 1.5591\n",
      "Epoch [3/8], Step [270/1094], Loss: 1.4038\n",
      "Epoch [3/8], Step [280/1094], Loss: 1.5315\n",
      "Epoch [3/8], Step [290/1094], Loss: 1.3774\n",
      "Epoch [3/8], Step [300/1094], Loss: 1.4946\n",
      "Epoch [3/8], Step [310/1094], Loss: 1.6536\n",
      "Epoch [3/8], Step [320/1094], Loss: 1.4389\n",
      "Epoch [3/8], Step [330/1094], Loss: 1.4388\n",
      "Epoch [3/8], Step [340/1094], Loss: 1.4343\n",
      "Epoch [3/8], Step [350/1094], Loss: 1.3257\n",
      "Epoch [3/8], Step [360/1094], Loss: 1.4987\n",
      "Epoch [3/8], Step [370/1094], Loss: 1.3850\n",
      "Epoch [3/8], Step [380/1094], Loss: 1.5403\n",
      "Epoch [3/8], Step [390/1094], Loss: 1.4644\n",
      "Epoch [3/8], Step [400/1094], Loss: 1.2731\n",
      "Epoch [3/8], Step [410/1094], Loss: 1.5022\n",
      "Epoch [3/8], Step [420/1094], Loss: 1.5757\n",
      "Epoch [3/8], Step [430/1094], Loss: 1.2954\n",
      "Epoch [3/8], Step [440/1094], Loss: 1.3702\n",
      "Epoch [3/8], Step [450/1094], Loss: 1.3777\n",
      "Epoch [3/8], Step [460/1094], Loss: 1.4611\n",
      "Epoch [3/8], Step [470/1094], Loss: 1.4687\n",
      "Epoch [3/8], Step [480/1094], Loss: 1.4093\n",
      "Epoch [3/8], Step [490/1094], Loss: 1.3519\n",
      "Epoch [3/8], Step [500/1094], Loss: 1.6965\n",
      "Epoch [3/8], Step [510/1094], Loss: 1.3512\n",
      "Epoch [3/8], Step [520/1094], Loss: 1.3789\n",
      "Epoch [3/8], Step [530/1094], Loss: 1.4493\n",
      "Epoch [3/8], Step [540/1094], Loss: 1.4001\n",
      "Epoch [3/8], Step [550/1094], Loss: 1.3422\n",
      "Epoch [3/8], Step [560/1094], Loss: 1.4031\n",
      "Epoch [3/8], Step [570/1094], Loss: 1.3705\n",
      "Epoch [3/8], Step [580/1094], Loss: 1.5220\n",
      "Epoch [3/8], Step [590/1094], Loss: 1.3796\n",
      "Epoch [3/8], Step [600/1094], Loss: 1.5948\n",
      "Epoch [3/8], Step [610/1094], Loss: 1.4631\n",
      "Epoch [3/8], Step [620/1094], Loss: 1.5952\n",
      "Epoch [3/8], Step [630/1094], Loss: 1.3419\n",
      "Epoch [3/8], Step [640/1094], Loss: 1.5492\n",
      "Epoch [3/8], Step [650/1094], Loss: 1.6080\n",
      "Epoch [3/8], Step [660/1094], Loss: 1.3837\n",
      "Epoch [3/8], Step [670/1094], Loss: 1.4769\n",
      "Epoch [3/8], Step [680/1094], Loss: 1.2604\n",
      "Epoch [3/8], Step [690/1094], Loss: 1.5433\n",
      "Epoch [3/8], Step [700/1094], Loss: 1.3246\n",
      "Epoch [3/8], Step [710/1094], Loss: 1.3430\n",
      "Epoch [3/8], Step [720/1094], Loss: 1.6136\n",
      "Epoch [3/8], Step [730/1094], Loss: 1.2786\n",
      "Epoch [3/8], Step [740/1094], Loss: 1.6268\n",
      "Epoch [3/8], Step [750/1094], Loss: 1.5380\n",
      "Epoch [3/8], Step [760/1094], Loss: 1.6640\n",
      "Epoch [3/8], Step [770/1094], Loss: 1.5050\n",
      "Epoch [3/8], Step [780/1094], Loss: 1.4706\n",
      "Epoch [3/8], Step [790/1094], Loss: 1.4453\n",
      "Epoch [3/8], Step [800/1094], Loss: 1.3254\n",
      "Epoch [3/8], Step [810/1094], Loss: 1.5522\n",
      "Epoch [3/8], Step [820/1094], Loss: 1.5055\n",
      "Epoch [3/8], Step [830/1094], Loss: 1.5548\n",
      "Epoch [3/8], Step [840/1094], Loss: 1.6409\n",
      "Epoch [3/8], Step [850/1094], Loss: 1.4865\n",
      "Epoch [3/8], Step [860/1094], Loss: 1.3285\n",
      "Epoch [3/8], Step [870/1094], Loss: 1.3382\n",
      "Epoch [3/8], Step [880/1094], Loss: 1.6558\n",
      "Epoch [3/8], Step [890/1094], Loss: 1.5418\n",
      "Epoch [3/8], Step [900/1094], Loss: 1.5850\n",
      "Epoch [3/8], Step [910/1094], Loss: 1.5058\n",
      "Epoch [3/8], Step [920/1094], Loss: 1.4254\n",
      "Epoch [3/8], Step [930/1094], Loss: 1.6905\n",
      "Epoch [3/8], Step [940/1094], Loss: 1.4980\n",
      "Epoch [3/8], Step [950/1094], Loss: 1.5962\n",
      "Epoch [3/8], Step [960/1094], Loss: 1.3896\n",
      "Epoch [3/8], Step [970/1094], Loss: 1.4798\n",
      "Epoch [3/8], Step [980/1094], Loss: 1.4795\n",
      "Epoch [3/8], Step [990/1094], Loss: 1.4263\n",
      "Epoch [3/8], Step [1000/1094], Loss: 1.4501\n",
      "Epoch [3/8], Step [1010/1094], Loss: 1.4077\n",
      "Epoch [3/8], Step [1020/1094], Loss: 1.3471\n",
      "Epoch [3/8], Step [1030/1094], Loss: 1.5298\n",
      "Epoch [3/8], Step [1040/1094], Loss: 1.4192\n",
      "Epoch [3/8], Step [1050/1094], Loss: 1.3009\n",
      "Epoch [3/8], Step [1060/1094], Loss: 1.3942\n",
      "Epoch [3/8], Step [1070/1094], Loss: 1.4619\n",
      "Epoch [3/8], Step [1080/1094], Loss: 1.5696\n",
      "Epoch [3/8], Step [1090/1094], Loss: 1.3590\n",
      "Epoch [3/8], Average Loss: 1.4657, Accuracy: 0.2818\n",
      "Class 0: Precision = 0.4468, Recall = 0.3868, F1 Score = 0.4146\n",
      "Class 1: Precision = 0.1974, Recall = 0.2706, F1 Score = 0.2283\n",
      "Class 2: Precision = 0.4658, Recall = 0.6036, F1 Score = 0.5258\n",
      "Class 3: Precision = 0.2049, Recall = 0.2322, F1 Score = 0.2177\n",
      "Class 4: Precision = 0.2110, Recall = 0.2750, F1 Score = 0.2388\n",
      "Class 5: Precision = 0.1780, Recall = 0.0278, F1 Score = 0.0481\n",
      "Class 6: Precision = 0.2020, Recall = 0.1766, F1 Score = 0.1884\n",
      "Epoch [4/8], Step [10/1094], Loss: 1.4963\n",
      "Epoch [4/8], Step [20/1094], Loss: 1.4868\n",
      "Epoch [4/8], Step [30/1094], Loss: 1.4503\n",
      "Epoch [4/8], Step [40/1094], Loss: 1.4775\n",
      "Epoch [4/8], Step [50/1094], Loss: 1.5123\n",
      "Epoch [4/8], Step [60/1094], Loss: 1.6148\n",
      "Epoch [4/8], Step [70/1094], Loss: 1.4892\n",
      "Epoch [4/8], Step [80/1094], Loss: 1.4225\n",
      "Epoch [4/8], Step [90/1094], Loss: 1.5428\n",
      "Epoch [4/8], Step [100/1094], Loss: 1.3150\n",
      "Epoch [4/8], Step [110/1094], Loss: 1.6910\n",
      "Epoch [4/8], Step [120/1094], Loss: 1.5289\n",
      "Epoch [4/8], Step [130/1094], Loss: 1.5434\n",
      "Epoch [4/8], Step [140/1094], Loss: 1.4100\n",
      "Epoch [4/8], Step [150/1094], Loss: 1.5242\n",
      "Epoch [4/8], Step [160/1094], Loss: 1.3510\n",
      "Epoch [4/8], Step [170/1094], Loss: 1.7098\n",
      "Epoch [4/8], Step [180/1094], Loss: 1.4993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/8], Step [190/1094], Loss: 1.4560\n",
      "Epoch [4/8], Step [200/1094], Loss: 1.3575\n",
      "Epoch [4/8], Step [210/1094], Loss: 1.4189\n",
      "Epoch [4/8], Step [220/1094], Loss: 1.3026\n",
      "Epoch [4/8], Step [230/1094], Loss: 1.6936\n",
      "Epoch [4/8], Step [240/1094], Loss: 1.3448\n",
      "Epoch [4/8], Step [250/1094], Loss: 1.3993\n",
      "Epoch [4/8], Step [260/1094], Loss: 1.3714\n",
      "Epoch [4/8], Step [270/1094], Loss: 1.5089\n",
      "Epoch [4/8], Step [280/1094], Loss: 1.6169\n",
      "Epoch [4/8], Step [290/1094], Loss: 1.6234\n",
      "Epoch [4/8], Step [300/1094], Loss: 1.4722\n",
      "Epoch [4/8], Step [310/1094], Loss: 1.4726\n",
      "Epoch [4/8], Step [320/1094], Loss: 1.4009\n",
      "Epoch [4/8], Step [330/1094], Loss: 1.4979\n",
      "Epoch [4/8], Step [340/1094], Loss: 1.5111\n",
      "Epoch [4/8], Step [350/1094], Loss: 1.4325\n",
      "Epoch [4/8], Step [360/1094], Loss: 1.6767\n",
      "Epoch [4/8], Step [370/1094], Loss: 1.4846\n",
      "Epoch [4/8], Step [380/1094], Loss: 1.4347\n",
      "Epoch [4/8], Step [390/1094], Loss: 1.5120\n",
      "Epoch [4/8], Step [400/1094], Loss: 1.4101\n",
      "Epoch [4/8], Step [410/1094], Loss: 1.4074\n",
      "Epoch [4/8], Step [420/1094], Loss: 1.6096\n",
      "Epoch [4/8], Step [430/1094], Loss: 1.4201\n",
      "Epoch [4/8], Step [440/1094], Loss: 1.2889\n",
      "Epoch [4/8], Step [450/1094], Loss: 1.5650\n",
      "Epoch [4/8], Step [460/1094], Loss: 1.3278\n",
      "Epoch [4/8], Step [470/1094], Loss: 1.3666\n",
      "Epoch [4/8], Step [480/1094], Loss: 1.5549\n",
      "Epoch [4/8], Step [490/1094], Loss: 1.3995\n",
      "Epoch [4/8], Step [500/1094], Loss: 1.5117\n",
      "Epoch [4/8], Step [510/1094], Loss: 1.5007\n",
      "Epoch [4/8], Step [520/1094], Loss: 1.4266\n",
      "Epoch [4/8], Step [530/1094], Loss: 1.4106\n",
      "Epoch [4/8], Step [540/1094], Loss: 1.3933\n",
      "Epoch [4/8], Step [550/1094], Loss: 1.4253\n",
      "Epoch [4/8], Step [560/1094], Loss: 1.5338\n",
      "Epoch [4/8], Step [570/1094], Loss: 1.6651\n",
      "Epoch [4/8], Step [580/1094], Loss: 1.4363\n",
      "Epoch [4/8], Step [590/1094], Loss: 1.3986\n",
      "Epoch [4/8], Step [600/1094], Loss: 1.4035\n",
      "Epoch [4/8], Step [610/1094], Loss: 1.4137\n",
      "Epoch [4/8], Step [620/1094], Loss: 1.4238\n",
      "Epoch [4/8], Step [630/1094], Loss: 1.6618\n",
      "Epoch [4/8], Step [640/1094], Loss: 1.7196\n",
      "Epoch [4/8], Step [650/1094], Loss: 1.4701\n",
      "Epoch [4/8], Step [660/1094], Loss: 1.5324\n",
      "Epoch [4/8], Step [670/1094], Loss: 1.4196\n",
      "Epoch [4/8], Step [680/1094], Loss: 1.4389\n",
      "Epoch [4/8], Step [690/1094], Loss: 1.3989\n",
      "Epoch [4/8], Step [700/1094], Loss: 1.5496\n",
      "Epoch [4/8], Step [710/1094], Loss: 1.5017\n",
      "Epoch [4/8], Step [720/1094], Loss: 1.4180\n",
      "Epoch [4/8], Step [730/1094], Loss: 1.3569\n",
      "Epoch [4/8], Step [740/1094], Loss: 1.3976\n",
      "Epoch [4/8], Step [750/1094], Loss: 1.5986\n",
      "Epoch [4/8], Step [760/1094], Loss: 1.2697\n",
      "Epoch [4/8], Step [770/1094], Loss: 1.4132\n",
      "Epoch [4/8], Step [780/1094], Loss: 1.6129\n",
      "Epoch [4/8], Step [790/1094], Loss: 1.4917\n",
      "Epoch [4/8], Step [800/1094], Loss: 1.4170\n",
      "Epoch [4/8], Step [810/1094], Loss: 1.4021\n",
      "Epoch [4/8], Step [820/1094], Loss: 1.4049\n",
      "Epoch [4/8], Step [830/1094], Loss: 1.4450\n",
      "Epoch [4/8], Step [840/1094], Loss: 1.5309\n",
      "Epoch [4/8], Step [850/1094], Loss: 1.3072\n",
      "Epoch [4/8], Step [860/1094], Loss: 1.7456\n",
      "Epoch [4/8], Step [870/1094], Loss: 1.4433\n",
      "Epoch [4/8], Step [880/1094], Loss: 1.2644\n",
      "Epoch [4/8], Step [890/1094], Loss: 1.3164\n",
      "Epoch [4/8], Step [900/1094], Loss: 1.4939\n",
      "Epoch [4/8], Step [910/1094], Loss: 1.3844\n",
      "Epoch [4/8], Step [920/1094], Loss: 1.5582\n",
      "Epoch [4/8], Step [930/1094], Loss: 1.4036\n",
      "Epoch [4/8], Step [940/1094], Loss: 1.4890\n",
      "Epoch [4/8], Step [950/1094], Loss: 1.3514\n",
      "Epoch [4/8], Step [960/1094], Loss: 1.5417\n",
      "Epoch [4/8], Step [970/1094], Loss: 1.6095\n",
      "Epoch [4/8], Step [980/1094], Loss: 1.4629\n",
      "Epoch [4/8], Step [990/1094], Loss: 1.4450\n",
      "Epoch [4/8], Step [1000/1094], Loss: 1.2736\n",
      "Epoch [4/8], Step [1010/1094], Loss: 1.3820\n",
      "Epoch [4/8], Step [1020/1094], Loss: 1.4821\n",
      "Epoch [4/8], Step [1030/1094], Loss: 1.4637\n",
      "Epoch [4/8], Step [1040/1094], Loss: 1.6838\n",
      "Epoch [4/8], Step [1050/1094], Loss: 1.3805\n",
      "Epoch [4/8], Step [1060/1094], Loss: 1.4984\n",
      "Epoch [4/8], Step [1070/1094], Loss: 1.6484\n",
      "Epoch [4/8], Step [1080/1094], Loss: 1.3246\n",
      "Epoch [4/8], Step [1090/1094], Loss: 1.4383\n",
      "Epoch [4/8], Average Loss: 1.4653, Accuracy: 0.2776\n",
      "Class 0: Precision = 0.4384, Recall = 0.3652, F1 Score = 0.3985\n",
      "Class 1: Precision = 0.2066, Recall = 0.3000, F1 Score = 0.2447\n",
      "Class 2: Precision = 0.4569, Recall = 0.6078, F1 Score = 0.5217\n",
      "Class 3: Precision = 0.2039, Recall = 0.2492, F1 Score = 0.2243\n",
      "Class 4: Precision = 0.2021, Recall = 0.2670, F1 Score = 0.2301\n",
      "Class 5: Precision = 0.1676, Recall = 0.0228, F1 Score = 0.0401\n",
      "Class 6: Precision = 0.1863, Recall = 0.1314, F1 Score = 0.1541\n",
      "Epoch [5/8], Step [10/1094], Loss: 1.3876\n",
      "Epoch [5/8], Step [20/1094], Loss: 1.5551\n",
      "Epoch [5/8], Step [30/1094], Loss: 1.3865\n",
      "Epoch [5/8], Step [40/1094], Loss: 1.2577\n",
      "Epoch [5/8], Step [50/1094], Loss: 1.3541\n",
      "Epoch [5/8], Step [60/1094], Loss: 1.5428\n",
      "Epoch [5/8], Step [70/1094], Loss: 1.3640\n",
      "Epoch [5/8], Step [80/1094], Loss: 1.5795\n",
      "Epoch [5/8], Step [90/1094], Loss: 1.5131\n",
      "Epoch [5/8], Step [100/1094], Loss: 1.2585\n",
      "Epoch [5/8], Step [110/1094], Loss: 1.6464\n",
      "Epoch [5/8], Step [120/1094], Loss: 1.4780\n",
      "Epoch [5/8], Step [130/1094], Loss: 1.4538\n",
      "Epoch [5/8], Step [140/1094], Loss: 1.3857\n",
      "Epoch [5/8], Step [150/1094], Loss: 1.6097\n",
      "Epoch [5/8], Step [160/1094], Loss: 1.5510\n",
      "Epoch [5/8], Step [170/1094], Loss: 1.5350\n",
      "Epoch [5/8], Step [180/1094], Loss: 1.7094\n",
      "Epoch [5/8], Step [190/1094], Loss: 1.4247\n",
      "Epoch [5/8], Step [200/1094], Loss: 1.7078\n",
      "Epoch [5/8], Step [210/1094], Loss: 1.3894\n",
      "Epoch [5/8], Step [220/1094], Loss: 1.7511\n",
      "Epoch [5/8], Step [230/1094], Loss: 1.5038\n",
      "Epoch [5/8], Step [240/1094], Loss: 1.5022\n",
      "Epoch [5/8], Step [250/1094], Loss: 1.5582\n",
      "Epoch [5/8], Step [260/1094], Loss: 1.4956\n",
      "Epoch [5/8], Step [270/1094], Loss: 1.3743\n",
      "Epoch [5/8], Step [280/1094], Loss: 1.4030\n",
      "Epoch [5/8], Step [290/1094], Loss: 1.4974\n",
      "Epoch [5/8], Step [300/1094], Loss: 1.5911\n",
      "Epoch [5/8], Step [310/1094], Loss: 1.4439\n",
      "Epoch [5/8], Step [320/1094], Loss: 1.5758\n",
      "Epoch [5/8], Step [330/1094], Loss: 1.3481\n",
      "Epoch [5/8], Step [340/1094], Loss: 1.3893\n",
      "Epoch [5/8], Step [350/1094], Loss: 1.3451\n",
      "Epoch [5/8], Step [360/1094], Loss: 1.4586\n",
      "Epoch [5/8], Step [370/1094], Loss: 1.5370\n",
      "Epoch [5/8], Step [380/1094], Loss: 1.3840\n",
      "Epoch [5/8], Step [390/1094], Loss: 1.4446\n",
      "Epoch [5/8], Step [400/1094], Loss: 1.3869\n",
      "Epoch [5/8], Step [410/1094], Loss: 1.4045\n",
      "Epoch [5/8], Step [420/1094], Loss: 1.3965\n",
      "Epoch [5/8], Step [430/1094], Loss: 1.5054\n",
      "Epoch [5/8], Step [440/1094], Loss: 1.4709\n",
      "Epoch [5/8], Step [450/1094], Loss: 1.2774\n",
      "Epoch [5/8], Step [460/1094], Loss: 1.5176\n",
      "Epoch [5/8], Step [470/1094], Loss: 1.3873\n",
      "Epoch [5/8], Step [480/1094], Loss: 1.4320\n",
      "Epoch [5/8], Step [490/1094], Loss: 1.4235\n",
      "Epoch [5/8], Step [500/1094], Loss: 1.3814\n",
      "Epoch [5/8], Step [510/1094], Loss: 1.4450\n",
      "Epoch [5/8], Step [520/1094], Loss: 1.5026\n",
      "Epoch [5/8], Step [530/1094], Loss: 1.4506\n",
      "Epoch [5/8], Step [540/1094], Loss: 1.3186\n",
      "Epoch [5/8], Step [550/1094], Loss: 1.7237\n",
      "Epoch [5/8], Step [560/1094], Loss: 1.4788\n",
      "Epoch [5/8], Step [570/1094], Loss: 1.5604\n",
      "Epoch [5/8], Step [580/1094], Loss: 1.3516\n",
      "Epoch [5/8], Step [590/1094], Loss: 1.3770\n",
      "Epoch [5/8], Step [600/1094], Loss: 1.4677\n",
      "Epoch [5/8], Step [610/1094], Loss: 1.6093\n",
      "Epoch [5/8], Step [620/1094], Loss: 1.6613\n",
      "Epoch [5/8], Step [630/1094], Loss: 1.4080\n",
      "Epoch [5/8], Step [640/1094], Loss: 1.6025\n",
      "Epoch [5/8], Step [650/1094], Loss: 1.3010\n",
      "Epoch [5/8], Step [660/1094], Loss: 1.4955\n",
      "Epoch [5/8], Step [670/1094], Loss: 1.6128\n",
      "Epoch [5/8], Step [680/1094], Loss: 1.4348\n",
      "Epoch [5/8], Step [690/1094], Loss: 1.4869\n",
      "Epoch [5/8], Step [700/1094], Loss: 1.4704\n",
      "Epoch [5/8], Step [710/1094], Loss: 1.4046\n",
      "Epoch [5/8], Step [720/1094], Loss: 1.3262\n",
      "Epoch [5/8], Step [730/1094], Loss: 1.3933\n",
      "Epoch [5/8], Step [740/1094], Loss: 1.4983\n",
      "Epoch [5/8], Step [750/1094], Loss: 1.3070\n",
      "Epoch [5/8], Step [760/1094], Loss: 1.4378\n",
      "Epoch [5/8], Step [770/1094], Loss: 1.4505\n",
      "Epoch [5/8], Step [780/1094], Loss: 1.3556\n",
      "Epoch [5/8], Step [790/1094], Loss: 1.5637\n",
      "Epoch [5/8], Step [800/1094], Loss: 1.4191\n",
      "Epoch [5/8], Step [810/1094], Loss: 1.5224\n",
      "Epoch [5/8], Step [820/1094], Loss: 1.5207\n",
      "Epoch [5/8], Step [830/1094], Loss: 1.5885\n",
      "Epoch [5/8], Step [840/1094], Loss: 1.4059\n",
      "Epoch [5/8], Step [850/1094], Loss: 1.3259\n",
      "Epoch [5/8], Step [860/1094], Loss: 1.5074\n",
      "Epoch [5/8], Step [870/1094], Loss: 1.3307\n",
      "Epoch [5/8], Step [880/1094], Loss: 1.4691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/8], Step [890/1094], Loss: 1.2994\n",
      "Epoch [5/8], Step [900/1094], Loss: 1.3592\n",
      "Epoch [5/8], Step [910/1094], Loss: 1.5257\n",
      "Epoch [5/8], Step [920/1094], Loss: 1.4805\n",
      "Epoch [5/8], Step [930/1094], Loss: 1.6389\n",
      "Epoch [5/8], Step [940/1094], Loss: 1.5629\n",
      "Epoch [5/8], Step [950/1094], Loss: 1.5483\n",
      "Epoch [5/8], Step [960/1094], Loss: 1.4697\n",
      "Epoch [5/8], Step [970/1094], Loss: 1.4589\n",
      "Epoch [5/8], Step [980/1094], Loss: 1.6202\n",
      "Epoch [5/8], Step [990/1094], Loss: 1.4010\n",
      "Epoch [5/8], Step [1000/1094], Loss: 1.4973\n",
      "Epoch [5/8], Step [1010/1094], Loss: 1.2816\n",
      "Epoch [5/8], Step [1020/1094], Loss: 1.4067\n",
      "Epoch [5/8], Step [1030/1094], Loss: 1.5466\n",
      "Epoch [5/8], Step [1040/1094], Loss: 1.5844\n",
      "Epoch [5/8], Step [1050/1094], Loss: 1.4809\n",
      "Epoch [5/8], Step [1060/1094], Loss: 1.4917\n",
      "Epoch [5/8], Step [1070/1094], Loss: 1.4998\n",
      "Epoch [5/8], Step [1080/1094], Loss: 1.5333\n",
      "Epoch [5/8], Step [1090/1094], Loss: 1.4574\n",
      "Epoch [5/8], Average Loss: 1.4647, Accuracy: 0.2813\n",
      "Class 0: Precision = 0.4386, Recall = 0.3768, F1 Score = 0.4054\n",
      "Class 1: Precision = 0.2083, Recall = 0.3300, F1 Score = 0.2554\n",
      "Class 2: Precision = 0.4565, Recall = 0.5958, F1 Score = 0.5169\n",
      "Class 3: Precision = 0.2027, Recall = 0.1456, F1 Score = 0.1695\n",
      "Class 4: Precision = 0.2103, Recall = 0.2714, F1 Score = 0.2370\n",
      "Class 5: Precision = 0.1706, Recall = 0.0218, F1 Score = 0.0387\n",
      "Class 6: Precision = 0.2045, Recall = 0.2280, F1 Score = 0.2156\n",
      "Epoch [6/8], Step [10/1094], Loss: 1.5194\n",
      "Epoch [6/8], Step [20/1094], Loss: 1.3752\n",
      "Epoch [6/8], Step [30/1094], Loss: 1.6425\n",
      "Epoch [6/8], Step [40/1094], Loss: 1.4986\n",
      "Epoch [6/8], Step [50/1094], Loss: 1.5055\n",
      "Epoch [6/8], Step [60/1094], Loss: 1.4765\n",
      "Epoch [6/8], Step [70/1094], Loss: 1.4131\n",
      "Epoch [6/8], Step [80/1094], Loss: 1.5111\n",
      "Epoch [6/8], Step [90/1094], Loss: 1.3901\n",
      "Epoch [6/8], Step [100/1094], Loss: 1.4713\n",
      "Epoch [6/8], Step [110/1094], Loss: 1.3927\n",
      "Epoch [6/8], Step [120/1094], Loss: 1.3816\n",
      "Epoch [6/8], Step [130/1094], Loss: 1.5702\n",
      "Epoch [6/8], Step [140/1094], Loss: 1.4797\n",
      "Epoch [6/8], Step [150/1094], Loss: 1.4461\n",
      "Epoch [6/8], Step [160/1094], Loss: 1.5234\n",
      "Epoch [6/8], Step [170/1094], Loss: 1.4681\n",
      "Epoch [6/8], Step [180/1094], Loss: 1.6833\n",
      "Epoch [6/8], Step [190/1094], Loss: 1.4617\n",
      "Epoch [6/8], Step [200/1094], Loss: 1.5110\n",
      "Epoch [6/8], Step [210/1094], Loss: 1.4224\n",
      "Epoch [6/8], Step [220/1094], Loss: 1.4275\n",
      "Epoch [6/8], Step [230/1094], Loss: 1.5947\n",
      "Epoch [6/8], Step [240/1094], Loss: 1.4464\n",
      "Epoch [6/8], Step [250/1094], Loss: 1.5322\n",
      "Epoch [6/8], Step [260/1094], Loss: 1.6108\n",
      "Epoch [6/8], Step [270/1094], Loss: 1.4322\n",
      "Epoch [6/8], Step [280/1094], Loss: 1.4356\n",
      "Epoch [6/8], Step [290/1094], Loss: 1.5730\n",
      "Epoch [6/8], Step [300/1094], Loss: 1.4875\n",
      "Epoch [6/8], Step [310/1094], Loss: 1.4364\n",
      "Epoch [6/8], Step [320/1094], Loss: 1.4301\n",
      "Epoch [6/8], Step [330/1094], Loss: 1.4128\n",
      "Epoch [6/8], Step [340/1094], Loss: 1.4991\n",
      "Epoch [6/8], Step [350/1094], Loss: 1.5004\n",
      "Epoch [6/8], Step [360/1094], Loss: 1.6354\n",
      "Epoch [6/8], Step [370/1094], Loss: 1.4508\n",
      "Epoch [6/8], Step [380/1094], Loss: 1.4648\n",
      "Epoch [6/8], Step [390/1094], Loss: 1.4080\n",
      "Epoch [6/8], Step [400/1094], Loss: 1.6262\n",
      "Epoch [6/8], Step [410/1094], Loss: 1.5435\n",
      "Epoch [6/8], Step [420/1094], Loss: 1.3580\n",
      "Epoch [6/8], Step [430/1094], Loss: 1.4402\n",
      "Epoch [6/8], Step [440/1094], Loss: 1.3201\n",
      "Epoch [6/8], Step [450/1094], Loss: 1.4915\n",
      "Epoch [6/8], Step [460/1094], Loss: 1.3368\n",
      "Epoch [6/8], Step [470/1094], Loss: 1.3918\n",
      "Epoch [6/8], Step [480/1094], Loss: 1.5588\n",
      "Epoch [6/8], Step [490/1094], Loss: 1.5619\n",
      "Epoch [6/8], Step [500/1094], Loss: 1.3865\n",
      "Epoch [6/8], Step [510/1094], Loss: 1.5944\n",
      "Epoch [6/8], Step [520/1094], Loss: 1.3556\n",
      "Epoch [6/8], Step [530/1094], Loss: 1.4129\n",
      "Epoch [6/8], Step [540/1094], Loss: 1.4286\n",
      "Epoch [6/8], Step [550/1094], Loss: 1.4143\n",
      "Epoch [6/8], Step [560/1094], Loss: 1.3237\n",
      "Epoch [6/8], Step [570/1094], Loss: 1.4709\n",
      "Epoch [6/8], Step [580/1094], Loss: 1.4158\n",
      "Epoch [6/8], Step [590/1094], Loss: 1.3539\n",
      "Epoch [6/8], Step [600/1094], Loss: 1.5109\n",
      "Epoch [6/8], Step [610/1094], Loss: 1.3543\n",
      "Epoch [6/8], Step [620/1094], Loss: 1.4176\n",
      "Epoch [6/8], Step [630/1094], Loss: 1.4949\n",
      "Epoch [6/8], Step [640/1094], Loss: 1.3040\n",
      "Epoch [6/8], Step [650/1094], Loss: 1.4017\n",
      "Epoch [6/8], Step [660/1094], Loss: 1.4909\n",
      "Epoch [6/8], Step [670/1094], Loss: 1.4377\n",
      "Epoch [6/8], Step [680/1094], Loss: 1.3538\n",
      "Epoch [6/8], Step [690/1094], Loss: 1.4672\n",
      "Epoch [6/8], Step [700/1094], Loss: 1.5782\n",
      "Epoch [6/8], Step [710/1094], Loss: 1.3719\n",
      "Epoch [6/8], Step [720/1094], Loss: 1.5269\n",
      "Epoch [6/8], Step [730/1094], Loss: 1.3638\n",
      "Epoch [6/8], Step [740/1094], Loss: 1.6072\n",
      "Epoch [6/8], Step [750/1094], Loss: 1.4354\n",
      "Epoch [6/8], Step [760/1094], Loss: 1.4543\n",
      "Epoch [6/8], Step [770/1094], Loss: 1.3689\n",
      "Epoch [6/8], Step [780/1094], Loss: 1.4454\n",
      "Epoch [6/8], Step [790/1094], Loss: 1.3629\n",
      "Epoch [6/8], Step [800/1094], Loss: 1.4215\n",
      "Epoch [6/8], Step [810/1094], Loss: 1.3451\n",
      "Epoch [6/8], Step [820/1094], Loss: 1.7068\n",
      "Epoch [6/8], Step [830/1094], Loss: 1.4959\n",
      "Epoch [6/8], Step [840/1094], Loss: 1.4661\n",
      "Epoch [6/8], Step [850/1094], Loss: 1.4060\n",
      "Epoch [6/8], Step [860/1094], Loss: 1.4462\n",
      "Epoch [6/8], Step [870/1094], Loss: 1.6088\n",
      "Epoch [6/8], Step [880/1094], Loss: 1.3947\n",
      "Epoch [6/8], Step [890/1094], Loss: 1.3908\n",
      "Epoch [6/8], Step [900/1094], Loss: 1.2221\n",
      "Epoch [6/8], Step [910/1094], Loss: 1.4145\n",
      "Epoch [6/8], Step [920/1094], Loss: 1.2928\n",
      "Epoch [6/8], Step [930/1094], Loss: 1.2730\n",
      "Epoch [6/8], Step [940/1094], Loss: 1.4796\n",
      "Epoch [6/8], Step [950/1094], Loss: 1.5717\n",
      "Epoch [6/8], Step [960/1094], Loss: 1.5445\n",
      "Epoch [6/8], Step [970/1094], Loss: 1.6239\n",
      "Epoch [6/8], Step [980/1094], Loss: 1.3937\n",
      "Epoch [6/8], Step [990/1094], Loss: 1.3267\n",
      "Epoch [6/8], Step [1000/1094], Loss: 1.5337\n",
      "Epoch [6/8], Step [1010/1094], Loss: 1.5220\n",
      "Epoch [6/8], Step [1020/1094], Loss: 1.5529\n",
      "Epoch [6/8], Step [1030/1094], Loss: 1.5493\n",
      "Epoch [6/8], Step [1040/1094], Loss: 1.4411\n",
      "Epoch [6/8], Step [1050/1094], Loss: 1.3269\n",
      "Epoch [6/8], Step [1060/1094], Loss: 1.5436\n",
      "Epoch [6/8], Step [1070/1094], Loss: 1.3922\n",
      "Epoch [6/8], Step [1080/1094], Loss: 1.3834\n",
      "Epoch [6/8], Step [1090/1094], Loss: 1.5619\n",
      "Epoch [6/8], Average Loss: 1.4649, Accuracy: 0.2790\n",
      "Class 0: Precision = 0.4383, Recall = 0.4014, F1 Score = 0.4190\n",
      "Class 1: Precision = 0.2001, Recall = 0.2726, F1 Score = 0.2308\n",
      "Class 2: Precision = 0.4595, Recall = 0.5736, F1 Score = 0.5102\n",
      "Class 3: Precision = 0.1925, Recall = 0.1444, F1 Score = 0.1650\n",
      "Class 4: Precision = 0.2106, Recall = 0.3652, F1 Score = 0.2672\n",
      "Class 5: Precision = 0.1765, Recall = 0.0144, F1 Score = 0.0266\n",
      "Class 6: Precision = 0.1995, Recall = 0.1812, F1 Score = 0.1899\n",
      "Epoch [7/8], Step [10/1094], Loss: 1.4415\n",
      "Epoch [7/8], Step [20/1094], Loss: 1.4247\n",
      "Epoch [7/8], Step [30/1094], Loss: 1.3837\n",
      "Epoch [7/8], Step [40/1094], Loss: 1.4131\n",
      "Epoch [7/8], Step [50/1094], Loss: 1.4224\n",
      "Epoch [7/8], Step [60/1094], Loss: 1.4379\n",
      "Epoch [7/8], Step [70/1094], Loss: 1.4534\n",
      "Epoch [7/8], Step [80/1094], Loss: 1.4774\n",
      "Epoch [7/8], Step [90/1094], Loss: 1.4651\n",
      "Epoch [7/8], Step [100/1094], Loss: 1.4800\n",
      "Epoch [7/8], Step [110/1094], Loss: 1.4429\n",
      "Epoch [7/8], Step [120/1094], Loss: 1.4088\n",
      "Epoch [7/8], Step [130/1094], Loss: 1.4163\n",
      "Epoch [7/8], Step [140/1094], Loss: 1.5909\n",
      "Epoch [7/8], Step [150/1094], Loss: 1.6746\n",
      "Epoch [7/8], Step [160/1094], Loss: 1.6581\n",
      "Epoch [7/8], Step [170/1094], Loss: 1.3568\n",
      "Epoch [7/8], Step [180/1094], Loss: 1.5942\n",
      "Epoch [7/8], Step [190/1094], Loss: 1.3198\n",
      "Epoch [7/8], Step [200/1094], Loss: 1.5568\n",
      "Epoch [7/8], Step [210/1094], Loss: 1.4559\n",
      "Epoch [7/8], Step [220/1094], Loss: 1.4032\n",
      "Epoch [7/8], Step [230/1094], Loss: 1.4534\n",
      "Epoch [7/8], Step [240/1094], Loss: 1.5580\n",
      "Epoch [7/8], Step [250/1094], Loss: 1.7246\n",
      "Epoch [7/8], Step [260/1094], Loss: 1.3328\n",
      "Epoch [7/8], Step [270/1094], Loss: 1.4671\n",
      "Epoch [7/8], Step [280/1094], Loss: 1.3638\n",
      "Epoch [7/8], Step [290/1094], Loss: 1.5039\n",
      "Epoch [7/8], Step [300/1094], Loss: 1.4409\n",
      "Epoch [7/8], Step [310/1094], Loss: 1.4858\n",
      "Epoch [7/8], Step [320/1094], Loss: 1.3565\n",
      "Epoch [7/8], Step [330/1094], Loss: 1.5528\n",
      "Epoch [7/8], Step [340/1094], Loss: 1.3772\n",
      "Epoch [7/8], Step [350/1094], Loss: 1.4037\n",
      "Epoch [7/8], Step [360/1094], Loss: 1.4140\n",
      "Epoch [7/8], Step [370/1094], Loss: 1.7173\n",
      "Epoch [7/8], Step [380/1094], Loss: 1.4643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/8], Step [390/1094], Loss: 1.5353\n",
      "Epoch [7/8], Step [400/1094], Loss: 1.5396\n",
      "Epoch [7/8], Step [410/1094], Loss: 1.6389\n",
      "Epoch [7/8], Step [420/1094], Loss: 1.3563\n",
      "Epoch [7/8], Step [430/1094], Loss: 1.4516\n",
      "Epoch [7/8], Step [440/1094], Loss: 1.4062\n",
      "Epoch [7/8], Step [450/1094], Loss: 1.4152\n",
      "Epoch [7/8], Step [460/1094], Loss: 1.4586\n",
      "Epoch [7/8], Step [470/1094], Loss: 1.3433\n",
      "Epoch [7/8], Step [480/1094], Loss: 1.2959\n",
      "Epoch [7/8], Step [490/1094], Loss: 1.3616\n",
      "Epoch [7/8], Step [500/1094], Loss: 1.3820\n",
      "Epoch [7/8], Step [510/1094], Loss: 1.4886\n",
      "Epoch [7/8], Step [520/1094], Loss: 1.3685\n",
      "Epoch [7/8], Step [530/1094], Loss: 1.5648\n",
      "Epoch [7/8], Step [540/1094], Loss: 1.4550\n",
      "Epoch [7/8], Step [550/1094], Loss: 1.4551\n",
      "Epoch [7/8], Step [560/1094], Loss: 1.4625\n",
      "Epoch [7/8], Step [570/1094], Loss: 1.3453\n",
      "Epoch [7/8], Step [580/1094], Loss: 1.3708\n",
      "Epoch [7/8], Step [590/1094], Loss: 1.5102\n",
      "Epoch [7/8], Step [600/1094], Loss: 1.5439\n",
      "Epoch [7/8], Step [610/1094], Loss: 1.8074\n",
      "Epoch [7/8], Step [620/1094], Loss: 1.4187\n",
      "Epoch [7/8], Step [630/1094], Loss: 1.3200\n",
      "Epoch [7/8], Step [640/1094], Loss: 1.5744\n",
      "Epoch [7/8], Step [650/1094], Loss: 1.3492\n",
      "Epoch [7/8], Step [660/1094], Loss: 1.4058\n",
      "Epoch [7/8], Step [670/1094], Loss: 1.4915\n",
      "Epoch [7/8], Step [680/1094], Loss: 1.4560\n",
      "Epoch [7/8], Step [690/1094], Loss: 1.4556\n",
      "Epoch [7/8], Step [700/1094], Loss: 1.3912\n",
      "Epoch [7/8], Step [710/1094], Loss: 1.5295\n",
      "Epoch [7/8], Step [720/1094], Loss: 1.4438\n",
      "Epoch [7/8], Step [730/1094], Loss: 1.5330\n",
      "Epoch [7/8], Step [740/1094], Loss: 1.4146\n",
      "Epoch [7/8], Step [750/1094], Loss: 1.3162\n",
      "Epoch [7/8], Step [760/1094], Loss: 1.4615\n",
      "Epoch [7/8], Step [770/1094], Loss: 1.3894\n",
      "Epoch [7/8], Step [780/1094], Loss: 1.4515\n",
      "Epoch [7/8], Step [790/1094], Loss: 1.5443\n",
      "Epoch [7/8], Step [800/1094], Loss: 1.3179\n",
      "Epoch [7/8], Step [810/1094], Loss: 1.3605\n",
      "Epoch [7/8], Step [820/1094], Loss: 1.4794\n",
      "Epoch [7/8], Step [830/1094], Loss: 1.4244\n",
      "Epoch [7/8], Step [840/1094], Loss: 1.4050\n",
      "Epoch [7/8], Step [850/1094], Loss: 1.4641\n",
      "Epoch [7/8], Step [860/1094], Loss: 1.4254\n",
      "Epoch [7/8], Step [870/1094], Loss: 1.4211\n",
      "Epoch [7/8], Step [880/1094], Loss: 1.5347\n",
      "Epoch [7/8], Step [890/1094], Loss: 1.4777\n",
      "Epoch [7/8], Step [900/1094], Loss: 1.4264\n",
      "Epoch [7/8], Step [910/1094], Loss: 1.4845\n",
      "Epoch [7/8], Step [920/1094], Loss: 1.3457\n",
      "Epoch [7/8], Step [930/1094], Loss: 1.6717\n",
      "Epoch [7/8], Step [940/1094], Loss: 1.4731\n",
      "Epoch [7/8], Step [950/1094], Loss: 1.5100\n",
      "Epoch [7/8], Step [960/1094], Loss: 1.5233\n",
      "Epoch [7/8], Step [970/1094], Loss: 1.3385\n",
      "Epoch [7/8], Step [980/1094], Loss: 1.3392\n",
      "Epoch [7/8], Step [990/1094], Loss: 1.4567\n",
      "Epoch [7/8], Step [1000/1094], Loss: 1.4229\n",
      "Epoch [7/8], Step [1010/1094], Loss: 1.5579\n",
      "Epoch [7/8], Step [1020/1094], Loss: 1.6735\n",
      "Epoch [7/8], Step [1030/1094], Loss: 1.4292\n",
      "Epoch [7/8], Step [1040/1094], Loss: 1.3848\n",
      "Epoch [7/8], Step [1050/1094], Loss: 1.3614\n",
      "Epoch [7/8], Step [1060/1094], Loss: 1.4821\n",
      "Epoch [7/8], Step [1070/1094], Loss: 1.4601\n",
      "Epoch [7/8], Step [1080/1094], Loss: 1.4985\n",
      "Epoch [7/8], Step [1090/1094], Loss: 1.3585\n",
      "Epoch [7/8], Average Loss: 1.4643, Accuracy: 0.2814\n",
      "Class 0: Precision = 0.4428, Recall = 0.3502, F1 Score = 0.3911\n",
      "Class 1: Precision = 0.1982, Recall = 0.2682, F1 Score = 0.2279\n",
      "Class 2: Precision = 0.4639, Recall = 0.6372, F1 Score = 0.5369\n",
      "Class 3: Precision = 0.1968, Recall = 0.1178, F1 Score = 0.1474\n",
      "Class 4: Precision = 0.2096, Recall = 0.3714, F1 Score = 0.2680\n",
      "Class 5: Precision = 0.1520, Recall = 0.0076, F1 Score = 0.0145\n",
      "Class 6: Precision = 0.2049, Recall = 0.2176, F1 Score = 0.2111\n",
      "Epoch [8/8], Step [10/1094], Loss: 1.3457\n",
      "Epoch [8/8], Step [20/1094], Loss: 1.3544\n",
      "Epoch [8/8], Step [30/1094], Loss: 1.5741\n",
      "Epoch [8/8], Step [40/1094], Loss: 1.4230\n",
      "Epoch [8/8], Step [50/1094], Loss: 1.4263\n",
      "Epoch [8/8], Step [60/1094], Loss: 1.4061\n",
      "Epoch [8/8], Step [70/1094], Loss: 1.6101\n",
      "Epoch [8/8], Step [80/1094], Loss: 1.4541\n",
      "Epoch [8/8], Step [90/1094], Loss: 1.3123\n",
      "Epoch [8/8], Step [100/1094], Loss: 1.4046\n",
      "Epoch [8/8], Step [110/1094], Loss: 1.5016\n",
      "Epoch [8/8], Step [120/1094], Loss: 1.5216\n",
      "Epoch [8/8], Step [130/1094], Loss: 1.4655\n",
      "Epoch [8/8], Step [140/1094], Loss: 1.3328\n",
      "Epoch [8/8], Step [150/1094], Loss: 1.2867\n",
      "Epoch [8/8], Step [160/1094], Loss: 1.4004\n",
      "Epoch [8/8], Step [170/1094], Loss: 1.5032\n",
      "Epoch [8/8], Step [180/1094], Loss: 1.3419\n",
      "Epoch [8/8], Step [190/1094], Loss: 1.4019\n",
      "Epoch [8/8], Step [200/1094], Loss: 1.4278\n",
      "Epoch [8/8], Step [210/1094], Loss: 1.4181\n",
      "Epoch [8/8], Step [220/1094], Loss: 1.4637\n",
      "Epoch [8/8], Step [230/1094], Loss: 1.4513\n",
      "Epoch [8/8], Step [240/1094], Loss: 1.7051\n",
      "Epoch [8/8], Step [250/1094], Loss: 1.5465\n",
      "Epoch [8/8], Step [260/1094], Loss: 1.4778\n",
      "Epoch [8/8], Step [270/1094], Loss: 1.3538\n",
      "Epoch [8/8], Step [280/1094], Loss: 1.3744\n",
      "Epoch [8/8], Step [290/1094], Loss: 1.5007\n",
      "Epoch [8/8], Step [300/1094], Loss: 1.3723\n",
      "Epoch [8/8], Step [310/1094], Loss: 1.3545\n",
      "Epoch [8/8], Step [320/1094], Loss: 1.4818\n",
      "Epoch [8/8], Step [330/1094], Loss: 1.5222\n",
      "Epoch [8/8], Step [340/1094], Loss: 1.4320\n",
      "Epoch [8/8], Step [350/1094], Loss: 1.5550\n",
      "Epoch [8/8], Step [360/1094], Loss: 1.4977\n",
      "Epoch [8/8], Step [370/1094], Loss: 1.3328\n",
      "Epoch [8/8], Step [380/1094], Loss: 1.6072\n",
      "Epoch [8/8], Step [390/1094], Loss: 1.4092\n",
      "Epoch [8/8], Step [400/1094], Loss: 1.3457\n",
      "Epoch [8/8], Step [410/1094], Loss: 1.4210\n",
      "Epoch [8/8], Step [420/1094], Loss: 1.4675\n",
      "Epoch [8/8], Step [430/1094], Loss: 1.5380\n",
      "Epoch [8/8], Step [440/1094], Loss: 1.4775\n",
      "Epoch [8/8], Step [450/1094], Loss: 1.3919\n",
      "Epoch [8/8], Step [460/1094], Loss: 1.4384\n",
      "Epoch [8/8], Step [470/1094], Loss: 1.6860\n",
      "Epoch [8/8], Step [480/1094], Loss: 1.3535\n",
      "Epoch [8/8], Step [490/1094], Loss: 1.4681\n",
      "Epoch [8/8], Step [500/1094], Loss: 1.3755\n",
      "Epoch [8/8], Step [510/1094], Loss: 1.5349\n",
      "Epoch [8/8], Step [520/1094], Loss: 1.6040\n",
      "Epoch [8/8], Step [530/1094], Loss: 1.3639\n",
      "Epoch [8/8], Step [540/1094], Loss: 1.6399\n",
      "Epoch [8/8], Step [550/1094], Loss: 1.7170\n",
      "Epoch [8/8], Step [560/1094], Loss: 1.4930\n",
      "Epoch [8/8], Step [570/1094], Loss: 1.4815\n",
      "Epoch [8/8], Step [580/1094], Loss: 1.4067\n",
      "Epoch [8/8], Step [590/1094], Loss: 1.3491\n",
      "Epoch [8/8], Step [600/1094], Loss: 1.3392\n",
      "Epoch [8/8], Step [610/1094], Loss: 1.4570\n",
      "Epoch [8/8], Step [620/1094], Loss: 1.3363\n",
      "Epoch [8/8], Step [630/1094], Loss: 1.4927\n",
      "Epoch [8/8], Step [640/1094], Loss: 1.4373\n",
      "Epoch [8/8], Step [650/1094], Loss: 1.4042\n",
      "Epoch [8/8], Step [660/1094], Loss: 1.3035\n",
      "Epoch [8/8], Step [670/1094], Loss: 1.4446\n",
      "Epoch [8/8], Step [680/1094], Loss: 1.4294\n",
      "Epoch [8/8], Step [690/1094], Loss: 1.4440\n",
      "Epoch [8/8], Step [700/1094], Loss: 1.3298\n",
      "Epoch [8/8], Step [710/1094], Loss: 1.3581\n",
      "Epoch [8/8], Step [720/1094], Loss: 1.5250\n",
      "Epoch [8/8], Step [730/1094], Loss: 1.5737\n",
      "Epoch [8/8], Step [740/1094], Loss: 1.4249\n",
      "Epoch [8/8], Step [750/1094], Loss: 1.5355\n",
      "Epoch [8/8], Step [760/1094], Loss: 1.4735\n",
      "Epoch [8/8], Step [770/1094], Loss: 1.5162\n",
      "Epoch [8/8], Step [780/1094], Loss: 1.4363\n",
      "Epoch [8/8], Step [790/1094], Loss: 1.3702\n",
      "Epoch [8/8], Step [800/1094], Loss: 1.4238\n",
      "Epoch [8/8], Step [810/1094], Loss: 1.5330\n",
      "Epoch [8/8], Step [820/1094], Loss: 1.5386\n",
      "Epoch [8/8], Step [830/1094], Loss: 1.7140\n",
      "Epoch [8/8], Step [840/1094], Loss: 1.4360\n",
      "Epoch [8/8], Step [850/1094], Loss: 1.3840\n",
      "Epoch [8/8], Step [860/1094], Loss: 1.4257\n",
      "Epoch [8/8], Step [870/1094], Loss: 1.3887\n",
      "Epoch [8/8], Step [880/1094], Loss: 1.4935\n",
      "Epoch [8/8], Step [890/1094], Loss: 1.5624\n",
      "Epoch [8/8], Step [900/1094], Loss: 1.5006\n",
      "Epoch [8/8], Step [910/1094], Loss: 1.4440\n",
      "Epoch [8/8], Step [920/1094], Loss: 1.5666\n",
      "Epoch [8/8], Step [930/1094], Loss: 1.6426\n",
      "Epoch [8/8], Step [940/1094], Loss: 1.2359\n",
      "Epoch [8/8], Step [950/1094], Loss: 1.6570\n",
      "Epoch [8/8], Step [960/1094], Loss: 1.4972\n",
      "Epoch [8/8], Step [970/1094], Loss: 1.5065\n",
      "Epoch [8/8], Step [980/1094], Loss: 1.2619\n",
      "Epoch [8/8], Step [990/1094], Loss: 1.6007\n",
      "Epoch [8/8], Step [1000/1094], Loss: 1.5828\n",
      "Epoch [8/8], Step [1010/1094], Loss: 1.3304\n",
      "Epoch [8/8], Step [1020/1094], Loss: 1.2776\n",
      "Epoch [8/8], Step [1030/1094], Loss: 1.3937\n",
      "Epoch [8/8], Step [1040/1094], Loss: 1.5470\n",
      "Epoch [8/8], Step [1050/1094], Loss: 1.4095\n",
      "Epoch [8/8], Step [1060/1094], Loss: 1.2835\n",
      "Epoch [8/8], Step [1070/1094], Loss: 1.6048\n",
      "Epoch [8/8], Step [1080/1094], Loss: 1.4433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/8], Step [1090/1094], Loss: 1.4304\n",
      "Epoch [8/8], Average Loss: 1.4650, Accuracy: 0.2795\n",
      "Class 0: Precision = 0.4337, Recall = 0.3902, F1 Score = 0.4108\n",
      "Class 1: Precision = 0.2104, Recall = 0.3432, F1 Score = 0.2609\n",
      "Class 2: Precision = 0.4564, Recall = 0.5774, F1 Score = 0.5098\n",
      "Class 3: Precision = 0.1968, Recall = 0.1816, F1 Score = 0.1889\n",
      "Class 4: Precision = 0.2077, Recall = 0.3058, F1 Score = 0.2474\n",
      "Class 5: Precision = 0.1798, Recall = 0.0238, F1 Score = 0.0420\n",
      "Class 6: Precision = 0.1987, Recall = 0.1346, F1 Score = 0.1605\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Constants\n",
    "NumPerElement = 1000  # Limit the number of samples to 10 per folder (for testing)\n",
    "sequence_length = 300  # The length of each sequence (number of time steps)\n",
    "\n",
    "input_size = 2  # Real and Imaginary components\n",
    "hidden_size = 130  # Number of hidden units per LSTM layer\n",
    "num_classes = 7  # Number of output classes\n",
    "num_layers = 6  # Number of stacked LSTM layers\n",
    "\n",
    "\n",
    "# Folder paths and corresponding labels\n",
    "folders_and_labels = [\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/4QAM/AWGN',0),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/4QAM/ImpulseNoise',0),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/4QAM/PinkNoise',0),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/4QAM/RayleighNoise',0),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/4QAM/RicianNoise',0),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM/AWGN',1),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM/ImpulseNoise',1),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM/PinkNoise',1),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM/RayleighNoise',1),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM/RicianNoise',1),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM-Flipped/AWGN',2),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM-Flipped/ImpulseNoise',2),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM-Flipped/PinkNoise',2),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM-Flipped/RayleighNoise',2),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM-Flipped/RicianNoise',2),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QPSK/AWGN',3),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QPSK/ImpulseNoise',3),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QPSK/PinkNoise',3),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QPSK/RayleighNoise',3),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QPSK/RicianNoise',3),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK/AWGN',4),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK/ImpulseNoise',4),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK/PinkNoise',4),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK/RayleighNoise',4),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK/RicianNoise',4),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK-Flipped/AWGN',5),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK-Flipped/ImpulseNoise',5),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK-Flipped/PinkNoise',5),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK-Flipped/RayleighNoise',5),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK-Flipped/RicianNoise',5),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/QPSK/AWGN',6),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/QPSK/ImpulseNoise',6),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/QPSK/PinkNoise',6),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/QPSK/RayleighNoise',6),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/QPSK/RicianNoise',6),\n",
    "]\n",
    "\n",
    "# Prepare data tensors\n",
    "im = torch.zeros(NumPerElement * len(folders_and_labels), sequence_length, input_size)  # (samples, time_steps, features)\n",
    "label = torch.zeros(NumPerElement * len(folders_and_labels))\n",
    "\n",
    "# Counters for the samples processed\n",
    "count = -1\n",
    "\n",
    "# Loop through each folder and load the data\n",
    "for folder_dir, folder_label in folders_and_labels:\n",
    "    count_in_folder = 0\n",
    "    for images in os.listdir(folder_dir):\n",
    "        if images != '.DS_Store' and count_in_folder < NumPerElement:\n",
    "            # Load the .mat file\n",
    "            AA = sio.loadmat(os.path.join(folder_dir, images))\n",
    "            count_in_folder += 1\n",
    "            count += 1\n",
    "            for key, value in AA.items():\n",
    "                if isinstance(value, np.ndarray) and np.iscomplexobj(value):\n",
    "                    im[count, :, 0] = torch.from_numpy(value.real[0, :])  # Real part\n",
    "                    im[count, :, 1] = torch.from_numpy(value.imag[0, :].copy())  # Imaginary part\n",
    "                label[count] = folder_label\n",
    "\n",
    "        if count_in_folder >= NumPerElement:  # Stop processing after 10 files per folder\n",
    "            break\n",
    "\n",
    "# Verify the data shape and labels\n",
    "print(f\"Total samples loaded: {count + 1}\")\n",
    "print(f\"Labels for first few samples: {label[:10]}\")\n",
    "\n",
    "# Define the Sequential LSTM Model\n",
    "model = nn.Sequential(\n",
    "    nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True),  # Stacked LSTM layers without dropout\n",
    "    nn.Linear(hidden_size, num_classes)  # Fully connected layer for classification\n",
    ")\n",
    "\n",
    "# Define the device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to the device\n",
    "model = model.to(device)\n",
    "\n",
    "# Convert to dataset and DataLoader\n",
    "dataset = TensorDataset(im, label)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 8\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Ensure the model is in training mode\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i, (data, labels) in enumerate(dataloader):\n",
    "        # Move data and labels to the same device as the model\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        lstm_out, _ = model[0](data)  # Only pass through LSTM layer\n",
    "        outputs = model[1](lstm_out[:, -1, :])  # Use output from last time step (last hidden state)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels.long())  # Ensure labels are of type long for classification\n",
    "        total_loss += loss.item() * data.size(0)  # Accumulate loss (weighted by batch size)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        # Collect all predictions and labels for metrics\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "        total_samples += data.size(0)\n",
    "\n",
    "        if (i+1) % 10 == 0:  # Print every 10 batches\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Convert all_preds and all_labels to numpy arrays\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    # Calculate Precision, Recall, F1 for each class, with zero_division set to 0\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average=None, labels=np.arange(num_classes), zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average=None, labels=np.arange(num_classes), zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average=None, labels=np.arange(num_classes), zero_division=0)\n",
    "\n",
    "    # Print the detailed metrics for each class\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {total_loss / total_samples:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    for i in range(num_classes):\n",
    "        print(f\"Class {i}: Precision = {precision[i]:.4f}, Recall = {recall[i]:.4f}, F1 Score = {f1[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3fbc81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d19fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc67e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82003cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157c4c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples loaded: 8000\n",
      "Labels for first few samples: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Constants\n",
    "NumPerElement = 2000  # Limit the number of samples to 10 per folder (for testing)\n",
    "sequence_length = 128  # The length of each sequence (number of time steps)\n",
    "input_size = 40  # The number of features per time step\n",
    "hidden_size = 128  # Number of hidden units per LSTM layer\n",
    "num_classes = 4  # Number of output classes\n",
    "num_layers = 4  # Number of stacked LSTM layers\n",
    "\n",
    "# Folder paths and corresponding labels\n",
    "folders_and_labels = [\n",
    "    ('/Users/anuraagthakur/Desktop/Imag/FMCWD1', 0),\n",
    "    ('/Users/anuraagthakur/Desktop/Imag/FMCWD2', 1),\n",
    "    ('/Users/anuraagthakur/Desktop/Imag/FMCWD3', 2),\n",
    "    ('/Users/anuraagthakur/Desktop/Imag/FMCWD4', 3),\n",
    "    # Add other folders here if needed\n",
    "]\n",
    "\n",
    "# Prepare data tensors\n",
    "im = torch.zeros(NumPerElement * len(folders_and_labels), input_size, sequence_length)  # (samples, features, time_steps)\n",
    "label = torch.zeros(NumPerElement * len(folders_and_labels))\n",
    "\n",
    "# Counters for the samples processed\n",
    "count = -1\n",
    "\n",
    "# Loop through each folder and load the data\n",
    "for folder_dir, folder_label in folders_and_labels:\n",
    "    count_in_folder = 0\n",
    "    for images in os.listdir(folder_dir):\n",
    "        if images != '.DS_Store' and count_in_folder < NumPerElement:\n",
    "            # Load the .mat file\n",
    "            AA = sio.loadmat(os.path.join(folder_dir, images))\n",
    "            count_in_folder += 1\n",
    "            count += 1\n",
    "            for key, value in AA.items():\n",
    "                # Assume value is a (128, 40) array representing 128 time steps and 40 features\n",
    "                if len(value.shape) == 2 and value.shape[0] == 128 and value.shape[1] == 40:\n",
    "                    # Transpose the (128, 40) array to (40, 128) for feature and time step flip\n",
    "                    flipped_data = value.T  # Shape becomes (40, 128)\n",
    "                    im[count, :, :] = torch.from_numpy(flipped_data)  # Store flipped data\n",
    "                else:\n",
    "                    print(f\"Unexpected shape for {images}: {value.shape}\")\n",
    "            label[count] = folder_label\n",
    "\n",
    "        if count_in_folder >= NumPerElement:  # Stop processing after NumPerElement files per folder\n",
    "            break\n",
    "\n",
    "# Verify the data shape and labels\n",
    "print(f\"Total samples loaded: {count + 1}\")\n",
    "print(f\"Labels for first few samples: {label[:10]}\")\n",
    "\n",
    "# Define the Sequential LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)  # Fully connected layer for classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through LSTM layer\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Use output from last time step (last hidden state)\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "model = LSTMModel(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes)\n",
    "\n",
    "# Define the device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Convert to dataset and DataLoader\n",
    "dataset = TensorDataset(im, label)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Ensure the model is in training mode\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i, (data, labels) in enumerate(dataloader):\n",
    "        # Move data and labels to the same device as the model\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(data)  # Pass data through LSTM model\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels.long())  # Ensure labels are of type long for classification\n",
    "        total_loss += loss.item() * data.size(0)  # Accumulate loss (weighted by batch size)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        # Collect all predictions and labels for metrics\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "        total_samples += data.size(0)\n",
    "\n",
    "        if (i + 1) % 10 == 0:  # Print every 10 batches\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Convert all_preds and all_labels to numpy arrays\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    # Calculate Precision, Recall, F1 for each class, with zero_division set to 0\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average=None, labels=np.arange(num_classes), zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average=None, labels=np.arange(num_classes), zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average=None, labels=np.arange(num_classes), zero_division=0)\n",
    "\n",
    "    # Print the detailed metrics for each class\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Average Loss: {total_loss / total_samples:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    for i in range(num_classes):\n",
    "        print(f\"Class {i}: Precision = {precision[i]:.4f}, Recall = {recall[i]:.4f}, F1 Score = {f1[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668f744a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f667c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4906de28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples loaded: 8000\n",
      "Labels for first few samples: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Epoch [1/5], Step [10/250], Loss: 1.3921\n",
      "Epoch [1/5], Step [20/250], Loss: 1.3893\n",
      "Epoch [1/5], Step [30/250], Loss: 1.3800\n",
      "Epoch [1/5], Step [40/250], Loss: 0.9711\n",
      "Epoch [1/5], Step [50/250], Loss: 1.5563\n",
      "Epoch [1/5], Step [60/250], Loss: 1.3703\n",
      "Epoch [1/5], Step [70/250], Loss: 1.3818\n",
      "Epoch [1/5], Step [80/250], Loss: 1.3870\n",
      "Epoch [1/5], Step [90/250], Loss: 2.4115\n",
      "Epoch [1/5], Step [100/250], Loss: 1.3989\n",
      "Epoch [1/5], Step [110/250], Loss: 1.4384\n",
      "Epoch [1/5], Step [120/250], Loss: 1.3968\n",
      "Epoch [1/5], Step [130/250], Loss: 1.3968\n",
      "Epoch [1/5], Step [140/250], Loss: 1.3546\n",
      "Epoch [1/5], Step [150/250], Loss: 1.3297\n",
      "Epoch [1/5], Step [160/250], Loss: 1.1937\n",
      "Epoch [1/5], Step [170/250], Loss: 0.9313\n",
      "Epoch [1/5], Step [180/250], Loss: 0.9662\n",
      "Epoch [1/5], Step [190/250], Loss: 1.3353\n",
      "Epoch [1/5], Step [200/250], Loss: 1.6262\n",
      "Epoch [1/5], Step [210/250], Loss: 1.1759\n",
      "Epoch [1/5], Step [220/250], Loss: 1.1349\n",
      "Epoch [1/5], Step [230/250], Loss: 0.8820\n",
      "Epoch [1/5], Step [240/250], Loss: 1.0280\n",
      "Epoch [1/5], Step [250/250], Loss: 0.7815\n",
      "Epoch [1/5], Average Loss: 1.2648, Accuracy: 0.3546\n",
      "Class 0: Precision = 0.3427, Recall = 0.2680, F1 Score = 0.3008\n",
      "Class 1: Precision = 0.4049, Recall = 0.3770, F1 Score = 0.3905\n",
      "Class 2: Precision = 0.3193, Recall = 0.2985, F1 Score = 0.3085\n",
      "Class 3: Precision = 0.3513, Recall = 0.4750, F1 Score = 0.4039\n",
      "Epoch [2/5], Step [10/250], Loss: 0.7632\n",
      "Epoch [2/5], Step [20/250], Loss: 0.7547\n",
      "Epoch [2/5], Step [30/250], Loss: 0.7272\n",
      "Epoch [2/5], Step [40/250], Loss: 0.7269\n",
      "Epoch [2/5], Step [50/250], Loss: 0.7630\n",
      "Epoch [2/5], Step [60/250], Loss: 0.7102\n",
      "Epoch [2/5], Step [70/250], Loss: 0.7053\n",
      "Epoch [2/5], Step [80/250], Loss: 0.7171\n",
      "Epoch [2/5], Step [90/250], Loss: 0.7431\n",
      "Epoch [2/5], Step [100/250], Loss: 0.6993\n",
      "Epoch [2/5], Step [110/250], Loss: 0.7236\n",
      "Epoch [2/5], Step [120/250], Loss: 0.6963\n",
      "Epoch [2/5], Step [130/250], Loss: 0.7053\n",
      "Epoch [2/5], Step [140/250], Loss: 0.7055\n",
      "Epoch [2/5], Step [150/250], Loss: 0.6970\n",
      "Epoch [2/5], Step [160/250], Loss: 0.6978\n",
      "Epoch [2/5], Step [170/250], Loss: 0.7034\n",
      "Epoch [2/5], Step [180/250], Loss: 0.7018\n",
      "Epoch [2/5], Step [190/250], Loss: 0.6905\n",
      "Epoch [2/5], Step [200/250], Loss: 0.6909\n",
      "Epoch [2/5], Step [210/250], Loss: 0.6945\n",
      "Epoch [2/5], Step [220/250], Loss: 0.7273\n",
      "Epoch [2/5], Step [230/250], Loss: 0.7168\n",
      "Epoch [2/5], Step [240/250], Loss: 0.6982\n",
      "Epoch [2/5], Step [250/250], Loss: 0.6766\n",
      "Epoch [2/5], Average Loss: 0.7137, Accuracy: 0.4895\n",
      "Class 0: Precision = 0.4910, Recall = 0.4360, F1 Score = 0.4619\n",
      "Class 1: Precision = 0.4879, Recall = 0.5220, F1 Score = 0.5043\n",
      "Class 2: Precision = 0.4860, Recall = 0.4520, F1 Score = 0.4684\n",
      "Class 3: Precision = 0.4928, Recall = 0.5480, F1 Score = 0.5189\n",
      "Epoch [3/5], Step [10/250], Loss: 0.6943\n",
      "Epoch [3/5], Step [20/250], Loss: 0.7145\n",
      "Epoch [3/5], Step [30/250], Loss: 0.7170\n",
      "Epoch [3/5], Step [40/250], Loss: 0.6967\n",
      "Epoch [3/5], Step [50/250], Loss: 0.7317\n",
      "Epoch [3/5], Step [60/250], Loss: 0.7179\n",
      "Epoch [3/5], Step [70/250], Loss: 0.6974\n",
      "Epoch [3/5], Step [80/250], Loss: 0.7298\n",
      "Epoch [3/5], Step [90/250], Loss: 0.6952\n",
      "Epoch [3/5], Step [100/250], Loss: 0.6933\n",
      "Epoch [3/5], Step [110/250], Loss: 0.7071\n",
      "Epoch [3/5], Step [120/250], Loss: 0.6961\n",
      "Epoch [3/5], Step [130/250], Loss: 0.6656\n",
      "Epoch [3/5], Step [140/250], Loss: 0.6830\n",
      "Epoch [3/5], Step [150/250], Loss: 0.6926\n",
      "Epoch [3/5], Step [160/250], Loss: 0.6893\n",
      "Epoch [3/5], Step [170/250], Loss: 0.7198\n",
      "Epoch [3/5], Step [180/250], Loss: 0.6883\n",
      "Epoch [3/5], Step [190/250], Loss: 1.0302\n",
      "Epoch [3/5], Step [200/250], Loss: 0.6982\n",
      "Epoch [3/5], Step [210/250], Loss: 1.7672\n",
      "Epoch [3/5], Step [220/250], Loss: 0.9056\n",
      "Epoch [3/5], Step [230/250], Loss: 0.8010\n",
      "Epoch [3/5], Step [240/250], Loss: 0.7221\n",
      "Epoch [3/5], Step [250/250], Loss: 0.7146\n",
      "Epoch [3/5], Average Loss: 0.7696, Accuracy: 0.5020\n",
      "Class 0: Precision = 0.5140, Recall = 0.6075, F1 Score = 0.5568\n",
      "Class 1: Precision = 0.4950, Recall = 0.4425, F1 Score = 0.4673\n",
      "Class 2: Precision = 0.4805, Recall = 0.5540, F1 Score = 0.5146\n",
      "Class 3: Precision = 0.5240, Recall = 0.4040, F1 Score = 0.4562\n",
      "Epoch [4/5], Step [10/250], Loss: 0.7537\n",
      "Epoch [4/5], Step [20/250], Loss: 0.6296\n",
      "Epoch [4/5], Step [30/250], Loss: 0.4327\n",
      "Epoch [4/5], Step [40/250], Loss: 0.4423\n",
      "Epoch [4/5], Step [50/250], Loss: 0.4567\n",
      "Epoch [4/5], Step [60/250], Loss: 0.3152\n",
      "Epoch [4/5], Step [70/250], Loss: 0.3874\n",
      "Epoch [4/5], Step [80/250], Loss: 0.3937\n",
      "Epoch [4/5], Step [90/250], Loss: 0.4460\n",
      "Epoch [4/5], Step [100/250], Loss: 0.3206\n",
      "Epoch [4/5], Step [110/250], Loss: 0.4199\n",
      "Epoch [4/5], Step [120/250], Loss: 0.3802\n",
      "Epoch [4/5], Step [130/250], Loss: 0.3114\n",
      "Epoch [4/5], Step [140/250], Loss: 0.3264\n",
      "Epoch [4/5], Step [150/250], Loss: 0.4307\n",
      "Epoch [4/5], Step [160/250], Loss: 0.3036\n",
      "Epoch [4/5], Step [170/250], Loss: 0.3979\n",
      "Epoch [4/5], Step [180/250], Loss: 0.4284\n",
      "Epoch [4/5], Step [190/250], Loss: 0.3738\n",
      "Epoch [4/5], Step [200/250], Loss: 0.3540\n",
      "Epoch [4/5], Step [210/250], Loss: 0.3659\n",
      "Epoch [4/5], Step [220/250], Loss: 0.4796\n",
      "Epoch [4/5], Step [230/250], Loss: 0.4163\n",
      "Epoch [4/5], Step [240/250], Loss: 0.3908\n",
      "Epoch [4/5], Step [250/250], Loss: 0.4125\n",
      "Epoch [4/5], Average Loss: 0.4072, Accuracy: 0.7340\n",
      "Class 0: Precision = 0.5038, Recall = 0.4995, F1 Score = 0.5016\n",
      "Class 1: Precision = 0.9554, Recall = 0.9740, F1 Score = 0.9646\n",
      "Class 2: Precision = 0.9735, Recall = 0.9545, F1 Score = 0.9639\n",
      "Class 3: Precision = 0.5037, Recall = 0.5080, F1 Score = 0.5059\n",
      "Epoch [5/5], Step [10/250], Loss: 0.3427\n",
      "Epoch [5/5], Step [20/250], Loss: 0.3891\n",
      "Epoch [5/5], Step [30/250], Loss: 0.3080\n",
      "Epoch [5/5], Step [40/250], Loss: 0.3069\n",
      "Epoch [5/5], Step [50/250], Loss: 0.3894\n",
      "Epoch [5/5], Step [60/250], Loss: 0.4955\n",
      "Epoch [5/5], Step [70/250], Loss: 0.4088\n",
      "Epoch [5/5], Step [80/250], Loss: 0.3749\n",
      "Epoch [5/5], Step [90/250], Loss: 0.2858\n",
      "Epoch [5/5], Step [100/250], Loss: 0.2917\n",
      "Epoch [5/5], Step [110/250], Loss: 0.3687\n",
      "Epoch [5/5], Step [120/250], Loss: 0.3852\n",
      "Epoch [5/5], Step [130/250], Loss: 0.4410\n",
      "Epoch [5/5], Step [140/250], Loss: 0.4103\n",
      "Epoch [5/5], Step [150/250], Loss: 0.3256\n",
      "Epoch [5/5], Step [160/250], Loss: 0.3406\n",
      "Epoch [5/5], Step [170/250], Loss: 0.3693\n",
      "Epoch [5/5], Step [180/250], Loss: 0.4285\n",
      "Epoch [5/5], Step [190/250], Loss: 0.3180\n",
      "Epoch [5/5], Step [200/250], Loss: 0.3039\n",
      "Epoch [5/5], Step [210/250], Loss: 0.2013\n",
      "Epoch [5/5], Step [220/250], Loss: 0.4980\n",
      "Epoch [5/5], Step [230/250], Loss: 0.3064\n",
      "Epoch [5/5], Step [240/250], Loss: 0.3692\n",
      "Epoch [5/5], Step [250/250], Loss: 0.3914\n",
      "Epoch [5/5], Average Loss: 0.3517, Accuracy: 0.7506\n",
      "Class 0: Precision = 0.5011, Recall = 0.5525, F1 Score = 0.5256\n",
      "Class 1: Precision = 1.0000, Recall = 0.9995, F1 Score = 0.9997\n",
      "Class 2: Precision = 1.0000, Recall = 1.0000, F1 Score = 1.0000\n",
      "Class 3: Precision = 0.5017, Recall = 0.4505, F1 Score = 0.4747\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Constants\n",
    "NumPerElement = 2000  # Limit the number of samples to 10 per folder (for testing)\n",
    "sequence_length = 128  # The length of each sequence (number of time steps)\n",
    "input_size = 40  # The number of features per time step\n",
    "hidden_size = 128  # Number of hidden units per LSTM layer\n",
    "num_classes = 4  # Number of output classes\n",
    "num_layers = 4  # Number of stacked LSTM layers\n",
    "\n",
    "# Folder paths and corresponding labels\n",
    "folders_and_labels = [\n",
    "    ('/Users/anuraagthakur/Desktop/Imag/FMCWD1', 0),\n",
    "    ('/Users/anuraagthakur/Desktop/Imag/FMCWD2', 1),\n",
    "    ('/Users/anuraagthakur/Desktop/Imag/FMCWD3', 2),\n",
    "    ('/Users/anuraagthakur/Desktop/Imag/FMCWD4', 3),\n",
    "    # Add other folders here if needed\n",
    "]\n",
    "\n",
    "# Prepare data tensors\n",
    "im = torch.zeros(NumPerElement * len(folders_and_labels), sequence_length, input_size)  # (samples, time_steps, features)\n",
    "label = torch.zeros(NumPerElement * len(folders_and_labels))\n",
    "\n",
    "# Counters for the samples processed\n",
    "count = -1\n",
    "\n",
    "# Loop through each folder and load the data\n",
    "for folder_dir, folder_label in folders_and_labels:\n",
    "    count_in_folder = 0\n",
    "    for images in os.listdir(folder_dir):\n",
    "        if images != '.DS_Store' and count_in_folder < NumPerElement:\n",
    "            # Load the .mat file\n",
    "            AA = sio.loadmat(os.path.join(folder_dir, images))\n",
    "            count_in_folder += 1\n",
    "            count += 1\n",
    "            for key, value in AA.items():\n",
    "                # Assume value is a (128, 40) array representing 128 time steps and 40 features\n",
    "                if len(value.shape) == 2 and value.shape[0] == 128 and value.shape[1] == 40:\n",
    "                    # Flatten the (128, 40) array into a 1D vector and assign it to the tensor\n",
    "                    flattened_data = value.flatten()  # Shape becomes (128 * 40,)\n",
    "                    im[count, :, :] = torch.from_numpy(flattened_data).view(sequence_length, input_size)  # Reshape to (128, 40)\n",
    "                else:\n",
    "                    print(f\"Unexpected shape for {images}: {value.shape}\")\n",
    "            label[count] = folder_label\n",
    "\n",
    "        if count_in_folder >= NumPerElement:  # Stop processing after NumPerElement files per folder\n",
    "            break\n",
    "\n",
    "# Verify the data shape and labels\n",
    "print(f\"Total samples loaded: {count + 1}\")\n",
    "print(f\"Labels for first few samples: {label[:10]}\")\n",
    "\n",
    "# Define the Sequential LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)  # Fully connected layer for classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through LSTM layer\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Use output from last time step (last hidden state)\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "model = LSTMModel(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes)\n",
    "\n",
    "# Define the device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Convert to dataset and DataLoader\n",
    "dataset = TensorDataset(im, label)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Ensure the model is in training mode\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i, (data, labels) in enumerate(dataloader):\n",
    "        # Move data and labels to the same device as the model\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(data)  # Pass data through LSTM model\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels.long())  # Ensure labels are of type long for classification\n",
    "        total_loss += loss.item() * data.size(0)  # Accumulate loss (weighted by batch size)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        # Collect all predictions and labels for metrics\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "        total_samples += data.size(0)\n",
    "\n",
    "        if (i + 1) % 10 == 0:  # Print every 10 batches\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Convert all_preds and all_labels to numpy arrays\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    # Calculate Precision, Recall, F1 for each class, with zero_division set to 0\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average=None, labels=np.arange(num_classes), zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average=None, labels=np.arange(num_classes), zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average=None, labels=np.arange(num_classes), zero_division=0)\n",
    "\n",
    "    # Print the detailed metrics for each class\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Average Loss: {total_loss / total_samples:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    for i in range(num_classes):\n",
    "        print(f\"Class {i}: Precision = {precision[i]:.4f}, Recall = {recall[i]:.4f}, F1 Score = {f1[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f07108e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d95f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c152c60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c812e50c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305b55d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5fe7f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d49938",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1e997b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acf1bf9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples loaded: 8000\n",
      "Labels for first few samples: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Epoch [1/5], Step [10/250], Loss: 1.3969\n",
      "Epoch [1/5], Step [20/250], Loss: 1.3652\n",
      "Epoch [1/5], Step [30/250], Loss: 1.3947\n",
      "Epoch [1/5], Step [40/250], Loss: 1.3908\n",
      "Epoch [1/5], Step [50/250], Loss: 1.3899\n",
      "Epoch [1/5], Step [60/250], Loss: 1.4032\n",
      "Epoch [1/5], Step [70/250], Loss: 1.3813\n",
      "Epoch [1/5], Step [80/250], Loss: 1.3946\n",
      "Epoch [1/5], Step [90/250], Loss: 1.3901\n",
      "Epoch [1/5], Step [100/250], Loss: 1.3874\n",
      "Epoch [1/5], Step [110/250], Loss: 1.3905\n",
      "Epoch [1/5], Step [120/250], Loss: 1.4006\n",
      "Epoch [1/5], Step [130/250], Loss: 1.3786\n",
      "Epoch [1/5], Step [140/250], Loss: 1.3810\n",
      "Epoch [1/5], Step [150/250], Loss: 1.3918\n",
      "Epoch [1/5], Step [160/250], Loss: 1.3865\n",
      "Epoch [1/5], Step [170/250], Loss: 1.3855\n",
      "Epoch [1/5], Step [180/250], Loss: 1.3896\n",
      "Epoch [1/5], Step [190/250], Loss: 1.3872\n",
      "Epoch [1/5], Step [200/250], Loss: 1.3950\n",
      "Epoch [1/5], Step [210/250], Loss: 1.3873\n",
      "Epoch [1/5], Step [220/250], Loss: 1.3861\n",
      "Epoch [1/5], Step [230/250], Loss: 1.3838\n",
      "Epoch [1/5], Step [240/250], Loss: 1.3865\n",
      "Epoch [1/5], Step [250/250], Loss: 1.3849\n",
      "Epoch [1/5], Average Loss: 1.3876, Accuracy: 0.2474\n",
      "Class 0: Precision = 0.2459, Recall = 0.1880, F1 Score = 0.2131\n",
      "Class 1: Precision = 0.2472, Recall = 0.4520, F1 Score = 0.3196\n",
      "Class 2: Precision = 0.2429, Recall = 0.0945, F1 Score = 0.1361\n",
      "Class 3: Precision = 0.2505, Recall = 0.2550, F1 Score = 0.2527\n",
      "Epoch [2/5], Step [10/250], Loss: 1.3824\n",
      "Epoch [2/5], Step [20/250], Loss: 1.3796\n",
      "Epoch [2/5], Step [30/250], Loss: 1.3796\n",
      "Epoch [2/5], Step [40/250], Loss: 1.3847\n",
      "Epoch [2/5], Step [50/250], Loss: 1.3774\n",
      "Epoch [2/5], Step [60/250], Loss: 1.3871\n",
      "Epoch [2/5], Step [70/250], Loss: 1.3845\n",
      "Epoch [2/5], Step [80/250], Loss: 1.3888\n",
      "Epoch [2/5], Step [90/250], Loss: 1.3889\n",
      "Epoch [2/5], Step [100/250], Loss: 1.3848\n",
      "Epoch [2/5], Step [110/250], Loss: 1.3824\n",
      "Epoch [2/5], Step [120/250], Loss: 1.3893\n",
      "Epoch [2/5], Step [130/250], Loss: 1.3864\n",
      "Epoch [2/5], Step [140/250], Loss: 1.3875\n",
      "Epoch [2/5], Step [150/250], Loss: 1.3833\n",
      "Epoch [2/5], Step [160/250], Loss: 1.3897\n",
      "Epoch [2/5], Step [170/250], Loss: 1.3833\n",
      "Epoch [2/5], Step [180/250], Loss: 1.3909\n",
      "Epoch [2/5], Step [190/250], Loss: 1.3854\n",
      "Epoch [2/5], Step [200/250], Loss: 1.3859\n",
      "Epoch [2/5], Step [210/250], Loss: 1.3916\n",
      "Epoch [2/5], Step [220/250], Loss: 1.3872\n",
      "Epoch [2/5], Step [230/250], Loss: 1.3909\n",
      "Epoch [2/5], Step [240/250], Loss: 1.3870\n",
      "Epoch [2/5], Step [250/250], Loss: 1.3906\n",
      "Epoch [2/5], Average Loss: 1.3868, Accuracy: 0.2474\n",
      "Class 0: Precision = 0.2444, Recall = 0.3235, F1 Score = 0.2785\n",
      "Class 1: Precision = 0.2443, Recall = 0.0960, F1 Score = 0.1378\n",
      "Class 2: Precision = 0.2437, Recall = 0.1925, F1 Score = 0.2151\n",
      "Class 3: Precision = 0.2528, Recall = 0.3775, F1 Score = 0.3028\n",
      "Epoch [3/5], Step [10/250], Loss: 1.3839\n",
      "Epoch [3/5], Step [20/250], Loss: 1.3874\n",
      "Epoch [3/5], Step [30/250], Loss: 1.3878\n",
      "Epoch [3/5], Step [40/250], Loss: 1.3831\n",
      "Epoch [3/5], Step [50/250], Loss: 1.3885\n",
      "Epoch [3/5], Step [60/250], Loss: 1.3846\n",
      "Epoch [3/5], Step [70/250], Loss: 1.3809\n",
      "Epoch [3/5], Step [80/250], Loss: 1.3914\n",
      "Epoch [3/5], Step [90/250], Loss: 1.3903\n",
      "Epoch [3/5], Step [100/250], Loss: 1.3875\n",
      "Epoch [3/5], Step [110/250], Loss: 1.3898\n",
      "Epoch [3/5], Step [120/250], Loss: 1.3897\n",
      "Epoch [3/5], Step [130/250], Loss: 1.3845\n",
      "Epoch [3/5], Step [140/250], Loss: 1.3845\n",
      "Epoch [3/5], Step [150/250], Loss: 1.3885\n",
      "Epoch [3/5], Step [160/250], Loss: 1.3787\n",
      "Epoch [3/5], Step [170/250], Loss: 1.3882\n",
      "Epoch [3/5], Step [180/250], Loss: 1.3745\n",
      "Epoch [3/5], Step [190/250], Loss: 1.3839\n",
      "Epoch [3/5], Step [200/250], Loss: 1.3903\n",
      "Epoch [3/5], Step [210/250], Loss: 1.3918\n",
      "Epoch [3/5], Step [220/250], Loss: 1.3904\n",
      "Epoch [3/5], Step [230/250], Loss: 1.3852\n",
      "Epoch [3/5], Step [240/250], Loss: 1.3949\n",
      "Epoch [3/5], Step [250/250], Loss: 1.3899\n",
      "Epoch [3/5], Average Loss: 1.3867, Accuracy: 0.2521\n",
      "Class 0: Precision = 0.2552, Recall = 0.2805, F1 Score = 0.2673\n",
      "Class 1: Precision = 0.2535, Recall = 0.4350, F1 Score = 0.3203\n",
      "Class 2: Precision = 0.2477, Recall = 0.1785, F1 Score = 0.2075\n",
      "Class 3: Precision = 0.2465, Recall = 0.1145, F1 Score = 0.1564\n",
      "Epoch [4/5], Step [10/250], Loss: 1.3905\n",
      "Epoch [4/5], Step [20/250], Loss: 1.3888\n",
      "Epoch [4/5], Step [30/250], Loss: 1.3879\n",
      "Epoch [4/5], Step [40/250], Loss: 1.3876\n",
      "Epoch [4/5], Step [50/250], Loss: 1.3862\n",
      "Epoch [4/5], Step [60/250], Loss: 1.3887\n",
      "Epoch [4/5], Step [70/250], Loss: 1.3842\n",
      "Epoch [4/5], Step [80/250], Loss: 1.3862\n",
      "Epoch [4/5], Step [90/250], Loss: 1.3892\n",
      "Epoch [4/5], Step [100/250], Loss: 1.3886\n",
      "Epoch [4/5], Step [110/250], Loss: 1.3893\n",
      "Epoch [4/5], Step [120/250], Loss: 1.3870\n",
      "Epoch [4/5], Step [130/250], Loss: 1.3830\n",
      "Epoch [4/5], Step [140/250], Loss: 1.3892\n",
      "Epoch [4/5], Step [150/250], Loss: 1.3824\n",
      "Epoch [4/5], Step [160/250], Loss: 1.3850\n",
      "Epoch [4/5], Step [170/250], Loss: 1.3849\n",
      "Epoch [4/5], Step [180/250], Loss: 1.3855\n",
      "Epoch [4/5], Step [190/250], Loss: 1.3864\n",
      "Epoch [4/5], Step [200/250], Loss: 1.3875\n",
      "Epoch [4/5], Step [210/250], Loss: 1.3874\n",
      "Epoch [4/5], Step [220/250], Loss: 1.3892\n",
      "Epoch [4/5], Step [230/250], Loss: 1.3827\n",
      "Epoch [4/5], Step [240/250], Loss: 1.3857\n",
      "Epoch [4/5], Step [250/250], Loss: 1.3870\n",
      "Epoch [4/5], Average Loss: 1.3867, Accuracy: 0.2451\n",
      "Class 0: Precision = 0.2482, Recall = 0.2605, F1 Score = 0.2542\n",
      "Class 1: Precision = 0.2418, Recall = 0.4425, F1 Score = 0.3127\n",
      "Class 2: Precision = 0.2496, Recall = 0.2350, F1 Score = 0.2421\n",
      "Class 3: Precision = 0.2374, Recall = 0.0425, F1 Score = 0.0721\n",
      "Epoch [5/5], Step [10/250], Loss: 1.3840\n",
      "Epoch [5/5], Step [20/250], Loss: 1.3888\n",
      "Epoch [5/5], Step [30/250], Loss: 1.3834\n",
      "Epoch [5/5], Step [40/250], Loss: 1.3862\n",
      "Epoch [5/5], Step [50/250], Loss: 1.3829\n",
      "Epoch [5/5], Step [60/250], Loss: 1.3943\n",
      "Epoch [5/5], Step [70/250], Loss: 1.3881\n",
      "Epoch [5/5], Step [80/250], Loss: 1.3871\n",
      "Epoch [5/5], Step [90/250], Loss: 1.3910\n",
      "Epoch [5/5], Step [100/250], Loss: 1.3940\n",
      "Epoch [5/5], Step [110/250], Loss: 1.3909\n",
      "Epoch [5/5], Step [120/250], Loss: 1.3857\n",
      "Epoch [5/5], Step [130/250], Loss: 1.3873\n",
      "Epoch [5/5], Step [140/250], Loss: 1.3779\n",
      "Epoch [5/5], Step [150/250], Loss: 1.3871\n",
      "Epoch [5/5], Step [160/250], Loss: 1.3878\n",
      "Epoch [5/5], Step [170/250], Loss: 1.3863\n",
      "Epoch [5/5], Step [180/250], Loss: 1.3866\n",
      "Epoch [5/5], Step [190/250], Loss: 1.3902\n",
      "Epoch [5/5], Step [200/250], Loss: 1.3810\n",
      "Epoch [5/5], Step [210/250], Loss: 1.3883\n",
      "Epoch [5/5], Step [220/250], Loss: 1.3860\n",
      "Epoch [5/5], Step [230/250], Loss: 1.3847\n",
      "Epoch [5/5], Step [240/250], Loss: 1.3822\n",
      "Epoch [5/5], Step [250/250], Loss: 1.3853\n",
      "Epoch [5/5], Average Loss: 1.3867, Accuracy: 0.2581\n",
      "Class 0: Precision = 0.2546, Recall = 0.3900, F1 Score = 0.3081\n",
      "Class 1: Precision = 0.2713, Recall = 0.1225, F1 Score = 0.1688\n",
      "Class 2: Precision = 0.3000, Recall = 0.0060, F1 Score = 0.0118\n",
      "Class 3: Precision = 0.2575, Recall = 0.5140, F1 Score = 0.3431\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Constants\n",
    "NumPerElement = 2000  # Limit the number of samples to 10 per folder (for testing)\n",
    "sequence_length = 128  # The length of each sequence (number of time steps)\n",
    "input_size = 40  # The number of features per time step\n",
    "hidden_size = 128  # Number of hidden units per LSTM layer\n",
    "num_classes = 4  # Number of output classes\n",
    "num_layers = 4  # Number of stacked LSTM layers\n",
    "\n",
    "# Folder paths and corresponding labels\n",
    "folders_and_labels = [\n",
    "    ('/Users/anuraagthakur/Desktop/Imag/2CW', 0),\n",
    "    ('/Users/anuraagthakur/Desktop/Imag/4CW', 1),\n",
    "    ('/Users/anuraagthakur/Desktop/Imag/6CW', 2),\n",
    "    ('/Users/anuraagthakur/Desktop/Imag/8CW', 3),\n",
    "    # Add other folders here if needed\n",
    "]\n",
    "\n",
    "# Prepare data tensors\n",
    "im = torch.zeros(NumPerElement * len(folders_and_labels), sequence_length, input_size)  # (samples, time_steps, features)\n",
    "label = torch.zeros(NumPerElement * len(folders_and_labels))\n",
    "\n",
    "# Counters for the samples processed\n",
    "count = -1\n",
    "\n",
    "# Loop through each folder and load the data\n",
    "for folder_dir, folder_label in folders_and_labels:\n",
    "    count_in_folder = 0\n",
    "    for images in os.listdir(folder_dir):\n",
    "        if images != '.DS_Store' and count_in_folder < NumPerElement:\n",
    "            # Load the .mat file\n",
    "            AA = sio.loadmat(os.path.join(folder_dir, images))\n",
    "            count_in_folder += 1\n",
    "            count += 1\n",
    "            for key, value in AA.items():\n",
    "                # Assume value is a (128, 40) array representing 128 time steps and 40 features\n",
    "                if len(value.shape) == 2 and value.shape[0] == 128 and value.shape[1] == 40:\n",
    "                    # Flatten the (128, 40) array into a 1D vector and assign it to the tensor\n",
    "                    flattened_data = value.flatten()  # Shape becomes (128 * 40,)\n",
    "                    im[count, :, :] = torch.from_numpy(flattened_data).view(sequence_length, input_size)  # Reshape to (128, 40)\n",
    "                else:\n",
    "                    print(f\"Unexpected shape for {images}: {value.shape}\")\n",
    "            label[count] = folder_label\n",
    "\n",
    "        if count_in_folder >= NumPerElement:  # Stop processing after NumPerElement files per folder\n",
    "            break\n",
    "\n",
    "# Verify the data shape and labels\n",
    "print(f\"Total samples loaded: {count + 1}\")\n",
    "print(f\"Labels for first few samples: {label[:10]}\")\n",
    "\n",
    "# Define the Sequential LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)  # Fully connected layer for classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through LSTM layer\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Use output from last time step (last hidden state)\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "model = LSTMModel(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes)\n",
    "\n",
    "# Define the device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Convert to dataset and DataLoader\n",
    "dataset = TensorDataset(im, label)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Ensure the model is in training mode\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i, (data, labels) in enumerate(dataloader):\n",
    "        # Move data and labels to the same device as the model\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(data)  # Pass data through LSTM model\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels.long())  # Ensure labels are of type long for classification\n",
    "        total_loss += loss.item() * data.size(0)  # Accumulate loss (weighted by batch size)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        # Collect all predictions and labels for metrics\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "        total_samples += data.size(0)\n",
    "\n",
    "        if (i + 1) % 10 == 0:  # Print every 10 batches\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Convert all_preds and all_labels to numpy arrays\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    # Calculate Precision, Recall, F1 for each class, with zero_division set to 0\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average=None, labels=np.arange(num_classes), zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average=None, labels=np.arange(num_classes), zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average=None, labels=np.arange(num_classes), zero_division=0)\n",
    "\n",
    "    # Print the detailed metrics for each class\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Average Loss: {total_loss / total_samples:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    for i in range(num_classes):\n",
    "        print(f\"Class {i}: Precision = {precision[i]:.4f}, Recall = {recall[i]:.4f}, F1 Score = {f1[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf6cfe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcf85cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c4a9b21",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/anuraagthakur/Desktop/Imag/FMCWUP2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kz/23c67y1112113d31_lhlx3_h0000gn/T/ipykernel_32303/3533801913.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfolder_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder_label\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfolders_and_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mcount_in_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'.DS_Store'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcount_in_folder\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mNumPerElement\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# Load the .mat file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/anuraagthakur/Desktop/Imag/FMCWUP2'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Constants\n",
    "NumPerElement = 2000  # Limit the number of samples to 10 per folder (for testing)\n",
    "sequence_length = 128  # The length of each sequence (number of time steps)\n",
    "input_size = 40  # The number of features per time step\n",
    "hidden_size = 128  # Number of hidden units per LSTM layer\n",
    "num_classes = 4  # Number of output classes\n",
    "num_layers = 4  # Number of stacked LSTM layers\n",
    "\n",
    "# Folder paths and corresponding labels\n",
    "folders_and_labels = [\n",
    "    ('/Users/anuraagthakur/Desktop/Imag/FMCWUP1', 0),\n",
    "    ('/Users/anuraagthakur/Desktop/Imag/FMCWUP2', 1),\n",
    "    ('/Users/anuraagthakur/Desktop/Imag/FMCWUP3', 2),\n",
    "    ('/Users/anuraagthakur/Desktop/Imag/FMCWUP4', 3),\n",
    "    # Add other folders here if needed\n",
    "]\n",
    "\n",
    "# Prepare data tensors\n",
    "im = torch.zeros(NumPerElement * len(folders_and_labels), sequence_length, input_size)  # (samples, time_steps, features)\n",
    "label = torch.zeros(NumPerElement * len(folders_and_labels))\n",
    "\n",
    "# Counters for the samples processed\n",
    "count = -1\n",
    "\n",
    "# Loop through each folder and load the data\n",
    "for folder_dir, folder_label in folders_and_labels:\n",
    "    count_in_folder = 0\n",
    "    for images in os.listdir(folder_dir):\n",
    "        if images != '.DS_Store' and count_in_folder < NumPerElement:\n",
    "            # Load the .mat file\n",
    "            AA = sio.loadmat(os.path.join(folder_dir, images))\n",
    "            count_in_folder += 1\n",
    "            count += 1\n",
    "            for key, value in AA.items():\n",
    "                # Assume value is a (128, 40) array representing 128 time steps and 40 features\n",
    "                if len(value.shape) == 2 and value.shape[0] == 128 and value.shape[1] == 40:\n",
    "                    # Flatten the (128, 40) array into a 1D vector and assign it to the tensor\n",
    "                    flattened_data = value.flatten()  # Shape becomes (128 * 40,)\n",
    "                    im[count, :, :] = torch.from_numpy(flattened_data).view(sequence_length, input_size)  # Reshape to (128, 40)\n",
    "                else:\n",
    "                    print(f\"Unexpected shape for {images}: {value.shape}\")\n",
    "            label[count] = folder_label\n",
    "\n",
    "        if count_in_folder >= NumPerElement:  # Stop processing after NumPerElement files per folder\n",
    "            break\n",
    "\n",
    "# Verify the data shape and labels\n",
    "print(f\"Total samples loaded: {count + 1}\")\n",
    "print(f\"Labels for first few samples: {label[:10]}\")\n",
    "\n",
    "# Define the Sequential LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)  # Fully connected layer for classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through LSTM layer\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Use output from last time step (last hidden state)\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "model = LSTMModel(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes)\n",
    "\n",
    "# Define the device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Convert to dataset and DataLoader\n",
    "dataset = TensorDataset(im, label)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Ensure the model is in training mode\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i, (data, labels) in enumerate(dataloader):\n",
    "        # Move data and labels to the same device as the model\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(data)  # Pass data through LSTM model\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels.long())  # Ensure labels are of type long for classification\n",
    "        total_loss += loss.item() * data.size(0)  # Accumulate loss (weighted by batch size)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        # Collect all predictions and labels for metrics\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "        total_samples += data.size(0)\n",
    "\n",
    "        if (i + 1) % 10 == 0:  # Print every 10 batches\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Convert all_preds and all_labels to numpy arrays\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    # Calculate Precision, Recall, F1 for each class, with zero_division set to 0\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average=None, labels=np.arange(num_classes), zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average=None, labels=np.arange(num_classes), zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average=None, labels=np.arange(num_classes), zero_division=0)\n",
    "\n",
    "    # Print the detailed metrics for each class\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Average Loss: {total_loss / total_samples:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    for i in range(num_classes):\n",
    "        print(f\"Class {i}: Precision = {precision[i]:.4f}, Recall = {recall[i]:.4f}, F1 Score = {f1[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca319d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af27a89a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c249388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fa1099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87ef94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Constants\n",
    "NumPerElement = 2000  # Limit the number of samples to 10 per folder (for testing)\n",
    "sequence_length = 300  # The length of each sequence (number of time steps)\n",
    "input_size = 2  # The number of features per time step\n",
    "hidden_size = 128  # Number of hidden units per LSTM layer\n",
    "num_classes = 4  # Number of output classes\n",
    "num_layers = 4  # Number of stacked LSTM layers\n",
    "\n",
    "# Folder paths and corresponding labels\n",
    "folders_and_labels = [\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/4QAM/AWGN',0),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/4QAM/ImpulseNoise',0),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/4QAM/PinkNoise',0),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/4QAM/RayleighNoise',0),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/4QAM/RicianNoise',0),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM/AWGN',1),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM/ImpulseNoise',1),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM/PinkNoise',1),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM/RayleighNoise',1),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM/RicianNoise',1),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM-Flipped/AWGN',2),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM-Flipped/ImpulseNoise',2),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM-Flipped/PinkNoise',2),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM-Flipped/RayleighNoise',2),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QAM-Flipped/RicianNoise',2),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QPSK/AWGN',3),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QPSK/ImpulseNoise',3),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QPSK/PinkNoise',3),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QPSK/RayleighNoise',3),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/8QPSK/RicianNoise',3),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK/AWGN',4),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK/ImpulseNoise',4),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK/PinkNoise',4),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK/RayleighNoise',4),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK/RicianNoise',4),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK-Flipped/AWGN',5),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK-Flipped/ImpulseNoise',5),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK-Flipped/PinkNoise',5),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK-Flipped/RayleighNoise',5),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/BPSK-Flipped/RicianNoise',5),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/QPSK/AWGN',6),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/QPSK/ImpulseNoise',6),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/QPSK/PinkNoise',6),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/QPSK/RayleighNoise',6),\n",
    "    ('/Users/anuraagthakur/Desktop/untitled folder 7/QPSK/RicianNoise',6),\n",
    "    # Add other folders here if needed\n",
    "]\n",
    "\n",
    "# Prepare data tensors\n",
    "im = torch.zeros(NumPerElement * len(folders_and_labels), sequence_length, input_size)  # (samples, time_steps, features)\n",
    "label = torch.zeros(NumPerElement * len(folders_and_labels))\n",
    "\n",
    "# Counters for the samples processed\n",
    "count = -1\n",
    "\n",
    "# Loop through each folder and load the data\n",
    "for folder_dir, folder_label in folders_and_labels:\n",
    "    count_in_folder = 0\n",
    "    for images in os.listdir(folder_dir):\n",
    "        if images != '.DS_Store' and count_in_folder < NumPerElement:\n",
    "            # Load the .mat file\n",
    "            AA = sio.loadmat(os.path.join(folder_dir, images))\n",
    "            count_in_folder += 1\n",
    "            count += 1\n",
    "            for key, value in AA.items():\n",
    "                # Assume value is a (128, 40) array representing 128 time steps and 40 features\n",
    "                if len(value.shape) == 2 and value.shape[0] == 128 and value.shape[1] == 40:\n",
    "                    # Flatten the (128, 40) array into a 1D vector and assign it to the tensor\n",
    "                    flattened_data = value.flatten()  # Shape becomes (128 * 40,)\n",
    "                    im[count, :, :] = torch.from_numpy(flattened_data).view(sequence_length, input_size)  # Reshape to (128, 40)\n",
    "                else:\n",
    "                    print(f\"Unexpected shape for {images}: {value.shape}\")\n",
    "            label[count] = folder_label\n",
    "\n",
    "        if count_in_folder >= NumPerElement:  # Stop processing after NumPerElement files per folder\n",
    "            break\n",
    "\n",
    "# Verify the data shape and labels\n",
    "print(f\"Total samples loaded: {count + 1}\")\n",
    "print(f\"Labels for first few samples: {label[:10]}\")\n",
    "\n",
    "# Define the Sequential LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)  # Fully connected layer for classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through LSTM layer\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Use output from last time step (last hidden state)\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "model = LSTMModel(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes)\n",
    "\n",
    "# Define the device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Convert to dataset and DataLoader\n",
    "dataset = TensorDataset(im, label)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Ensure the model is in training mode\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i, (data, labels) in enumerate(dataloader):\n",
    "        # Move data and labels to the same device as the model\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(data)  # Pass data through LSTM model\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels.long())  # Ensure labels are of type long for classification\n",
    "        total_loss += loss.item() * data.size(0)  # Accumulate loss (weighted by batch size)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        # Collect all predictions and labels for metrics\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "        total_samples += data.size(0)\n",
    "\n",
    "        if (i + 1) % 10 == 0:  # Print every 10 batches\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Convert all_preds and all_labels to numpy arrays\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    # Calculate Precision, Recall, F1 for each class, with zero_division set to 0\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average=None, labels=np.arange(num_classes), zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average=None, labels=np.arange(num_classes), zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average=None, labels=np.arange(num_classes), zero_division=0)\n",
    "\n",
    "    # Print the detailed metrics for each class\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Average Loss: {total_loss / total_samples:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    for i in range(num_classes):\n",
    "        print(f\"Class {i}: Precision = {precision[i]:.4f}, Recall = {recall[i]:.4f}, F1 Score = {f1[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6722414d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33108ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7619924a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b595405b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb2da57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Constants\n",
    "NumPerElement = 2000  # Limit the number of samples to 10 per folder (for testing)\n",
    "sequence_length = 128  # The length of each sequence (number of time steps)\n",
    "input_size = 40  # The number of features per time step\n",
    "hidden_size = 128  # Number of hidden units per LSTM layer\n",
    "num_classes = 4  # Number of output classes\n",
    "num_layers = 4  # Number of stacked LSTM layers\n",
    "\n",
    "# Folder paths and corresponding labels\n",
    "folders_and_labels = [\n",
    "    ('/Users/anuraagthakur/Desktop/Imag/2CW', 0),\n",
    "    ('/Users/anuraagthakur/Desktop/Imag/4CW', 1),\n",
    "    ('/Users/anuraagthakur/Desktop/Imag/6CW', 2),\n",
    "    ('/Users/anuraagthakur/Desktop/Imag/8CW', 3),\n",
    "    # Add other folders here if needed\n",
    "]\n",
    "\n",
    "# Prepare data tensors\n",
    "im = torch.zeros(NumPerElement * len(folders_and_labels), sequence_length, input_size)  # (samples, time_steps, features)\n",
    "label = torch.zeros(NumPerElement * len(folders_and_labels))\n",
    "\n",
    "# Counters for the samples processed\n",
    "count = -1\n",
    "\n",
    "# Loop through each folder and load the data\n",
    "for folder_dir, folder_label in folders_and_labels:\n",
    "    count_in_folder = 0\n",
    "    for images in os.listdir(folder_dir):\n",
    "        if images != '.DS_Store' and count_in_folder < NumPerElement:\n",
    "            # Load the .mat file\n",
    "            AA = sio.loadmat(os.path.join(folder_dir, images))\n",
    "            count_in_folder += 1\n",
    "            count += 1\n",
    "            for key, value in AA.items():\n",
    "                # Assume value is a (128, 40) array representing 128 time steps and 40 features\n",
    "                if len(value.shape) == 2 and value.shape[0] == 128 and value.shape[1] == 40:\n",
    "                    # Flatten the (128, 40) array into a 1D vector and assign it to the tensor\n",
    "                    flattened_data = value.flatten()  # Shape becomes (128 * 40,)\n",
    "                    im[count, :, :] = torch.from_numpy(flattened_data).view(sequence_length, input_size)  # Reshape to (128, 40)\n",
    "                else:\n",
    "                    print(f\"Unexpected shape for {images}: {value.shape}\")\n",
    "            label[count] = folder_label\n",
    "\n",
    "        if count_in_folder >= NumPerElement:  # Stop processing after NumPerElement files per folder\n",
    "            break\n",
    "\n",
    "# Verify the data shape and labels\n",
    "print(f\"Total samples loaded: {count + 1}\")\n",
    "print(f\"Labels for first few samples: {label[:10]}\")\n",
    "\n",
    "# Define the Sequential LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)  # Fully connected layer for classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through LSTM layer\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Use output from last time step (last hidden state)\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "model = LSTMModel(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes)\n",
    "\n",
    "# Define the device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Convert to dataset and DataLoader\n",
    "dataset = TensorDataset(im, label)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Ensure the model is in training mode\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i, (data, labels) in enumerate(dataloader):\n",
    "        # Move data and labels to the same device as the model\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(data)  # Pass data through LSTM model\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels.long())  # Ensure labels are of type long for classification\n",
    "        total_loss += loss.item() * data.size(0)  # Accumulate loss (weighted by batch size)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        # Collect all predictions and labels for metrics\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "        total_samples += data.size(0)\n",
    "\n",
    "        if (i + 1) % 10 == 0:  # Print every 10 batches\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Convert all_preds and all_labels to numpy arrays\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    # Calculate Precision, Recall, F1 for each class, with zero_division set to 0\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average=None, labels=np.arange(num_classes), zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average=None, labels=np.arange(num_classes), zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average=None, labels=np.arange(num_classes), zero_division=0)\n",
    "\n",
    "    # Print the detailed metrics for each class\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Average Loss: {total_loss / total_samples:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    for i in range(num_classes):\n",
    "        print(f\"Class {i}: Precision = {precision[i]:.4f}, Recall = {recall[i]:.4f}, F1 Score = {f1[i]:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
